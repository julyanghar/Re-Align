/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-14 23:08:38,445] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-14 23:08:39,001] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-14 23:08:39,001] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12354 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-0 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-0 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 0.5 --use_anchor False --yilin_anchor True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-14 23:08:40,705] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-14 23:08:41,302] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-14 23:08:41,302] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-14 23:08:41,302] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-14 23:08:41,302] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-14 23:08:41,302] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-14 23:08:45,017] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-14 23:08:45,243] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-14 23:08:45,332] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-14 23:08:45,573] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-14 23:08:45,848] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-14 23:08:46,084] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-14 23:08:46,336] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-14 23:08:46,569] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-14 23:08:46,569] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-10-14 23:08:46,757] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-14 23:08:46,763] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-14 23:08:47,088] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-14 23:08:47,090] [INFO] [comm.py:637:init_distributed] cdb=None
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251014_230847-rq5w1xao
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-0
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/rq5w1xao
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.32s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.27s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.80s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.79s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.75s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.85s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.89s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.50s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.85s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.51s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.85s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.57s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.90s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.90s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.53s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.89s/it]
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.59s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.47s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.86s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.78s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.09s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.03s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.70s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.64s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.66s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.02s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.89s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.49s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.81s/it]
Formatting inputs...Skip in lazy mode
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.014561176300048828 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.011158227920532227 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.011845111846923828 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10104966163635254 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10137796401977539 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10131001472473145 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/186 [00:08<27:03,  8.78s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1307 > 1048). Running this sequence through the model will result in indexing errors
  1%|          | 2/186 [00:18<29:28,  9.61s/it]  2%|▏         | 3/186 [00:29<30:18,  9.94s/it]  2%|▏         | 4/186 [00:39<30:48, 10.16s/it]  3%|▎         | 5/186 [00:50<31:12, 10.34s/it]  3%|▎         | 6/186 [01:01<31:15, 10.42s/it]  4%|▍         | 7/186 [01:11<31:16, 10.48s/it]  4%|▍         | 8/186 [01:22<31:18, 10.55s/it]  5%|▍         | 9/186 [01:33<31:14, 10.59s/it]  5%|▌         | 10/186 [01:44<31:42, 10.81s/it]  6%|▌         | 11/186 [01:55<32:03, 10.99s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1158 > 1048). Running this sequence through the model will result in indexing errors
  6%|▋         | 12/186 [02:07<32:26, 11.19s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1153 > 1048). Running this sequence through the model will result in indexing errors
  7%|▋         | 13/186 [02:18<32:21, 11.22s/it]  8%|▊         | 14/186 [02:30<32:41, 11.41s/it]  8%|▊         | 15/186 [02:41<32:29, 11.40s/it]  9%|▊         | 16/186 [02:53<32:22, 11.43s/it]  9%|▉         | 17/186 [03:04<32:05, 11.39s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 10%|▉         | 18/186 [03:16<32:10, 11.49s/it] 10%|█         | 19/186 [03:27<31:56, 11.48s/it] 11%|█         | 20/186 [03:39<31:47, 11.49s/it] 11%|█▏        | 21/186 [03:50<31:35, 11.49s/it] 12%|█▏        | 22/186 [04:02<31:16, 11.44s/it] 12%|█▏        | 23/186 [04:13<31:15, 11.51s/it] 13%|█▎        | 24/186 [04:25<31:10, 11.55s/it] 13%|█▎        | 25/186 [04:36<30:46, 11.47s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1240 > 1048). Running this sequence through the model will result in indexing errors
 14%|█▍        | 26/186 [04:48<30:42, 11.51s/it] 15%|█▍        | 27/186 [05:00<30:50, 11.64s/it] 15%|█▌        | 28/186 [05:12<30:49, 11.71s/it] 16%|█▌        | 29/186 [05:23<30:29, 11.65s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1245 > 1048). Running this sequence through the model will result in indexing errors
 16%|█▌        | 30/186 [05:35<30:20, 11.67s/it] 17%|█▋        | 31/186 [05:47<30:26, 11.79s/it] 17%|█▋        | 32/186 [05:59<30:06, 11.73s/it] 18%|█▊        | 33/186 [06:10<29:35, 11.60s/it] 18%|█▊        | 34/186 [06:22<29:28, 11.64s/it] 19%|█▉        | 35/186 [06:33<29:23, 11.68s/it] 19%|█▉        | 36/186 [06:45<28:57, 11.58s/it] 20%|█▉        | 37/186 [06:56<28:43, 11.57s/it] 20%|██        | 38/186 [07:08<28:25, 11.52s/it] 21%|██        | 39/186 [07:20<28:48, 11.76s/it] 22%|██▏       | 40/186 [07:32<28:37, 11.77s/it] 22%|██▏       | 41/186 [07:43<28:21, 11.73s/it] 23%|██▎       | 42/186 [07:55<28:03, 11.69s/it] 23%|██▎       | 43/186 [08:06<27:38, 11.60s/it] 24%|██▎       | 44/186 [08:18<27:14, 11.51s/it] 24%|██▍       | 45/186 [08:30<27:21, 11.64s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 25%|██▍       | 46/186 [08:41<27:01, 11.58s/it] 25%|██▌       | 47/186 [08:53<27:09, 11.72s/it] 26%|██▌       | 48/186 [09:05<27:08, 11.80s/it] 26%|██▋       | 49/186 [09:17<26:52, 11.77s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
 27%|██▋       | 50/186 [09:29<26:41, 11.77s/it] 27%|██▋       | 51/186 [09:40<26:23, 11.73s/it] 28%|██▊       | 52/186 [09:52<25:56, 11.61s/it] 28%|██▊       | 53/186 [10:03<25:39, 11.57s/it] 29%|██▉       | 54/186 [10:15<25:27, 11.57s/it] 30%|██▉       | 55/186 [10:26<25:11, 11.54s/it] 30%|███       | 56/186 [10:37<24:53, 11.49s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1073 > 1048). Running this sequence through the model will result in indexing errors
 31%|███       | 57/186 [10:50<25:19, 11.78s/it] 31%|███       | 58/186 [11:02<25:09, 11.79s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
 32%|███▏      | 59/186 [11:14<25:03, 11.84s/it] 32%|███▏      | 60/186 [11:25<24:39, 11.74s/it] 33%|███▎      | 61/186 [11:37<24:15, 11.65s/it] 33%|███▎      | 62/186 [11:48<23:56, 11.59s/it] 34%|███▍      | 63/186 [12:00<24:04, 11.75s/it] 34%|███▍      | 64/186 [12:12<23:56, 11.77s/it] 35%|███▍      | 65/186 [12:23<23:27, 11.63s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 35%|███▌      | 66/186 [12:35<23:21, 11.68s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 36%|███▌      | 67/186 [12:47<23:03, 11.62s/it] 37%|███▋      | 68/186 [12:58<22:47, 11.59s/it] 37%|███▋      | 69/186 [13:10<22:39, 11.62s/it] 38%|███▊      | 70/186 [13:22<22:50, 11.81s/it] 38%|███▊      | 71/186 [13:34<22:48, 11.90s/it] 39%|███▊      | 72/186 [13:46<22:25, 11.80s/it] 39%|███▉      | 73/186 [13:57<22:06, 11.74s/it] 40%|███▉      | 74/186 [14:09<22:02, 11.81s/it] 40%|████      | 75/186 [14:21<21:39, 11.71s/it] 41%|████      | 76/186 [14:33<21:28, 11.71s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 41%|████▏     | 77/186 [14:44<21:07, 11.63s/it] 42%|████▏     | 78/186 [14:55<20:51, 11.59s/it] 42%|████▏     | 79/186 [15:07<20:43, 11.62s/it] 43%|████▎     | 80/186 [15:19<20:41, 11.71s/it] 44%|████▎     | 81/186 [15:31<20:20, 11.63s/it] 44%|████▍     | 82/186 [15:42<20:15, 11.68s/it] 45%|████▍     | 83/186 [15:54<20:06, 11.71s/it] 45%|████▌     | 84/186 [16:06<19:55, 11.72s/it] 46%|████▌     | 85/186 [16:18<19:43, 11.71s/it] 46%|████▌     | 86/186 [16:29<19:22, 11.63s/it] 47%|████▋     | 87/186 [16:40<19:02, 11.54s/it] 47%|████▋     | 88/186 [16:52<18:46, 11.49s/it] 48%|████▊     | 89/186 [17:03<18:38, 11.53s/it] 48%|████▊     | 90/186 [17:15<18:26, 11.53s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 49%|████▉     | 91/186 [17:27<18:23, 11.62s/it] 49%|████▉     | 92/186 [17:38<18:06, 11.55s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 50%|█████     | 93/186 [17:50<18:06, 11.68s/it] 51%|█████     | 94/186 [18:02<17:59, 11.73s/it] 51%|█████     | 95/186 [18:14<17:46, 11.72s/it] 52%|█████▏    | 96/186 [18:25<17:26, 11.63s/it] 52%|█████▏    | 97/186 [18:36<17:09, 11.57s/it] 53%|█████▎    | 98/186 [18:48<16:58, 11.57s/it] 53%|█████▎    | 99/186 [19:00<16:54, 11.66s/it] 54%|█████▍    | 100/186 [19:11<16:34, 11.57s/it] 54%|█████▍    | 101/186 [19:23<16:25, 11.59s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 55%|█████▍    | 102/186 [19:34<16:07, 11.52s/it] 55%|█████▌    | 103/186 [19:46<15:59, 11.56s/it] 56%|█████▌    | 104/186 [19:58<15:57, 11.68s/it] 56%|█████▋    | 105/186 [20:09<15:38, 11.58s/it] 57%|█████▋    | 106/186 [20:21<15:23, 11.55s/it] 58%|█████▊    | 107/186 [20:32<15:06, 11.48s/it] 58%|█████▊    | 108/186 [20:44<14:59, 11.54s/it] 59%|█████▊    | 109/186 [20:55<14:44, 11.49s/it] 59%|█████▉    | 110/186 [21:07<14:38, 11.56s/it] 60%|█████▉    | 111/186 [21:19<14:31, 11.62s/it] 60%|██████    | 112/186 [21:30<14:17, 11.59s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 61%|██████    | 113/186 [21:41<14:02, 11.54s/it] 61%|██████▏   | 114/186 [21:53<13:49, 11.52s/it] 62%|██████▏   | 115/186 [22:04<13:37, 11.52s/it] 62%|██████▏   | 116/186 [22:16<13:32, 11.60s/it] 63%|██████▎   | 117/186 [22:28<13:18, 11.57s/it] 63%|██████▎   | 118/186 [22:39<13:07, 11.59s/it] 64%|██████▍   | 119/186 [22:51<12:53, 11.54s/it] 65%|██████▍   | 120/186 [23:02<12:41, 11.54s/it] 65%|██████▌   | 121/186 [23:14<12:29, 11.53s/it] 66%|██████▌   | 122/186 [23:25<12:16, 11.50s/it] 66%|██████▌   | 123/186 [23:37<12:08, 11.56s/it] 67%|██████▋   | 124/186 [23:48<11:55, 11.53s/it] 67%|██████▋   | 125/186 [24:00<11:45, 11.57s/it] 68%|██████▊   | 126/186 [24:12<11:35, 11.59s/it] 68%|██████▊   | 127/186 [24:23<11:21, 11.56s/it] 69%|██████▉   | 128/186 [24:35<11:07, 11.50s/it] 69%|██████▉   | 129/186 [24:46<10:58, 11.55s/it] 70%|██████▉   | 130/186 [24:58<10:45, 11.53s/it] 70%|███████   | 131/186 [25:09<10:33, 11.52s/it] 71%|███████   | 132/186 [25:21<10:23, 11.54s/it] 72%|███████▏  | 133/186 [25:32<10:11, 11.55s/it] 72%|███████▏  | 134/186 [25:44<09:58, 11.50s/it] 73%|███████▎  | 135/186 [25:55<09:48, 11.53s/it] 73%|███████▎  | 136/186 [26:07<09:36, 11.54s/it] 74%|███████▎  | 137/186 [26:19<09:29, 11.62s/it] 74%|███████▍  | 138/186 [26:30<09:14, 11.56s/it] 75%|███████▍  | 139/186 [26:42<09:05, 11.61s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 75%|███████▌  | 140/186 [26:53<08:53, 11.59s/it] 76%|███████▌  | 141/186 [27:05<08:39, 11.54s/it] 76%|███████▋  | 142/186 [27:16<08:26, 11.52s/it] 77%|███████▋  | 143/186 [27:28<08:16, 11.54s/it] 77%|███████▋  | 144/186 [27:39<08:05, 11.55s/it] 78%|███████▊  | 145/186 [27:51<07:53, 11.54s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 78%|███████▊  | 146/186 [28:03<07:45, 11.63s/it] 79%|███████▉  | 147/186 [28:15<07:34, 11.65s/it] 80%|███████▉  | 148/186 [28:26<07:21, 11.61s/it] 80%|████████  | 149/186 [28:38<07:08, 11.58s/it] 81%|████████  | 150/186 [28:49<06:56, 11.56s/it] 81%|████████  | 151/186 [29:01<06:43, 11.54s/it] 82%|████████▏ | 152/186 [29:12<06:31, 11.52s/it] 82%|████████▏ | 153/186 [29:24<06:21, 11.55s/it] 83%|████████▎ | 154/186 [29:35<06:08, 11.51s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 83%|████████▎ | 155/186 [29:46<05:55, 11.47s/it] 84%|████████▍ | 156/186 [29:59<05:50, 11.70s/it] 84%|████████▍ | 157/186 [30:10<05:35, 11.58s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 85%|████████▍ | 158/186 [30:21<05:23, 11.55s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 85%|████████▌ | 159/186 [30:33<05:10, 11.50s/it] 86%|████████▌ | 160/186 [30:45<05:01, 11.59s/it] 87%|████████▋ | 161/186 [30:56<04:47, 11.49s/it] 87%|████████▋ | 162/186 [31:08<04:38, 11.62s/it] 88%|████████▊ | 163/186 [31:19<04:27, 11.61s/it] 88%|████████▊ | 164/186 [31:31<04:14, 11.57s/it] 89%|████████▊ | 165/186 [31:42<04:02, 11.54s/it] 89%|████████▉ | 166/186 [31:54<03:50, 11.50s/it] 90%|████████▉ | 167/186 [32:05<03:39, 11.53s/it] 90%|█████████ | 168/186 [32:17<03:27, 11.51s/it] 91%|█████████ | 169/186 [32:28<03:16, 11.54s/it] 91%|█████████▏| 170/186 [32:40<03:04, 11.50s/it] 92%|█████████▏| 171/186 [32:52<02:53, 11.57s/it] 92%|█████████▏| 172/186 [33:03<02:41, 11.54s/it] 93%|█████████▎| 173/186 [33:14<02:29, 11.48s/it] 94%|█████████▎| 174/186 [33:26<02:18, 11.51s/it]WARNING: tokenization mismatch: 1 vs. 47. (ignored)
 94%|█████████▍| 175/186 [33:38<02:06, 11.53s/it] 95%|█████████▍| 176/186 [33:49<01:54, 11.48s/it] 95%|█████████▌| 177/186 [34:00<01:43, 11.46s/it] 96%|█████████▌| 178/186 [34:12<01:31, 11.42s/it] 96%|█████████▌| 179/186 [34:23<01:19, 11.42s/it] 97%|█████████▋| 180/186 [34:35<01:08, 11.44s/it] 97%|█████████▋| 181/186 [34:47<00:57, 11.60s/it] 98%|█████████▊| 182/186 [34:58<00:46, 11.57s/it] 98%|█████████▊| 183/186 [35:10<00:34, 11.61s/it] 99%|█████████▉| 184/186 [35:21<00:23, 11.62s/it] 99%|█████████▉| 185/186 [35:33<00:11, 11.71s/it]100%|██████████| 186/186 [35:45<00:00, 11.65s/it]                                                 {'train_runtime': 2145.3651, 'train_samples_per_second': 6.262, 'train_steps_per_second': 0.087, 'train_loss': 0.776051018827705, 'epoch': 1.0}
100%|██████████| 186/186 [35:46<00:00, 11.65s/it]100%|██████████| 186/186 [35:46<00:00, 11.54s/it]
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-0
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33mdirect-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-0[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251014_230847-rq5w1xao/logs[0m
[2025-10-14 23:46:19,101] [INFO] [launch.py:347:main] Process 477281 exits successfully.
[2025-10-14 23:46:19,102] [INFO] [launch.py:347:main] Process 477280 exits successfully.
[2025-10-14 23:46:20,103] [INFO] [launch.py:347:main] Process 477282 exits successfully.
[2025-10-14 23:46:20,103] [INFO] [launch.py:347:main] Process 477279 exits successfully.
[2025-10-14 23:46:20,104] [INFO] [launch.py:347:main] Process 477283 exits successfully.
[2025-10-14 23:46:24,108] [INFO] [launch.py:347:main] Process 477278 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-14 23:46:27,421] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-14 23:46:27,976] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-14 23:46:27,976] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12354 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.7-0 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.7-0 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 0.7 --use_anchor False --yilin_anchor True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-14 23:46:29,681] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-14 23:46:30,275] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-14 23:46:30,275] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-14 23:46:30,276] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-14 23:46:30,276] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-14 23:46:30,276] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-14 23:46:33,985] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-14 23:46:34,103] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-14 23:46:34,323] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-14 23:46:34,387] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-14 23:46:34,431] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-14 23:46:34,544] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-14 23:46:34,612] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-14 23:46:34,780] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-14 23:46:34,780] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-14 23:46:34,847] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-14 23:46:34,934] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
[2025-10-14 23:46:36,115] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-14 23:46:36,690] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251014_234636-qr51ub9h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.7-0
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/qr51ub9h
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.85s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.87s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.57s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.72s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.05s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.81s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.94s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.73s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.57s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.91s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.59s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.94s/it]
to bfloat16...
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.51s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.84s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.72s/it]
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.27s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.42s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.84s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.20s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.97s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.33s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.34s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.09s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.80s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.88s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.77s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.15s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.74s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.09s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.89s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.59s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.93s/it]
Formatting inputs...Skip in lazy mode
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.013957977294921875 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.011435985565185547 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10143232345581055 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...

Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.01186513900756836 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.1014711856842041 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10306143760681152 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/186 [00:09<28:01,  9.09s/it]  1%|          | 2/186 [00:18<28:56,  9.44s/it]  2%|▏         | 3/186 [00:29<30:07,  9.88s/it]  2%|▏         | 4/186 [00:39<30:29, 10.05s/it]  3%|▎         | 5/186 [00:50<31:00, 10.28s/it]  3%|▎         | 6/186 [01:00<30:53, 10.30s/it]  4%|▍         | 7/186 [01:10<30:53, 10.35s/it]  4%|▍         | 8/186 [01:21<30:54, 10.42s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
  5%|▍         | 9/186 [01:32<31:04, 10.53s/it]  5%|▌         | 10/186 [01:43<31:22, 10.70s/it]  6%|▌         | 11/186 [01:54<31:09, 10.68s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1080 > 1048). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1080 > 1048). Running this sequence through the model will result in indexing errors
  6%|▋         | 12/186 [02:05<31:35, 10.89s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1158 > 1048). Running this sequence through the model will result in indexing errors
  7%|▋         | 13/186 [02:17<32:02, 11.11s/it]  8%|▊         | 14/186 [02:28<32:22, 11.29s/it]  8%|▊         | 15/186 [02:40<32:22, 11.36s/it]  9%|▊         | 16/186 [02:51<32:27, 11.46s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
  9%|▉         | 17/186 [03:03<32:25, 11.51s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 10%|▉         | 18/186 [03:15<32:15, 11.52s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 10%|█         | 19/186 [03:26<32:06, 11.54s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 11%|█         | 20/186 [03:38<32:00, 11.57s/it] 11%|█▏        | 21/186 [03:49<31:52, 11.59s/it] 12%|█▏        | 22/186 [04:01<31:44, 11.61s/it] 12%|█▏        | 23/186 [04:13<31:30, 11.60s/it] 13%|█▎        | 24/186 [04:25<31:59, 11.85s/it] 13%|█▎        | 25/186 [04:37<31:34, 11.77s/it] 14%|█▍        | 26/186 [04:48<31:13, 11.71s/it] 15%|█▍        | 27/186 [05:01<31:37, 11.93s/it] 15%|█▌        | 28/186 [05:12<31:04, 11.80s/it] 16%|█▌        | 29/186 [05:24<30:47, 11.77s/it] 16%|█▌        | 30/186 [05:35<30:25, 11.70s/it] 17%|█▋        | 31/186 [05:47<30:08, 11.67s/it] 17%|█▋        | 32/186 [05:58<29:44, 11.59s/it] 18%|█▊        | 33/186 [06:10<29:36, 11.61s/it] 18%|█▊        | 34/186 [06:22<29:16, 11.56s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1069 > 1048). Running this sequence through the model will result in indexing errors
 19%|█▉        | 35/186 [06:34<29:30, 11.72s/it] 19%|█▉        | 36/186 [06:45<29:12, 11.68s/it] 20%|█▉        | 37/186 [06:57<28:54, 11.64s/it] 20%|██        | 38/186 [07:09<28:47, 11.67s/it] 21%|██        | 39/186 [07:21<29:02, 11.85s/it] 22%|██▏       | 40/186 [07:33<28:57, 11.90s/it] 22%|██▏       | 41/186 [07:45<28:39, 11.86s/it] 23%|██▎       | 42/186 [07:56<28:20, 11.81s/it] 23%|██▎       | 43/186 [08:08<28:05, 11.79s/it] 24%|██▎       | 44/186 [08:20<27:54, 11.79s/it] 24%|██▍       | 45/186 [08:32<27:44, 11.80s/it] 25%|██▍       | 46/186 [08:44<27:40, 11.86s/it] 25%|██▌       | 47/186 [08:56<27:46, 11.99s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 26%|██▌       | 48/186 [09:08<27:28, 11.95s/it] 26%|██▋       | 49/186 [09:19<27:05, 11.87s/it] 27%|██▋       | 50/186 [09:31<26:57, 11.90s/it] 27%|██▋       | 51/186 [09:43<26:39, 11.85s/it] 28%|██▊       | 52/186 [09:55<26:22, 11.81s/it] 28%|██▊       | 53/186 [10:06<26:01, 11.74s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 29%|██▉       | 54/186 [10:18<25:48, 11.73s/it] 30%|██▉       | 55/186 [10:30<25:35, 11.72s/it] 30%|███       | 56/186 [10:42<25:22, 11.71s/it] 31%|███       | 57/186 [10:54<25:24, 11.82s/it] 31%|███       | 58/186 [11:05<25:10, 11.80s/it] 32%|███▏      | 59/186 [11:17<25:10, 11.89s/it]WARNING: tokenization mismatch: 1 vs. 47. (ignored)
 32%|███▏      | 60/186 [11:29<24:54, 11.86s/it]WARNING: tokenization mismatch: 1 vs. 47. (ignored)
 33%|███▎      | 61/186 [11:41<24:40, 11.84s/it] 33%|███▎      | 62/186 [11:53<24:22, 11.79s/it] 34%|███▍      | 63/186 [12:04<24:03, 11.73s/it] 34%|███▍      | 64/186 [12:16<23:54, 11.76s/it] 35%|███▍      | 65/186 [12:28<23:46, 11.79s/it] 35%|███▌      | 66/186 [12:40<23:32, 11.77s/it] 36%|███▌      | 67/186 [12:52<23:21, 11.78s/it] 37%|███▋      | 68/186 [13:03<23:09, 11.78s/it] 37%|███▋      | 69/186 [13:15<23:10, 11.89s/it] 38%|███▊      | 70/186 [13:28<23:13, 12.02s/it] 38%|███▊      | 71/186 [13:40<23:02, 12.02s/it] 39%|███▊      | 72/186 [13:51<22:38, 11.92s/it] 39%|███▉      | 73/186 [14:04<22:32, 11.97s/it] 40%|███▉      | 74/186 [14:15<22:15, 11.92s/it] 40%|████      | 75/186 [14:27<21:55, 11.85s/it] 41%|████      | 76/186 [14:39<21:35, 11.78s/it] 41%|████▏     | 77/186 [14:50<21:24, 11.78s/it] 42%|████▏     | 78/186 [15:02<21:07, 11.74s/it] 42%|████▏     | 79/186 [15:14<21:03, 11.81s/it] 43%|████▎     | 80/186 [15:26<20:53, 11.82s/it] 44%|████▎     | 81/186 [15:38<20:39, 11.81s/it] 44%|████▍     | 82/186 [15:49<20:24, 11.77s/it] 45%|████▍     | 83/186 [16:01<20:07, 11.72s/it] 45%|████▌     | 84/186 [16:13<19:51, 11.68s/it] 46%|████▌     | 85/186 [16:24<19:40, 11.69s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1065 > 1048). Running this sequence through the model will result in indexing errors
 46%|████▌     | 86/186 [16:36<19:32, 11.72s/it] 47%|████▋     | 87/186 [16:48<19:23, 11.76s/it] 47%|████▋     | 88/186 [16:59<19:06, 11.69s/it] 48%|████▊     | 89/186 [17:11<18:57, 11.72s/it] 48%|████▊     | 90/186 [17:23<18:43, 11.70s/it] 49%|████▉     | 91/186 [17:36<19:05, 12.05s/it]WARNING: tokenization mismatch: 1 vs. 48. (ignored)
 49%|████▉     | 92/186 [17:48<18:59, 12.13s/it] 50%|█████     | 93/186 [18:00<18:54, 12.20s/it] 51%|█████     | 94/186 [18:12<18:29, 12.06s/it] 51%|█████     | 95/186 [18:24<18:16, 12.05s/it] 52%|█████▏    | 96/186 [18:36<17:54, 11.93s/it] 52%|█████▏    | 97/186 [18:48<17:39, 11.91s/it] 53%|█████▎    | 98/186 [18:59<17:23, 11.86s/it] 53%|█████▎    | 99/186 [19:11<17:13, 11.87s/it] 54%|█████▍    | 100/186 [19:23<17:02, 11.89s/it] 54%|█████▍    | 101/186 [19:35<16:51, 11.90s/it] 55%|█████▍    | 102/186 [19:47<16:38, 11.88s/it] 55%|█████▌    | 103/186 [19:59<16:30, 11.93s/it] 56%|█████▌    | 104/186 [20:11<16:17, 11.93s/it] 56%|█████▋    | 105/186 [20:23<16:00, 11.86s/it] 57%|█████▋    | 106/186 [20:35<15:47, 11.84s/it] 58%|█████▊    | 107/186 [20:46<15:33, 11.82s/it] 58%|█████▊    | 108/186 [20:58<15:18, 11.77s/it] 59%|█████▊    | 109/186 [21:10<15:11, 11.83s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 59%|█████▉    | 110/186 [21:22<15:01, 11.86s/it] 60%|█████▉    | 111/186 [21:34<14:51, 11.88s/it] 60%|██████    | 112/186 [21:46<14:44, 11.95s/it] 61%|██████    | 113/186 [21:58<14:35, 11.99s/it] 61%|██████▏   | 114/186 [22:10<14:30, 12.09s/it]WARNING: tokenization mismatch: 1 vs. 48. (ignored)
 62%|██████▏   | 115/186 [22:22<14:19, 12.10s/it] 62%|██████▏   | 116/186 [22:35<14:15, 12.22s/it] 63%|██████▎   | 117/186 [22:47<13:59, 12.16s/it] 63%|██████▎   | 118/186 [22:59<13:47, 12.17s/it] 64%|██████▍   | 119/186 [23:11<13:31, 12.11s/it] 65%|██████▍   | 120/186 [23:23<13:17, 12.09s/it] 65%|██████▌   | 121/186 [23:35<13:07, 12.11s/it] 66%|██████▌   | 122/186 [23:48<12:57, 12.15s/it] 66%|██████▌   | 123/186 [24:00<12:56, 12.33s/it] 67%|██████▋   | 124/186 [24:12<12:40, 12.26s/it] 67%|██████▋   | 125/186 [24:24<12:21, 12.16s/it] 68%|██████▊   | 126/186 [24:36<11:58, 11.98s/it] 68%|██████▊   | 127/186 [24:48<11:51, 12.06s/it] 69%|██████▉   | 128/186 [25:00<11:35, 11.99s/it] 69%|██████▉   | 129/186 [25:12<11:21, 11.96s/it] 70%|██████▉   | 130/186 [25:24<11:12, 12.01s/it] 70%|███████   | 131/186 [25:36<11:00, 12.02s/it] 71%|███████   | 132/186 [25:48<10:45, 11.96s/it] 72%|███████▏  | 133/186 [26:00<10:33, 11.96s/it] 72%|███████▏  | 134/186 [26:12<10:21, 11.95s/it] 73%|███████▎  | 135/186 [26:24<10:06, 11.90s/it] 73%|███████▎  | 136/186 [26:36<09:55, 11.91s/it] 74%|███████▎  | 137/186 [26:48<09:50, 12.04s/it] 74%|███████▍  | 138/186 [27:00<09:37, 12.03s/it] 75%|███████▍  | 139/186 [27:12<09:32, 12.19s/it] 75%|███████▌  | 140/186 [27:24<09:16, 12.10s/it] 76%|███████▌  | 141/186 [27:36<08:59, 11.98s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1143 > 1048). Running this sequence through the model will result in indexing errors
 76%|███████▋  | 142/186 [27:48<08:50, 12.05s/it] 77%|███████▋  | 143/186 [28:00<08:31, 11.89s/it] 77%|███████▋  | 144/186 [28:12<08:23, 11.98s/it] 78%|███████▊  | 145/186 [28:24<08:11, 11.98s/it] 78%|███████▊  | 146/186 [28:36<07:57, 11.95s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 79%|███████▉  | 147/186 [28:48<07:45, 11.94s/it]WARNING: tokenization mismatch: 1 vs. 48. (ignored)
 80%|███████▉  | 148/186 [28:59<07:30, 11.85s/it] 80%|████████  | 149/186 [29:11<07:18, 11.84s/it] 81%|████████  | 150/186 [29:23<07:01, 11.72s/it] 81%|████████  | 151/186 [29:34<06:49, 11.69s/it] 82%|████████▏ | 152/186 [29:46<06:35, 11.64s/it] 82%|████████▏ | 153/186 [29:57<06:22, 11.60s/it] 83%|████████▎ | 154/186 [30:09<06:10, 11.57s/it] 83%|████████▎ | 155/186 [30:20<05:57, 11.52s/it] 84%|████████▍ | 156/186 [30:33<05:53, 11.80s/it] 84%|████████▍ | 157/186 [30:44<05:40, 11.72s/it] 85%|████████▍ | 158/186 [30:55<05:24, 11.61s/it] 85%|████████▌ | 159/186 [31:07<05:13, 11.62s/it] 86%|████████▌ | 160/186 [31:19<05:03, 11.68s/it] 87%|████████▋ | 161/186 [31:30<04:50, 11.62s/it] 87%|████████▋ | 162/186 [31:42<04:39, 11.65s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 88%|████████▊ | 163/186 [31:54<04:27, 11.63s/it] 88%|████████▊ | 164/186 [32:05<04:16, 11.66s/it] 89%|████████▊ | 165/186 [32:17<04:03, 11.57s/it] 89%|████████▉ | 166/186 [32:28<03:51, 11.57s/it] 90%|████████▉ | 167/186 [32:40<03:38, 11.53s/it] 90%|█████████ | 168/186 [32:51<03:27, 11.51s/it] 91%|█████████ | 169/186 [33:03<03:18, 11.65s/it] 91%|█████████▏| 170/186 [33:15<03:05, 11.56s/it] 92%|█████████▏| 171/186 [33:26<02:54, 11.60s/it] 92%|█████████▏| 172/186 [33:38<02:41, 11.56s/it] 93%|█████████▎| 173/186 [33:49<02:29, 11.50s/it] 94%|█████████▎| 174/186 [34:01<02:18, 11.54s/it] 94%|█████████▍| 175/186 [34:13<02:07, 11.60s/it] 95%|█████████▍| 176/186 [34:24<01:55, 11.55s/it] 95%|█████████▌| 177/186 [34:36<01:44, 11.62s/it] 96%|█████████▌| 178/186 [34:47<01:33, 11.65s/it] 96%|█████████▌| 179/186 [34:59<01:21, 11.67s/it] 97%|█████████▋| 180/186 [35:11<01:09, 11.64s/it] 97%|█████████▋| 181/186 [35:22<00:58, 11.62s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 98%|█████████▊| 182/186 [35:34<00:46, 11.75s/it] 98%|█████████▊| 183/186 [35:46<00:35, 11.69s/it] 99%|█████████▉| 184/186 [35:58<00:23, 11.67s/it] 99%|█████████▉| 185/186 [36:09<00:11, 11.71s/it]100%|██████████| 186/186 [36:21<00:00, 11.71s/it]                                                 {'train_runtime': 2181.5563, 'train_samples_per_second': 6.158, 'train_steps_per_second': 0.085, 'train_loss': 0.7719545261834257, 'epoch': 1.0}
100%|██████████| 186/186 [36:22<00:00, 11.71s/it]100%|██████████| 186/186 [36:22<00:00, 11.73s/it]
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.7-0
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33mdirect-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.7-0[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251014_234636-qr51ub9h/logs[0m
[2025-10-15 00:24:48,071] [INFO] [launch.py:347:main] Process 479998 exits successfully.
[2025-10-15 00:24:48,071] [INFO] [launch.py:347:main] Process 480001 exits successfully.
[2025-10-15 00:24:49,073] [INFO] [launch.py:347:main] Process 479999 exits successfully.
[2025-10-15 00:24:49,073] [INFO] [launch.py:347:main] Process 480000 exits successfully.
[2025-10-15 00:24:49,073] [INFO] [launch.py:347:main] Process 479997 exits successfully.
[2025-10-15 00:24:55,080] [INFO] [launch.py:347:main] Process 479996 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-15 00:24:58,276] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 00:24:58,832] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-15 00:24:58,832] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12354 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.3-0 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.3-0 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 0.3 --use_anchor False --yilin_anchor True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-15 00:25:00,240] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 00:25:00,811] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-15 00:25:00,811] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-15 00:25:00,811] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-15 00:25:00,811] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-15 00:25:00,811] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-15 00:25:04,380] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 00:25:04,499] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-15 00:25:04,656] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 00:25:04,699] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-15 00:25:04,699] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 00:25:04,958] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 00:25:05,071] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-15 00:25:05,165] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 00:25:05,487] [INFO] [comm.py:637:init_distributed] cdb=None
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
[2025-10-15 00:25:05,630] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 00:25:06,197] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-15 00:25:06,333] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 00:25:06,957] [INFO] [comm.py:637:init_distributed] cdb=None
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: creating run
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251015_002507-lecho4v0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.3-0
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/lecho4v0
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.82s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.71s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.55s/it]
to bfloat16...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.94s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.75s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.08s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.81s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.65s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.76s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.95s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.47s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.80s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.89s/it]
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.38s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.47s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.21s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.28s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.84s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.24s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.58s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.95s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.23s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.54s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.58s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.90s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.24s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.37s/it]
Formatting inputs...Skip in lazy mode
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.012060880661010742 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.009767293930053711 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.010689496994018555 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.013756513595581055 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10182857513427734 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.1025686264038086 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Token indices sequence length is longer than the specified maximum sequence length for this model (1069 > 1048). Running this sequence through the model will result in indexing errors
  1%|          | 1/186 [00:09<28:22,  9.20s/it]  1%|          | 2/186 [00:18<29:14,  9.53s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1263 > 1048). Running this sequence through the model will result in indexing errors
  2%|▏         | 3/186 [00:29<30:26,  9.98s/it]  2%|▏         | 4/186 [00:39<30:28, 10.05s/it]  3%|▎         | 5/186 [00:50<30:58, 10.27s/it]  3%|▎         | 6/186 [01:00<31:12, 10.40s/it]  4%|▍         | 7/186 [01:11<31:16, 10.48s/it]  4%|▍         | 8/186 [01:22<31:12, 10.52s/it]  5%|▍         | 9/186 [01:32<31:08, 10.56s/it]  5%|▌         | 10/186 [01:43<30:54, 10.54s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
  6%|▌         | 11/186 [01:54<31:03, 10.65s/it]WARNING: tokenization mismatch: 1 vs. 48. (ignored)
  6%|▋         | 12/186 [02:04<30:56, 10.67s/it]  7%|▋         | 13/186 [02:15<30:54, 10.72s/it]  8%|▊         | 14/186 [02:26<30:39, 10.69s/it]  8%|▊         | 15/186 [02:37<30:22, 10.66s/it]  9%|▊         | 16/186 [02:48<30:43, 10.85s/it]  9%|▉         | 17/186 [02:59<31:14, 11.09s/it] 10%|▉         | 18/186 [03:11<31:33, 11.27s/it] 10%|█         | 19/186 [03:23<31:34, 11.35s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 11%|█         | 20/186 [03:34<31:28, 11.38s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1245 > 1048). Running this sequence through the model will result in indexing errors
 11%|█▏        | 21/186 [03:46<31:18, 11.39s/it] 12%|█▏        | 22/186 [03:57<31:24, 11.49s/it] 12%|█▏        | 23/186 [04:09<31:17, 11.52s/it] 13%|█▎        | 24/186 [04:21<31:21, 11.61s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1143 > 1048). Running this sequence through the model will result in indexing errors
 13%|█▎        | 25/186 [04:32<31:10, 11.62s/it] 14%|█▍        | 26/186 [04:44<30:55, 11.60s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1285 > 1048). Running this sequence through the model will result in indexing errors
 15%|█▍        | 27/186 [04:56<31:03, 11.72s/it] 15%|█▌        | 28/186 [05:07<30:46, 11.69s/it] 16%|█▌        | 29/186 [05:19<30:30, 11.66s/it] 16%|█▌        | 30/186 [05:31<30:12, 11.62s/it] 17%|█▋        | 31/186 [05:42<29:52, 11.57s/it] 17%|█▋        | 32/186 [05:54<29:42, 11.58s/it] 18%|█▊        | 33/186 [06:05<29:41, 11.64s/it] 18%|█▊        | 34/186 [06:17<29:32, 11.66s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 47. (ignored)
 19%|█▉        | 35/186 [06:29<29:23, 11.68s/it] 19%|█▉        | 36/186 [06:41<29:21, 11.74s/it] 20%|█▉        | 37/186 [06:52<28:54, 11.64s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 20%|██        | 38/186 [07:04<28:38, 11.61s/it] 21%|██        | 39/186 [07:15<28:26, 11.61s/it] 22%|██▏       | 40/186 [07:27<28:10, 11.58s/it] 22%|██▏       | 41/186 [07:38<28:03, 11.61s/it] 23%|██▎       | 42/186 [07:50<27:48, 11.59s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 23%|██▎       | 43/186 [08:02<27:33, 11.57s/it] 24%|██▎       | 44/186 [08:13<27:19, 11.55s/it] 24%|██▍       | 45/186 [08:25<27:08, 11.55s/it] 25%|██▍       | 46/186 [08:36<26:54, 11.53s/it] 25%|██▌       | 47/186 [08:48<26:54, 11.62s/it] 26%|██▌       | 48/186 [08:59<26:37, 11.57s/it] 26%|██▋       | 49/186 [09:11<26:18, 11.52s/it] 27%|██▋       | 50/186 [09:22<26:09, 11.54s/it] 27%|██▋       | 51/186 [09:34<25:58, 11.54s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 28%|██▊       | 52/186 [09:45<25:46, 11.54s/it] 28%|██▊       | 53/186 [09:57<25:33, 11.53s/it]WARNING: tokenization mismatch: 1 vs. 47. (ignored)
 29%|██▉       | 54/186 [10:09<25:30, 11.60s/it] 30%|██▉       | 55/186 [10:20<25:14, 11.56s/it] 30%|███       | 56/186 [10:32<25:03, 11.57s/it] 31%|███       | 57/186 [10:43<24:52, 11.57s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 31%|███       | 58/186 [10:55<24:43, 11.59s/it] 32%|███▏      | 59/186 [11:07<24:42, 11.67s/it] 32%|███▏      | 60/186 [11:18<24:23, 11.61s/it] 33%|███▎      | 61/186 [11:30<24:12, 11.62s/it] 33%|███▎      | 62/186 [11:41<23:57, 11.59s/it] 34%|███▍      | 63/186 [11:53<23:46, 11.60s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 34%|███▍      | 64/186 [12:05<23:33, 11.58s/it] 35%|███▍      | 65/186 [12:16<23:30, 11.65s/it] 35%|███▌      | 66/186 [12:28<23:28, 11.74s/it] 36%|███▌      | 67/186 [12:40<23:14, 11.72s/it] 37%|███▋      | 68/186 [12:52<22:58, 11.68s/it] 37%|███▋      | 69/186 [13:04<22:57, 11.77s/it] 38%|███▊      | 70/186 [13:15<22:44, 11.76s/it] 38%|███▊      | 71/186 [13:27<22:24, 11.69s/it] 39%|███▊      | 72/186 [13:38<22:07, 11.64s/it] 39%|███▉      | 73/186 [13:50<21:53, 11.62s/it] 40%|███▉      | 74/186 [14:01<21:34, 11.56s/it] 40%|████      | 75/186 [14:13<21:27, 11.60s/it] 41%|████      | 76/186 [14:25<21:18, 11.62s/it] 41%|████▏     | 77/186 [14:36<21:02, 11.58s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 42%|████▏     | 78/186 [14:48<20:46, 11.54s/it] 42%|████▏     | 79/186 [14:59<20:30, 11.50s/it] 43%|████▎     | 80/186 [15:11<20:24, 11.56s/it] 44%|████▎     | 81/186 [15:23<20:18, 11.61s/it] 44%|████▍     | 82/186 [15:34<20:06, 11.60s/it] 45%|████▍     | 83/186 [15:46<20:09, 11.74s/it] 45%|████▌     | 84/186 [15:58<20:04, 11.81s/it] 46%|████▌     | 85/186 [16:10<19:57, 11.86s/it] 46%|████▌     | 86/186 [16:22<19:40, 11.80s/it] 47%|████▋     | 87/186 [16:33<19:22, 11.74s/it] 47%|████▋     | 88/186 [16:45<19:03, 11.67s/it] 48%|████▊     | 89/186 [16:57<18:57, 11.72s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1075 > 1048). Running this sequence through the model will result in indexing errors
 48%|████▊     | 90/186 [17:08<18:46, 11.73s/it] 49%|████▉     | 91/186 [17:20<18:37, 11.76s/it] 49%|████▉     | 92/186 [17:32<18:23, 11.74s/it] 50%|█████     | 93/186 [17:45<18:39, 12.03s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 51%|█████     | 94/186 [17:56<18:18, 11.94s/it] 51%|█████     | 95/186 [18:08<18:08, 11.96s/it] 52%|█████▏    | 96/186 [18:20<17:46, 11.85s/it] 52%|█████▏    | 97/186 [18:32<17:25, 11.75s/it] 53%|█████▎    | 98/186 [18:43<17:04, 11.64s/it] 53%|█████▎    | 99/186 [18:54<16:47, 11.58s/it] 54%|█████▍    | 100/186 [19:06<16:35, 11.58s/it] 54%|█████▍    | 101/186 [19:17<16:21, 11.55s/it] 55%|█████▍    | 102/186 [19:29<16:13, 11.59s/it] 55%|█████▌    | 103/186 [19:41<16:09, 11.68s/it] 56%|█████▌    | 104/186 [19:53<15:55, 11.65s/it] 56%|█████▋    | 105/186 [20:04<15:42, 11.63s/it] 57%|█████▋    | 106/186 [20:16<15:25, 11.57s/it] 58%|█████▊    | 107/186 [20:27<15:20, 11.65s/it] 58%|█████▊    | 108/186 [20:39<15:07, 11.63s/it] 59%|█████▊    | 109/186 [20:51<14:52, 11.60s/it] 59%|█████▉    | 110/186 [21:02<14:38, 11.55s/it] 60%|█████▉    | 111/186 [21:14<14:31, 11.63s/it] 60%|██████    | 112/186 [21:25<14:17, 11.59s/it] 61%|██████    | 113/186 [21:37<14:04, 11.56s/it] 61%|██████▏   | 114/186 [21:48<13:53, 11.57s/it] 62%|██████▏   | 115/186 [22:00<13:45, 11.63s/it] 62%|██████▏   | 116/186 [22:12<13:37, 11.68s/it] 63%|██████▎   | 117/186 [22:24<13:28, 11.71s/it] 63%|██████▎   | 118/186 [22:35<13:16, 11.72s/it] 64%|██████▍   | 119/186 [22:47<12:58, 11.62s/it] 65%|██████▍   | 120/186 [22:59<12:47, 11.62s/it] 65%|██████▌   | 121/186 [23:10<12:39, 11.68s/it] 66%|██████▌   | 122/186 [23:22<12:22, 11.60s/it] 66%|██████▌   | 123/186 [23:34<12:15, 11.67s/it] 67%|██████▋   | 124/186 [23:45<12:00, 11.62s/it] 67%|██████▋   | 125/186 [23:57<11:47, 11.60s/it] 68%|██████▊   | 126/186 [24:08<11:34, 11.57s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 68%|██████▊   | 127/186 [24:20<11:23, 11.58s/it] 69%|██████▉   | 128/186 [24:31<11:10, 11.56s/it] 69%|██████▉   | 129/186 [24:43<11:01, 11.61s/it] 70%|██████▉   | 130/186 [24:54<10:48, 11.58s/it] 70%|███████   | 131/186 [25:07<10:45, 11.74s/it] 71%|███████   | 132/186 [25:19<10:39, 11.84s/it] 72%|███████▏  | 133/186 [25:30<10:25, 11.80s/it] 72%|███████▏  | 134/186 [25:42<10:14, 11.82s/it] 73%|███████▎  | 135/186 [25:54<09:57, 11.72s/it] 73%|███████▎  | 136/186 [26:05<09:45, 11.72s/it] 74%|███████▎  | 137/186 [26:17<09:34, 11.73s/it] 74%|███████▍  | 138/186 [26:29<09:18, 11.64s/it] 75%|███████▍  | 139/186 [26:41<09:11, 11.74s/it] 75%|███████▌  | 140/186 [26:52<08:57, 11.69s/it] 76%|███████▌  | 141/186 [27:04<08:47, 11.72s/it] 76%|███████▋  | 142/186 [27:15<08:32, 11.66s/it] 77%|███████▋  | 143/186 [27:27<08:23, 11.71s/it] 77%|███████▋  | 144/186 [27:39<08:09, 11.65s/it] 78%|███████▊  | 145/186 [27:50<07:55, 11.59s/it] 78%|███████▊  | 146/186 [28:02<07:41, 11.54s/it] 79%|███████▉  | 147/186 [28:14<07:34, 11.65s/it] 80%|███████▉  | 148/186 [28:25<07:20, 11.59s/it] 80%|████████  | 149/186 [28:37<07:10, 11.62s/it] 81%|████████  | 150/186 [28:48<06:59, 11.64s/it] 81%|████████  | 151/186 [29:00<06:47, 11.64s/it] 82%|████████▏ | 152/186 [29:11<06:33, 11.57s/it] 82%|████████▏ | 153/186 [29:23<06:21, 11.57s/it] 83%|████████▎ | 154/186 [29:35<06:10, 11.59s/it] 83%|████████▎ | 155/186 [29:46<05:58, 11.57s/it] 84%|████████▍ | 156/186 [29:58<05:52, 11.74s/it] 84%|████████▍ | 157/186 [30:10<05:39, 11.69s/it] 85%|████████▍ | 158/186 [30:22<05:28, 11.73s/it] 85%|████████▌ | 159/186 [30:34<05:17, 11.76s/it] 86%|████████▌ | 160/186 [30:45<05:04, 11.72s/it] 87%|████████▋ | 161/186 [30:57<04:52, 11.70s/it] 87%|████████▋ | 162/186 [31:09<04:41, 11.72s/it] 88%|████████▊ | 163/186 [31:21<04:31, 11.79s/it] 88%|████████▊ | 164/186 [31:32<04:18, 11.74s/it] 89%|████████▊ | 165/186 [31:44<04:04, 11.65s/it] 89%|████████▉ | 166/186 [31:55<03:53, 11.66s/it] 90%|████████▉ | 167/186 [32:07<03:41, 11.65s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
 90%|█████████ | 168/186 [32:18<03:28, 11.58s/it] 91%|█████████ | 169/186 [32:30<03:16, 11.56s/it] 91%|█████████▏| 170/186 [32:41<03:04, 11.54s/it] 92%|█████████▏| 171/186 [32:53<02:54, 11.66s/it] 92%|█████████▏| 172/186 [33:05<02:43, 11.69s/it] 93%|█████████▎| 173/186 [33:17<02:31, 11.64s/it] 94%|█████████▎| 174/186 [33:28<02:20, 11.68s/it] 94%|█████████▍| 175/186 [33:41<02:10, 11.82s/it] 95%|█████████▍| 176/186 [33:52<01:57, 11.71s/it] 95%|█████████▌| 177/186 [34:04<01:45, 11.74s/it] 96%|█████████▌| 178/186 [34:16<01:34, 11.77s/it] 96%|█████████▌| 179/186 [34:27<01:21, 11.69s/it] 97%|█████████▋| 180/186 [34:38<01:09, 11.59s/it] 97%|█████████▋| 181/186 [34:50<00:58, 11.60s/it] 98%|█████████▊| 182/186 [35:02<00:46, 11.54s/it] 98%|█████████▊| 183/186 [35:13<00:34, 11.54s/it] 99%|█████████▉| 184/186 [35:24<00:22, 11.47s/it] 99%|█████████▉| 185/186 [35:37<00:11, 11.68s/it]100%|██████████| 186/186 [35:48<00:00, 11.65s/it]                                                 {'train_runtime': 2148.6172, 'train_samples_per_second': 6.253, 'train_steps_per_second': 0.087, 'train_loss': 0.7831944291309644, 'epoch': 1.0}
100%|██████████| 186/186 [35:49<00:00, 11.65s/it]100%|██████████| 186/186 [35:49<00:00, 11.56s/it]
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.3-0
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33mdirect-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.3-0[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251015_002507-lecho4v0/logs[0m
[2025-10-15 01:02:40,419] [INFO] [launch.py:347:main] Process 481750 exits successfully.
[2025-10-15 01:02:40,419] [INFO] [launch.py:347:main] Process 481752 exits successfully.
[2025-10-15 01:02:40,419] [INFO] [launch.py:347:main] Process 481749 exits successfully.
[2025-10-15 01:02:40,420] [INFO] [launch.py:347:main] Process 481753 exits successfully.
[2025-10-15 01:02:40,420] [INFO] [launch.py:347:main] Process 481751 exits successfully.
[2025-10-15 01:02:45,425] [INFO] [launch.py:347:main] Process 481748 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-15 01:02:48,629] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 01:02:49,186] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-15 01:02:49,186] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12354 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-0 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-0 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 1 --use_anchor False --yilin_anchor True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-15 01:02:50,649] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 01:02:51,228] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-15 01:02:51,228] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-15 01:02:51,228] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-15 01:02:51,228] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-15 01:02:51,228] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-15 01:02:54,917] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 01:02:55,057] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 01:02:55,255] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-15 01:02:55,356] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 01:02:55,375] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-15 01:02:55,375] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-10-15 01:02:55,446] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 01:02:55,678] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-15 01:02:55,732] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 01:02:55,761] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 01:02:56,070] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-10-15 01:02:56,595] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 01:02:57,146] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251015_010257-ia5z9mxy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-0
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/ia5z9mxy
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.09s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.05s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.65s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.88s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.20s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.96s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.93s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.97s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.54s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.91s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.55s/it]to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.48s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.85s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.97s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.86s/it]
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.10s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.16s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.22s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.34s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.52s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.92s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.88s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.53s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.83s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.72s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.05s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.67s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.00s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.66s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.77s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.05s/it]
Formatting inputs...Skip in lazy mode
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.011192560195922852 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.011168718338012695 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.012785673141479492 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10109305381774902 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.1011042594909668 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10300683975219727 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/186 [00:08<26:44,  8.67s/it]  1%|          | 2/186 [00:18<28:26,  9.28s/it]  2%|▏         | 3/186 [00:28<29:23,  9.64s/it]  2%|▏         | 4/186 [00:38<29:58,  9.88s/it]  3%|▎         | 5/186 [00:49<30:16, 10.04s/it]  3%|▎         | 6/186 [00:59<30:39, 10.22s/it]  4%|▍         | 7/186 [01:10<30:59, 10.39s/it]  4%|▍         | 8/186 [01:21<31:06, 10.49s/it]  5%|▍         | 9/186 [01:31<31:05, 10.54s/it]  5%|▌         | 10/186 [01:42<31:14, 10.65s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1165 > 1048). Running this sequence through the model will result in indexing errors
  6%|▌         | 11/186 [01:54<32:06, 11.01s/it]  6%|▋         | 12/186 [02:05<32:16, 11.13s/it]  7%|▋         | 13/186 [02:17<32:23, 11.23s/it]  8%|▊         | 14/186 [02:28<32:28, 11.33s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1263 > 1048). Running this sequence through the model will result in indexing errors
  8%|▊         | 15/186 [02:40<32:52, 11.53s/it]  9%|▊         | 16/186 [02:52<32:36, 11.51s/it]  9%|▉         | 17/186 [03:03<32:22, 11.50s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1200 > 1048). Running this sequence through the model will result in indexing errors
 10%|▉         | 18/186 [03:15<32:38, 11.66s/it] 10%|█         | 19/186 [03:27<32:47, 11.78s/it] 11%|█         | 20/186 [03:39<32:45, 11.84s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1240 > 1048). Running this sequence through the model will result in indexing errors
 11%|█▏        | 21/186 [03:51<32:38, 11.87s/it] 12%|█▏        | 22/186 [04:03<32:12, 11.79s/it] 12%|█▏        | 23/186 [04:14<31:45, 11.69s/it] 13%|█▎        | 24/186 [04:26<31:45, 11.76s/it] 13%|█▎        | 25/186 [04:38<31:21, 11.69s/it] 14%|█▍        | 26/186 [04:49<31:03, 11.65s/it] 15%|█▍        | 27/186 [05:01<31:04, 11.73s/it] 15%|█▌        | 28/186 [05:13<30:40, 11.65s/it] 16%|█▌        | 29/186 [05:24<30:17, 11.58s/it] 16%|█▌        | 30/186 [05:36<30:12, 11.62s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
 17%|█▋        | 31/186 [05:48<30:04, 11.64s/it] 17%|█▋        | 32/186 [05:59<29:57, 11.67s/it] 18%|█▊        | 33/186 [06:11<29:41, 11.64s/it] 18%|█▊        | 34/186 [06:22<29:20, 11.58s/it] 19%|█▉        | 35/186 [06:34<29:18, 11.64s/it] 19%|█▉        | 36/186 [06:46<28:58, 11.59s/it] 20%|█▉        | 37/186 [06:57<28:48, 11.60s/it] 20%|██        | 38/186 [07:09<28:25, 11.53s/it] 21%|██        | 39/186 [07:20<28:18, 11.56s/it] 22%|██▏       | 40/186 [07:32<28:13, 11.60s/it] 22%|██▏       | 41/186 [07:43<28:01, 11.60s/it] 23%|██▎       | 42/186 [07:55<27:42, 11.54s/it] 23%|██▎       | 43/186 [08:06<27:30, 11.54s/it] 24%|██▎       | 44/186 [08:18<27:20, 11.55s/it] 24%|██▍       | 45/186 [08:29<27:04, 11.52s/it] 25%|██▍       | 46/186 [08:41<27:10, 11.65s/it] 25%|██▌       | 47/186 [08:53<27:02, 11.67s/it] 26%|██▌       | 48/186 [09:05<26:41, 11.61s/it] 26%|██▋       | 49/186 [09:16<26:26, 11.58s/it] 27%|██▋       | 50/186 [09:28<26:10, 11.55s/it] 27%|██▋       | 51/186 [09:39<25:54, 11.51s/it] 28%|██▊       | 52/186 [09:51<25:48, 11.56s/it] 28%|██▊       | 53/186 [10:02<25:43, 11.60s/it] 29%|██▉       | 54/186 [10:14<25:30, 11.59s/it] 30%|██▉       | 55/186 [10:25<25:09, 11.52s/it] 30%|███       | 56/186 [10:37<24:55, 11.51s/it] 31%|███       | 57/186 [10:49<24:58, 11.62s/it] 31%|███       | 58/186 [11:00<24:45, 11.60s/it] 32%|███▏      | 59/186 [11:12<24:44, 11.69s/it] 32%|███▏      | 60/186 [11:24<24:42, 11.77s/it] 33%|███▎      | 61/186 [11:36<24:24, 11.72s/it] 33%|███▎      | 62/186 [11:47<24:12, 11.71s/it] 34%|███▍      | 63/186 [11:59<23:56, 11.68s/it] 34%|███▍      | 64/186 [12:10<23:36, 11.61s/it] 35%|███▍      | 65/186 [12:22<23:19, 11.57s/it] 35%|███▌      | 66/186 [12:33<23:03, 11.53s/it] 36%|███▌      | 67/186 [12:45<22:52, 11.53s/it] 37%|███▋      | 68/186 [12:56<22:37, 11.50s/it] 37%|███▋      | 69/186 [13:08<22:51, 11.72s/it] 38%|███▊      | 70/186 [13:20<22:46, 11.78s/it] 38%|███▊      | 71/186 [13:32<22:29, 11.73s/it] 39%|███▊      | 72/186 [13:44<22:10, 11.67s/it] 39%|███▉      | 73/186 [13:55<21:50, 11.60s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 40%|███▉      | 74/186 [14:07<21:37, 11.59s/it] 40%|████      | 75/186 [14:18<21:25, 11.58s/it] 41%|████      | 76/186 [14:30<21:22, 11.66s/it] 41%|████▏     | 77/186 [14:42<21:13, 11.68s/it] 42%|████▏     | 78/186 [14:53<21:00, 11.67s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 42%|████▏     | 79/186 [15:05<20:47, 11.66s/it] 43%|████▎     | 80/186 [15:17<20:41, 11.71s/it] 44%|████▎     | 81/186 [15:29<20:34, 11.76s/it] 44%|████▍     | 82/186 [15:40<20:13, 11.67s/it] 45%|████▍     | 83/186 [15:52<20:01, 11.66s/it] 45%|████▌     | 84/186 [16:03<19:48, 11.65s/it] 46%|████▌     | 85/186 [16:15<19:45, 11.74s/it] 46%|████▌     | 86/186 [16:27<19:45, 11.85s/it] 47%|████▋     | 87/186 [16:39<19:30, 11.82s/it] 47%|████▋     | 88/186 [16:51<19:24, 11.88s/it] 48%|████▊     | 89/186 [17:03<19:05, 11.81s/it] 48%|████▊     | 90/186 [17:15<18:50, 11.77s/it] 49%|████▉     | 91/186 [17:27<18:51, 11.91s/it] 49%|████▉     | 92/186 [17:39<18:49, 12.02s/it] 50%|█████     | 93/186 [17:51<18:34, 11.98s/it] 51%|█████     | 94/186 [18:02<18:10, 11.85s/it] 51%|█████     | 95/186 [18:14<17:55, 11.82s/it] 52%|█████▏    | 96/186 [18:26<17:33, 11.70s/it] 52%|█████▏    | 97/186 [18:37<17:18, 11.67s/it] 53%|█████▎    | 98/186 [18:49<17:06, 11.67s/it] 53%|█████▎    | 99/186 [19:00<16:50, 11.62s/it] 54%|█████▍    | 100/186 [19:12<16:43, 11.67s/it] 54%|█████▍    | 101/186 [19:24<16:30, 11.65s/it] 55%|█████▍    | 102/186 [19:35<16:16, 11.62s/it] 55%|█████▌    | 103/186 [19:47<16:14, 11.75s/it] 56%|█████▌    | 104/186 [19:59<15:55, 11.65s/it] 56%|█████▋    | 105/186 [20:10<15:40, 11.61s/it] 57%|█████▋    | 106/186 [20:22<15:31, 11.64s/it] 58%|█████▊    | 107/186 [20:34<15:15, 11.59s/it] 58%|█████▊    | 108/186 [20:45<15:04, 11.60s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 59%|█████▊    | 109/186 [20:57<14:56, 11.64s/it] 59%|█████▉    | 110/186 [21:09<14:45, 11.65s/it]WARNING: tokenization mismatch: 1 vs. 47. (ignored)
 60%|█████▉    | 111/186 [21:20<14:28, 11.58s/it] 60%|██████    | 112/186 [21:32<14:20, 11.63s/it] 61%|██████    | 113/186 [21:43<14:08, 11.63s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1125 > 1048). Running this sequence through the model will result in indexing errors
 61%|██████▏   | 114/186 [21:55<14:03, 11.71s/it] 62%|██████▏   | 115/186 [22:07<13:43, 11.60s/it] 62%|██████▏   | 116/186 [22:18<13:34, 11.64s/it] 63%|██████▎   | 117/186 [22:30<13:19, 11.58s/it] 63%|██████▎   | 118/186 [22:41<13:04, 11.54s/it] 64%|██████▍   | 119/186 [22:53<12:57, 11.60s/it] 65%|██████▍   | 120/186 [23:05<12:50, 11.67s/it] 65%|██████▌   | 121/186 [23:16<12:36, 11.63s/it] 66%|██████▌   | 122/186 [23:28<12:28, 11.70s/it] 66%|██████▌   | 123/186 [23:40<12:23, 11.80s/it] 67%|██████▋   | 124/186 [23:52<12:07, 11.73s/it] 67%|██████▋   | 125/186 [24:03<11:54, 11.72s/it] 68%|██████▊   | 126/186 [24:15<11:39, 11.66s/it] 68%|██████▊   | 127/186 [24:26<11:24, 11.59s/it] 69%|██████▉   | 128/186 [24:38<11:13, 11.61s/it] 69%|██████▉   | 129/186 [24:50<10:59, 11.57s/it] 70%|██████▉   | 130/186 [25:01<10:48, 11.58s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 70%|███████   | 131/186 [25:13<10:36, 11.57s/it] 71%|███████   | 132/186 [25:24<10:23, 11.55s/it] 72%|███████▏  | 133/186 [25:36<10:08, 11.49s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 72%|███████▏  | 134/186 [25:47<09:59, 11.53s/it] 73%|███████▎  | 135/186 [25:59<09:48, 11.53s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 73%|███████▎  | 136/186 [26:10<09:38, 11.56s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 74%|███████▎  | 137/186 [26:22<09:34, 11.72s/it] 74%|███████▍  | 138/186 [26:34<09:18, 11.64s/it] 75%|███████▍  | 139/186 [26:46<09:13, 11.77s/it] 75%|███████▌  | 140/186 [26:57<08:57, 11.68s/it] 76%|███████▌  | 141/186 [27:09<08:45, 11.67s/it] 76%|███████▋  | 142/186 [27:21<08:34, 11.69s/it] 77%|███████▋  | 143/186 [27:32<08:20, 11.64s/it] 77%|███████▋  | 144/186 [27:44<08:06, 11.58s/it] 78%|███████▊  | 145/186 [27:56<07:58, 11.67s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1263 > 1048). Running this sequence through the model will result in indexing errors
 78%|███████▊  | 146/186 [28:07<07:47, 11.70s/it] 79%|███████▉  | 147/186 [28:19<07:37, 11.72s/it] 80%|███████▉  | 148/186 [28:31<07:21, 11.62s/it] 80%|████████  | 149/186 [28:42<07:09, 11.59s/it] 81%|████████  | 150/186 [28:54<06:55, 11.55s/it] 81%|████████  | 151/186 [29:05<06:45, 11.59s/it] 82%|████████▏ | 152/186 [29:17<06:31, 11.51s/it] 82%|████████▏ | 153/186 [29:28<06:18, 11.48s/it] 83%|████████▎ | 154/186 [29:40<06:08, 11.53s/it] 83%|████████▎ | 155/186 [29:51<05:56, 11.49s/it] 84%|████████▍ | 156/186 [30:03<05:47, 11.58s/it] 84%|████████▍ | 157/186 [30:14<05:34, 11.55s/it] 85%|████████▍ | 158/186 [30:26<05:22, 11.52s/it] 85%|████████▌ | 159/186 [30:38<05:13, 11.62s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
 86%|████████▌ | 160/186 [30:50<05:08, 11.88s/it] 87%|████████▋ | 161/186 [31:02<04:54, 11.79s/it] 87%|████████▋ | 162/186 [31:14<04:45, 11.88s/it] 88%|████████▊ | 163/186 [31:26<04:34, 11.94s/it] 88%|████████▊ | 164/186 [31:37<04:20, 11.82s/it] 89%|████████▊ | 165/186 [31:49<04:09, 11.86s/it] 89%|████████▉ | 166/186 [32:01<03:53, 11.70s/it] 90%|████████▉ | 167/186 [32:13<03:42, 11.73s/it] 90%|█████████ | 168/186 [32:24<03:29, 11.66s/it] 91%|█████████ | 169/186 [32:36<03:17, 11.63s/it] 91%|█████████▏| 170/186 [32:47<03:06, 11.63s/it] 92%|█████████▏| 171/186 [32:59<02:54, 11.65s/it] 92%|█████████▏| 172/186 [33:11<02:44, 11.74s/it] 93%|█████████▎| 173/186 [33:22<02:31, 11.67s/it] 94%|█████████▎| 174/186 [33:34<02:20, 11.69s/it] 94%|█████████▍| 175/186 [33:46<02:08, 11.69s/it] 95%|█████████▍| 176/186 [33:57<01:56, 11.66s/it] 95%|█████████▌| 177/186 [34:09<01:44, 11.60s/it] 96%|█████████▌| 178/186 [34:20<01:32, 11.60s/it] 96%|█████████▌| 179/186 [34:32<01:21, 11.59s/it] 97%|█████████▋| 180/186 [34:44<01:09, 11.59s/it] 97%|█████████▋| 181/186 [34:55<00:57, 11.55s/it] 98%|█████████▊| 182/186 [35:07<00:46, 11.58s/it] 98%|█████████▊| 183/186 [35:18<00:34, 11.64s/it] 99%|█████████▉| 184/186 [35:30<00:23, 11.62s/it] 99%|█████████▉| 185/186 [35:42<00:11, 11.72s/it]100%|██████████| 186/186 [35:54<00:00, 11.79s/it]                                                 {'train_runtime': 2154.4425, 'train_samples_per_second': 6.236, 'train_steps_per_second': 0.086, 'train_loss': 0.7789481378370716, 'epoch': 1.0}
100%|██████████| 186/186 [35:55<00:00, 11.79s/it]100%|██████████| 186/186 [35:55<00:00, 11.59s/it]
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-0
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33mdirect-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-0[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251015_010257-ia5z9mxy/logs[0m
[2025-10-15 01:40:37,973] [INFO] [launch.py:347:main] Process 483513 exits successfully.
[2025-10-15 01:40:37,974] [INFO] [launch.py:347:main] Process 483515 exits successfully.
[2025-10-15 01:40:37,974] [INFO] [launch.py:347:main] Process 483512 exits successfully.
[2025-10-15 01:40:37,974] [INFO] [launch.py:347:main] Process 483516 exits successfully.
[2025-10-15 01:40:37,974] [INFO] [launch.py:347:main] Process 483514 exits successfully.
[2025-10-15 01:40:42,980] [INFO] [launch.py:347:main] Process 483511 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-15 01:40:46,385] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 01:40:46,944] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-15 01:40:46,944] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12354 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-1 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-1 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 0.5 --use_anchor False --yilin_anchor True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-15 01:40:48,636] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 01:40:49,213] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-15 01:40:49,213] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-15 01:40:49,213] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-15 01:40:49,213] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-15 01:40:49,213] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-15 01:40:52,957] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 01:40:53,280] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-15 01:40:53,280] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-10-15 01:40:53,299] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 01:40:53,625] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-15 01:40:53,643] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 01:40:53,959] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-10-15 01:40:54,163] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-15 01:40:54,727] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 01:40:54,898] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 01:40:54,923] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-10-15 01:40:55,187] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
[2025-10-15 01:40:55,381] [INFO] [comm.py:637:init_distributed] cdb=None
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.78s/it]wandb: creating run
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251015_014055-850mznil
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-1
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/850mznil
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.12s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.07s/it]
to bfloat16...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.80s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.55s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.17s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.99s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.80s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.64s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.67s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.02s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.67s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.99s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.54s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.85s/it]
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.54s/it]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.28s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.62s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.34s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.02s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.53s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.80s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.72s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.07s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.95s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.38s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.62s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.77s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.86s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.67s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.98s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.98s/it]
Formatting inputs...Skip in lazy mode
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...

Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.01367807388305664 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.012076854705810547 seconds
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...Time to load fused_adam op: 0.10113215446472168 seconds

Time to load fused_adam op: 0.1014106273651123 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10104131698608398 seconds
Time to load fused_adam op: 0.10162878036499023 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/186 [00:08<26:53,  8.72s/it]  1%|          | 2/186 [00:18<28:39,  9.34s/it]  2%|▏         | 3/186 [00:28<29:28,  9.66s/it]  2%|▏         | 4/186 [00:38<29:51,  9.84s/it]  3%|▎         | 5/186 [00:49<30:28, 10.10s/it]  3%|▎         | 6/186 [01:00<31:07, 10.37s/it]  4%|▍         | 7/186 [01:10<31:18, 10.49s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1089 > 1048). Running this sequence through the model will result in indexing errors
  4%|▍         | 8/186 [01:21<31:36, 10.66s/it]  5%|▍         | 9/186 [01:32<31:38, 10.73s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1160 > 1048). Running this sequence through the model will result in indexing errors
  5%|▌         | 10/186 [01:43<31:44, 10.82s/it]  6%|▌         | 11/186 [01:54<31:28, 10.79s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
  6%|▋         | 12/186 [02:05<31:04, 10.72s/it]  7%|▋         | 13/186 [02:15<31:03, 10.77s/it]  8%|▊         | 14/186 [02:26<30:46, 10.74s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1150 > 1048). Running this sequence through the model will result in indexing errors
  8%|▊         | 15/186 [02:37<31:03, 10.90s/it]  9%|▊         | 16/186 [02:48<30:46, 10.86s/it]  9%|▉         | 17/186 [02:59<30:32, 10.84s/it] 10%|▉         | 18/186 [03:10<30:34, 10.92s/it] 10%|█         | 19/186 [03:21<30:22, 10.91s/it] 11%|█         | 20/186 [03:32<30:03, 10.86s/it] 11%|█▏        | 21/186 [03:42<29:48, 10.84s/it] 12%|█▏        | 22/186 [03:53<29:33, 10.81s/it] 12%|█▏        | 23/186 [04:04<29:29, 10.85s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1233 > 1048). Running this sequence through the model will result in indexing errors
 13%|█▎        | 24/186 [04:17<30:42, 11.37s/it] 13%|█▎        | 25/186 [04:28<30:43, 11.45s/it] 14%|█▍        | 26/186 [04:40<30:30, 11.44s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1065 > 1048). Running this sequence through the model will result in indexing errors
 15%|█▍        | 27/186 [04:52<30:45, 11.61s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1158 > 1048). Running this sequence through the model will result in indexing errors
 15%|█▌        | 28/186 [05:04<31:00, 11.78s/it] 16%|█▌        | 29/186 [05:15<30:35, 11.69s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 16%|█▌        | 30/186 [05:27<30:18, 11.66s/it] 17%|█▋        | 31/186 [05:39<30:08, 11.67s/it] 17%|█▋        | 32/186 [05:50<29:47, 11.61s/it] 18%|█▊        | 33/186 [06:02<29:25, 11.54s/it] 18%|█▊        | 34/186 [06:13<29:10, 11.52s/it] 19%|█▉        | 35/186 [06:25<29:22, 11.67s/it] 19%|█▉        | 36/186 [06:37<29:03, 11.62s/it] 20%|█▉        | 37/186 [06:48<28:48, 11.60s/it] 20%|██        | 38/186 [07:00<28:39, 11.62s/it] 21%|██        | 39/186 [07:12<28:35, 11.67s/it] 22%|██▏       | 40/186 [07:23<28:23, 11.67s/it] 22%|██▏       | 41/186 [07:35<28:06, 11.63s/it] 23%|██▎       | 42/186 [07:46<27:50, 11.60s/it] 23%|██▎       | 43/186 [07:58<27:35, 11.58s/it] 24%|██▎       | 44/186 [08:10<27:26, 11.60s/it] 24%|██▍       | 45/186 [08:21<27:06, 11.53s/it] 25%|██▍       | 46/186 [08:33<27:12, 11.66s/it] 25%|██▌       | 47/186 [08:45<27:18, 11.79s/it] 26%|██▌       | 48/186 [08:57<26:59, 11.73s/it] 26%|██▋       | 49/186 [09:08<26:43, 11.71s/it] 27%|██▋       | 50/186 [09:20<26:27, 11.67s/it] 27%|██▋       | 51/186 [09:31<26:16, 11.67s/it] 28%|██▊       | 52/186 [09:43<26:00, 11.65s/it] 28%|██▊       | 53/186 [09:55<25:48, 11.64s/it] 29%|██▉       | 54/186 [10:06<25:42, 11.69s/it] 30%|██▉       | 55/186 [10:18<25:22, 11.62s/it] 30%|███       | 56/186 [10:29<25:07, 11.60s/it] 31%|███       | 57/186 [10:41<24:58, 11.62s/it] 31%|███       | 58/186 [10:53<24:40, 11.57s/it] 32%|███▏      | 59/186 [11:05<24:46, 11.70s/it] 32%|███▏      | 60/186 [11:16<24:24, 11.62s/it] 33%|███▎      | 61/186 [11:28<24:15, 11.64s/it] 33%|███▎      | 62/186 [11:39<24:02, 11.63s/it] 34%|███▍      | 63/186 [11:51<23:53, 11.65s/it] 34%|███▍      | 64/186 [12:03<23:48, 11.71s/it] 35%|███▍      | 65/186 [12:14<23:32, 11.67s/it] 35%|███▌      | 66/186 [12:26<23:17, 11.64s/it] 36%|███▌      | 67/186 [12:38<22:58, 11.59s/it] 37%|███▋      | 68/186 [12:49<22:48, 11.60s/it] 37%|███▋      | 69/186 [13:01<22:56, 11.77s/it] 38%|███▊      | 70/186 [13:14<23:01, 11.91s/it] 38%|███▊      | 71/186 [13:25<22:43, 11.85s/it] 39%|███▊      | 72/186 [13:37<22:21, 11.77s/it] 39%|███▉      | 73/186 [13:49<22:08, 11.76s/it] 40%|███▉      | 74/186 [14:00<22:01, 11.80s/it] 40%|████      | 75/186 [14:12<21:35, 11.67s/it] 41%|████      | 76/186 [14:23<21:16, 11.61s/it] 41%|████▏     | 77/186 [14:35<21:01, 11.57s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 42%|████▏     | 78/186 [14:46<20:48, 11.56s/it] 42%|████▏     | 79/186 [14:58<20:47, 11.66s/it] 43%|████▎     | 80/186 [15:10<20:41, 11.71s/it] 44%|████▎     | 81/186 [15:22<20:24, 11.66s/it] 44%|████▍     | 82/186 [15:33<20:06, 11.60s/it] 45%|████▍     | 83/186 [15:45<20:05, 11.71s/it] 45%|████▌     | 84/186 [15:57<19:57, 11.74s/it] 46%|████▌     | 85/186 [16:08<19:41, 11.69s/it] 46%|████▌     | 86/186 [16:20<19:32, 11.73s/it] 47%|████▋     | 87/186 [16:32<19:26, 11.78s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 47%|████▋     | 88/186 [16:44<19:11, 11.75s/it] 48%|████▊     | 89/186 [16:55<18:53, 11.69s/it] 48%|████▊     | 90/186 [17:07<18:35, 11.62s/it] 49%|████▉     | 91/186 [17:19<18:38, 11.78s/it] 49%|████▉     | 92/186 [17:31<18:32, 11.83s/it] 50%|█████     | 93/186 [17:43<18:23, 11.87s/it] 51%|█████     | 94/186 [17:55<18:18, 11.94s/it] 51%|█████     | 95/186 [18:07<18:01, 11.89s/it] 52%|█████▏    | 96/186 [18:19<17:48, 11.87s/it] 52%|█████▏    | 97/186 [18:30<17:33, 11.83s/it] 53%|█████▎    | 98/186 [18:42<17:16, 11.78s/it] 53%|█████▎    | 99/186 [18:53<16:55, 11.67s/it] 54%|█████▍    | 100/186 [19:05<16:38, 11.60s/it] 54%|█████▍    | 101/186 [19:16<16:24, 11.59s/it] 55%|█████▍    | 102/186 [19:28<16:10, 11.56s/it] 55%|█████▌    | 103/186 [19:40<16:11, 11.71s/it] 56%|█████▌    | 104/186 [19:51<15:54, 11.64s/it] 56%|█████▋    | 105/186 [20:03<15:42, 11.63s/it] 57%|█████▋    | 106/186 [20:15<15:31, 11.64s/it] 58%|█████▊    | 107/186 [20:26<15:19, 11.65s/it] 58%|█████▊    | 108/186 [20:38<15:09, 11.66s/it] 59%|█████▊    | 109/186 [20:50<14:54, 11.61s/it] 59%|█████▉    | 110/186 [21:01<14:40, 11.59s/it] 60%|█████▉    | 111/186 [21:13<14:30, 11.60s/it] 60%|██████    | 112/186 [21:24<14:19, 11.62s/it] 61%|██████    | 113/186 [21:36<14:07, 11.60s/it]WARNING: tokenization mismatch: 1 vs. 47. (ignored)
 61%|██████▏   | 114/186 [21:48<14:06, 11.75s/it] 62%|██████▏   | 115/186 [22:00<13:57, 11.80s/it] 62%|██████▏   | 116/186 [22:12<13:47, 11.82s/it] 63%|██████▎   | 117/186 [22:23<13:29, 11.74s/it] 63%|██████▎   | 118/186 [22:35<13:19, 11.75s/it] 64%|██████▍   | 119/186 [22:47<13:02, 11.68s/it] 65%|██████▍   | 120/186 [22:58<12:50, 11.68s/it] 65%|██████▌   | 121/186 [23:10<12:37, 11.65s/it] 66%|██████▌   | 122/186 [23:22<12:32, 11.76s/it] 66%|██████▌   | 123/186 [23:34<12:27, 11.86s/it] 67%|██████▋   | 124/186 [23:46<12:12, 11.81s/it] 67%|██████▋   | 125/186 [23:57<11:58, 11.78s/it] 68%|██████▊   | 126/186 [24:09<11:44, 11.75s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 68%|██████▊   | 127/186 [24:21<11:30, 11.71s/it] 69%|██████▉   | 128/186 [24:32<11:18, 11.69s/it] 69%|██████▉   | 129/186 [24:44<11:08, 11.73s/it] 70%|██████▉   | 130/186 [24:56<10:53, 11.67s/it] 70%|███████   | 131/186 [25:07<10:43, 11.70s/it] 71%|███████   | 132/186 [25:19<10:30, 11.67s/it] 72%|███████▏  | 133/186 [25:31<10:20, 11.70s/it] 72%|███████▏  | 134/186 [25:43<10:11, 11.76s/it] 73%|███████▎  | 135/186 [25:54<09:58, 11.73s/it] 73%|███████▎  | 136/186 [26:06<09:45, 11.70s/it] 74%|███████▎  | 137/186 [26:18<09:42, 11.89s/it] 74%|███████▍  | 138/186 [26:30<09:29, 11.86s/it] 75%|███████▍  | 139/186 [26:42<09:17, 11.87s/it] 75%|███████▌  | 140/186 [26:54<09:00, 11.75s/it] 76%|███████▌  | 141/186 [27:05<08:49, 11.78s/it] 76%|███████▋  | 142/186 [27:17<08:35, 11.72s/it] 77%|███████▋  | 143/186 [27:29<08:25, 11.76s/it] 77%|███████▋  | 144/186 [27:41<08:13, 11.75s/it] 78%|███████▊  | 145/186 [27:52<07:58, 11.68s/it] 78%|███████▊  | 146/186 [28:04<07:48, 11.70s/it] 79%|███████▉  | 147/186 [28:15<07:34, 11.65s/it] 80%|███████▉  | 148/186 [28:27<07:23, 11.67s/it] 80%|████████  | 149/186 [28:38<07:09, 11.60s/it] 81%|████████  | 150/186 [28:50<06:59, 11.66s/it] 81%|████████  | 151/186 [29:02<06:46, 11.62s/it] 82%|████████▏ | 152/186 [29:14<06:37, 11.68s/it] 82%|████████▏ | 153/186 [29:25<06:23, 11.63s/it] 83%|████████▎ | 154/186 [29:36<06:08, 11.53s/it] 83%|████████▎ | 155/186 [29:48<05:57, 11.55s/it] 84%|████████▍ | 156/186 [30:00<05:50, 11.70s/it] 84%|████████▍ | 157/186 [30:12<05:39, 11.71s/it] 85%|████████▍ | 158/186 [30:23<05:25, 11.61s/it] 85%|████████▌ | 159/186 [30:35<05:13, 11.60s/it] 86%|████████▌ | 160/186 [30:46<05:02, 11.64s/it] 87%|████████▋ | 161/186 [30:58<04:48, 11.54s/it] 87%|████████▋ | 162/186 [31:10<04:40, 11.69s/it] 88%|████████▊ | 163/186 [31:21<04:28, 11.68s/it] 88%|████████▊ | 164/186 [31:33<04:15, 11.59s/it] 89%|████████▊ | 165/186 [31:44<04:02, 11.53s/it] 89%|████████▉ | 166/186 [31:56<03:50, 11.51s/it] 90%|████████▉ | 167/186 [32:07<03:38, 11.48s/it] 90%|█████████ | 168/186 [32:18<03:26, 11.45s/it] 91%|█████████ | 169/186 [32:30<03:14, 11.46s/it] 91%|█████████▏| 170/186 [32:42<03:05, 11.62s/it] 92%|█████████▏| 171/186 [32:54<02:54, 11.66s/it] 92%|█████████▏| 172/186 [33:06<02:44, 11.72s/it] 93%|█████████▎| 173/186 [33:18<02:33, 11.81s/it] 94%|█████████▎| 174/186 [33:29<02:20, 11.73s/it] 94%|█████████▍| 175/186 [33:41<02:08, 11.67s/it] 95%|█████████▍| 176/186 [33:52<01:56, 11.69s/it] 95%|█████████▌| 177/186 [34:04<01:45, 11.71s/it] 96%|█████████▌| 178/186 [34:16<01:33, 11.63s/it] 96%|█████████▌| 179/186 [34:27<01:21, 11.58s/it] 97%|█████████▋| 180/186 [34:39<01:09, 11.62s/it] 97%|█████████▋| 181/186 [34:50<00:57, 11.57s/it] 98%|█████████▊| 182/186 [35:02<00:46, 11.62s/it] 98%|█████████▊| 183/186 [35:14<00:35, 11.69s/it] 99%|█████████▉| 184/186 [35:26<00:23, 11.78s/it] 99%|█████████▉| 185/186 [35:38<00:11, 11.76s/it]100%|██████████| 186/186 [35:49<00:00, 11.71s/it]                                                 {'train_runtime': 2149.6501, 'train_samples_per_second': 6.25, 'train_steps_per_second': 0.087, 'train_loss': 0.7566841699743784, 'epoch': 1.0}
100%|██████████| 186/186 [35:50<00:00, 11.71s/it]100%|██████████| 186/186 [35:50<00:00, 11.56s/it]
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-1
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33mdirect-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-1[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251015_014055-850mznil/logs[0m
[2025-10-15 02:18:29,927] [INFO] [launch.py:347:main] Process 485254 exits successfully.
[2025-10-15 02:18:29,928] [INFO] [launch.py:347:main] Process 485251 exits successfully.
[2025-10-15 02:18:29,928] [INFO] [launch.py:347:main] Process 485255 exits successfully.
[2025-10-15 02:18:29,928] [INFO] [launch.py:347:main] Process 485253 exits successfully.
[2025-10-15 02:18:29,928] [INFO] [launch.py:347:main] Process 485252 exits successfully.
[2025-10-15 02:18:34,934] [INFO] [launch.py:347:main] Process 485250 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-15 02:18:38,317] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 02:18:38,873] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-15 02:18:38,873] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12354 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.7-1 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.7-1 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 0.7 --use_anchor False --yilin_anchor True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-15 02:18:40,561] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 02:18:41,157] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-15 02:18:41,157] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-15 02:18:41,157] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-15 02:18:41,157] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-15 02:18:41,157] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-15 02:18:44,780] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 02:18:44,890] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 02:18:44,910] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-15 02:18:45,079] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 02:18:45,163] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 02:18:45,222] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-15 02:18:45,230] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 02:18:45,395] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-15 02:18:45,814] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 02:18:45,846] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 02:18:46,138] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-15 02:18:46,138] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 02:18:46,161] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251015_021846-m9aohqev
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.7-1
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/m9aohqev
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.17s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.13s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.53s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.92s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.19s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.21s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.22s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.61s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.66s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.04s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.17s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.19s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.19s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.49s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.81s/it]
to bfloat16...
to bfloat16...
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.67s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.02s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.21s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.24s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.84s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.20s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.84s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.86s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.28s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.73s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.68s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.00s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.12s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.78s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.09s/it]
Formatting inputs...Skip in lazy mode
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.010642766952514648 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.01076650619506836 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.1011345386505127 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.011483192443847656 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.012715339660644531 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10151839256286621 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/186 [00:08<27:13,  8.83s/it]  1%|          | 2/186 [00:18<28:57,  9.44s/it]  2%|▏         | 3/186 [00:28<29:59,  9.83s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1263 > 1048). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1158 > 1048). Running this sequence through the model will result in indexing errors
  2%|▏         | 4/186 [00:39<31:10, 10.28s/it]  3%|▎         | 5/186 [00:51<31:51, 10.56s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
  3%|▎         | 6/186 [01:02<32:31, 10.84s/it]  4%|▍         | 7/186 [01:14<33:05, 11.09s/it]  4%|▍         | 8/186 [01:25<33:28, 11.29s/it]  5%|▍         | 9/186 [01:37<33:18, 11.29s/it]  5%|▌         | 10/186 [01:48<33:14, 11.33s/it]  6%|▌         | 11/186 [01:59<33:06, 11.35s/it]  6%|▋         | 12/186 [02:11<32:52, 11.34s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1160 > 1048). Running this sequence through the model will result in indexing errors
  7%|▋         | 13/186 [02:22<32:45, 11.36s/it]  8%|▊         | 14/186 [02:34<32:56, 11.49s/it]  8%|▊         | 15/186 [02:46<33:00, 11.58s/it]  9%|▊         | 16/186 [02:57<32:43, 11.55s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1069 > 1048). Running this sequence through the model will result in indexing errors
  9%|▉         | 17/186 [03:09<32:30, 11.54s/it] 10%|▉         | 18/186 [03:20<32:17, 11.53s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 10%|█         | 19/186 [03:32<31:57, 11.48s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1200 > 1048). Running this sequence through the model will result in indexing errors
 11%|█         | 20/186 [03:43<32:02, 11.58s/it] 11%|█▏        | 21/186 [03:55<31:45, 11.55s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 12%|█▏        | 22/186 [04:06<31:26, 11.50s/it]WARNING: tokenization mismatch: 1 vs. 47. (ignored)
 12%|█▏        | 23/186 [04:18<31:06, 11.45s/it] 13%|█▎        | 24/186 [04:29<31:07, 11.53s/it] 13%|█▎        | 25/186 [04:41<30:58, 11.54s/it] 14%|█▍        | 26/186 [04:52<30:37, 11.48s/it] 15%|█▍        | 27/186 [05:04<30:29, 11.51s/it] 15%|█▌        | 28/186 [05:15<30:25, 11.55s/it] 16%|█▌        | 29/186 [05:27<30:05, 11.50s/it] 16%|█▌        | 30/186 [05:39<30:14, 11.63s/it] 17%|█▋        | 31/186 [05:50<30:05, 11.65s/it] 17%|█▋        | 32/186 [06:02<29:49, 11.62s/it] 18%|█▊        | 33/186 [06:14<29:48, 11.69s/it] 18%|█▊        | 34/186 [06:26<29:44, 11.74s/it] 19%|█▉        | 35/186 [06:38<29:42, 11.81s/it] 19%|█▉        | 36/186 [06:49<29:31, 11.81s/it] 20%|█▉        | 37/186 [07:01<29:12, 11.76s/it] 20%|██        | 38/186 [07:13<29:10, 11.83s/it] 21%|██        | 39/186 [07:25<28:50, 11.78s/it] 22%|██▏       | 40/186 [07:37<28:40, 11.79s/it] 22%|██▏       | 41/186 [07:48<28:26, 11.77s/it] 23%|██▎       | 42/186 [08:00<28:08, 11.72s/it] 23%|██▎       | 43/186 [08:11<27:48, 11.67s/it] 24%|██▎       | 44/186 [08:23<27:33, 11.64s/it] 24%|██▍       | 45/186 [08:35<27:33, 11.73s/it] 25%|██▍       | 46/186 [08:47<27:24, 11.75s/it] 25%|██▌       | 47/186 [08:59<27:36, 11.91s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 26%|██▌       | 48/186 [09:11<27:12, 11.83s/it] 26%|██▋       | 49/186 [09:22<26:57, 11.81s/it] 27%|██▋       | 50/186 [09:34<26:44, 11.80s/it] 27%|██▋       | 51/186 [09:46<26:26, 11.75s/it] 28%|██▊       | 52/186 [09:58<26:16, 11.76s/it] 28%|██▊       | 53/186 [10:09<26:04, 11.77s/it] 29%|██▉       | 54/186 [10:22<26:13, 11.92s/it] 30%|██▉       | 55/186 [10:33<25:46, 11.81s/it] 30%|███       | 56/186 [10:45<25:37, 11.82s/it] 31%|███       | 57/186 [10:57<25:21, 11.80s/it] 31%|███       | 58/186 [11:09<25:08, 11.79s/it] 32%|███▏      | 59/186 [11:21<25:13, 11.91s/it] 32%|███▏      | 60/186 [11:32<24:53, 11.86s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 33%|███▎      | 61/186 [11:44<24:34, 11.80s/it] 33%|███▎      | 62/186 [11:56<24:20, 11.78s/it] 34%|███▍      | 63/186 [12:08<24:08, 11.78s/it] 34%|███▍      | 64/186 [12:19<23:53, 11.75s/it] 35%|███▍      | 65/186 [12:31<23:31, 11.67s/it] 35%|███▌      | 66/186 [12:43<23:33, 11.78s/it] 36%|███▌      | 67/186 [12:55<23:19, 11.76s/it] 37%|███▋      | 68/186 [13:06<23:06, 11.75s/it] 37%|███▋      | 69/186 [13:18<23:00, 11.80s/it] 38%|███▊      | 70/186 [13:31<23:06, 11.95s/it] 38%|███▊      | 71/186 [13:42<22:47, 11.89s/it] 39%|███▊      | 72/186 [13:54<22:35, 11.89s/it] 39%|███▉      | 73/186 [14:06<22:15, 11.82s/it] 40%|███▉      | 74/186 [14:17<21:57, 11.76s/it] 40%|████      | 75/186 [14:29<21:51, 11.81s/it] 41%|████      | 76/186 [14:41<21:37, 11.79s/it] 41%|████▏     | 77/186 [14:53<21:18, 11.73s/it] 42%|████▏     | 78/186 [15:04<21:07, 11.73s/it] 42%|████▏     | 79/186 [15:16<20:59, 11.77s/it] 43%|████▎     | 80/186 [15:28<20:50, 11.80s/it] 44%|████▎     | 81/186 [15:40<20:39, 11.80s/it] 44%|████▍     | 82/186 [15:52<20:24, 11.77s/it] 45%|████▍     | 83/186 [16:04<20:21, 11.86s/it] 45%|████▌     | 84/186 [16:15<20:03, 11.80s/it] 46%|████▌     | 85/186 [16:27<19:46, 11.75s/it] 46%|████▌     | 86/186 [16:39<19:35, 11.76s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
 47%|████▋     | 87/186 [16:51<19:23, 11.75s/it] 47%|████▋     | 88/186 [17:02<19:03, 11.67s/it] 48%|████▊     | 89/186 [17:14<19:12, 11.88s/it] 48%|████▊     | 90/186 [17:27<19:07, 11.95s/it] 49%|████▉     | 91/186 [17:39<19:06, 12.07s/it] 49%|████▉     | 92/186 [17:51<18:48, 12.01s/it] 50%|█████     | 93/186 [18:03<18:41, 12.06s/it] 51%|█████     | 94/186 [18:15<18:19, 11.95s/it] 51%|█████     | 95/186 [18:26<18:04, 11.91s/it] 52%|█████▏    | 96/186 [18:38<17:47, 11.86s/it] 52%|█████▏    | 97/186 [18:50<17:37, 11.88s/it] 53%|█████▎    | 98/186 [19:02<17:15, 11.76s/it] 53%|█████▎    | 99/186 [19:13<17:01, 11.74s/it] 54%|█████▍    | 100/186 [19:25<16:52, 11.77s/it] 54%|█████▍    | 101/186 [19:37<16:42, 11.79s/it] 55%|█████▍    | 102/186 [19:49<16:29, 11.78s/it] 55%|█████▌    | 103/186 [20:01<16:27, 11.90s/it] 56%|█████▌    | 104/186 [20:13<16:13, 11.87s/it] 56%|█████▋    | 105/186 [20:24<15:59, 11.84s/it] 57%|█████▋    | 106/186 [20:36<15:46, 11.83s/it] 58%|█████▊    | 107/186 [20:48<15:31, 11.79s/it] 58%|█████▊    | 108/186 [21:00<15:16, 11.76s/it] 59%|█████▊    | 109/186 [21:12<15:08, 11.79s/it] 59%|█████▉    | 110/186 [21:23<14:56, 11.79s/it] 60%|█████▉    | 111/186 [21:35<14:43, 11.79s/it] 60%|██████    | 112/186 [21:47<14:35, 11.84s/it] 61%|██████    | 113/186 [21:59<14:29, 11.91s/it] 61%|██████▏   | 114/186 [22:11<14:14, 11.86s/it] 62%|██████▏   | 115/186 [22:22<13:54, 11.76s/it] 62%|██████▏   | 116/186 [22:34<13:45, 11.80s/it] 63%|██████▎   | 117/186 [22:46<13:29, 11.73s/it] 63%|██████▎   | 118/186 [22:58<13:18, 11.74s/it] 64%|██████▍   | 119/186 [23:10<13:13, 11.84s/it] 65%|██████▍   | 120/186 [23:22<13:03, 11.87s/it] 65%|██████▌   | 121/186 [23:34<12:51, 11.87s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 66%|██████▌   | 122/186 [23:45<12:31, 11.75s/it] 66%|██████▌   | 123/186 [23:57<12:32, 11.94s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 67%|██████▋   | 124/186 [24:09<12:14, 11.84s/it] 67%|██████▋   | 125/186 [24:20<11:55, 11.73s/it] 68%|██████▊   | 126/186 [24:32<11:45, 11.75s/it] 68%|██████▊   | 127/186 [24:44<11:30, 11.71s/it] 69%|██████▉   | 128/186 [24:55<11:17, 11.68s/it] 69%|██████▉   | 129/186 [25:07<11:00, 11.58s/it] 70%|██████▉   | 130/186 [25:18<10:44, 11.51s/it] 70%|███████   | 131/186 [25:30<10:32, 11.49s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 71%|███████   | 132/186 [25:41<10:24, 11.57s/it] 72%|███████▏  | 133/186 [25:53<10:11, 11.53s/it] 72%|███████▏  | 134/186 [26:04<09:59, 11.52s/it]WARNING: tokenization mismatch: 1 vs. 47. (ignored)
 73%|███████▎  | 135/186 [26:16<09:46, 11.49s/it] 73%|███████▎  | 136/186 [26:27<09:32, 11.46s/it] 74%|███████▎  | 137/186 [26:39<09:30, 11.64s/it] 74%|███████▍  | 138/186 [26:51<09:15, 11.58s/it] 75%|███████▍  | 139/186 [27:03<09:13, 11.78s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 75%|███████▌  | 140/186 [27:14<08:58, 11.72s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 76%|███████▌  | 141/186 [27:26<08:50, 11.78s/it] 76%|███████▋  | 142/186 [27:38<08:33, 11.68s/it] 77%|███████▋  | 143/186 [27:49<08:21, 11.66s/it] 77%|███████▋  | 144/186 [28:01<08:06, 11.59s/it] 78%|███████▊  | 145/186 [28:12<07:53, 11.54s/it] 78%|███████▊  | 146/186 [28:24<07:42, 11.57s/it] 79%|███████▉  | 147/186 [28:36<07:32, 11.59s/it] 80%|███████▉  | 148/186 [28:47<07:19, 11.56s/it] 80%|████████  | 149/186 [28:58<07:06, 11.53s/it] 81%|████████  | 150/186 [29:10<06:57, 11.61s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 81%|████████  | 151/186 [29:22<06:44, 11.57s/it] 82%|████████▏ | 152/186 [29:33<06:34, 11.61s/it] 82%|████████▏ | 153/186 [29:45<06:20, 11.54s/it] 83%|████████▎ | 154/186 [29:56<06:07, 11.47s/it] 83%|████████▎ | 155/186 [30:08<05:56, 11.51s/it] 84%|████████▍ | 156/186 [30:20<05:51, 11.71s/it] 84%|████████▍ | 157/186 [30:31<05:37, 11.64s/it] 85%|████████▍ | 158/186 [30:43<05:24, 11.58s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 85%|████████▌ | 159/186 [30:55<05:13, 11.62s/it] 86%|████████▌ | 160/186 [31:06<05:01, 11.58s/it] 87%|████████▋ | 161/186 [31:18<04:50, 11.62s/it] 87%|████████▋ | 162/186 [31:30<04:40, 11.68s/it] 88%|████████▊ | 163/186 [31:41<04:27, 11.61s/it] 88%|████████▊ | 164/186 [31:53<04:15, 11.59s/it] 89%|████████▊ | 165/186 [32:04<04:01, 11.50s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1263 > 1048). Running this sequence through the model will result in indexing errors
 89%|████████▉ | 166/186 [32:16<03:51, 11.57s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
 90%|████████▉ | 167/186 [32:27<03:39, 11.54s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 90%|█████████ | 168/186 [32:38<03:26, 11.46s/it] 91%|█████████ | 169/186 [32:50<03:13, 11.40s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 91%|█████████▏| 170/186 [33:01<03:01, 11.37s/it] 92%|█████████▏| 171/186 [33:12<02:51, 11.42s/it] 92%|█████████▏| 172/186 [33:24<02:39, 11.38s/it] 93%|█████████▎| 173/186 [33:36<02:29, 11.53s/it] 94%|█████████▎| 174/186 [33:47<02:18, 11.55s/it] 94%|█████████▍| 175/186 [33:59<02:07, 11.58s/it] 95%|█████████▍| 176/186 [34:11<01:56, 11.61s/it] 95%|█████████▌| 177/186 [34:22<01:44, 11.58s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
 96%|█████████▌| 178/186 [34:34<01:32, 11.59s/it] 96%|█████████▌| 179/186 [34:45<01:21, 11.65s/it] 97%|█████████▋| 180/186 [34:57<01:09, 11.58s/it] 97%|█████████▋| 181/186 [35:08<00:57, 11.55s/it] 98%|█████████▊| 182/186 [35:20<00:46, 11.54s/it] 98%|█████████▊| 183/186 [35:32<00:34, 11.56s/it] 99%|█████████▉| 184/186 [35:43<00:23, 11.62s/it] 99%|█████████▉| 185/186 [35:55<00:11, 11.69s/it]100%|██████████| 186/186 [36:07<00:00, 11.65s/it]                                                 {'train_runtime': 2167.1838, 'train_samples_per_second': 6.199, 'train_steps_per_second': 0.086, 'train_loss': 0.7913861018355175, 'epoch': 1.0}
100%|██████████| 186/186 [36:08<00:00, 11.65s/it]100%|██████████| 186/186 [36:08<00:00, 11.66s/it]
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.7-1
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33mdirect-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.7-1[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251015_021846-m9aohqev/logs[0m
[2025-10-15 02:56:45,967] [INFO] [launch.py:347:main] Process 486983 exits successfully.
[2025-10-15 02:56:45,967] [INFO] [launch.py:347:main] Process 486986 exits successfully.
[2025-10-15 02:56:46,968] [INFO] [launch.py:347:main] Process 486984 exits successfully.
[2025-10-15 02:56:46,969] [INFO] [launch.py:347:main] Process 486985 exits successfully.
[2025-10-15 02:56:46,969] [INFO] [launch.py:347:main] Process 486982 exits successfully.
[2025-10-15 02:56:51,974] [INFO] [launch.py:347:main] Process 486981 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-15 02:56:55,221] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 02:56:55,773] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-15 02:56:55,773] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12354 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.3-1 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.3-1 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 0.3 --use_anchor False --yilin_anchor True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-15 02:56:57,473] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 02:56:58,069] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-15 02:56:58,070] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-15 02:56:58,070] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-15 02:56:58,070] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-15 02:56:58,070] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-15 02:57:01,739] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 02:57:01,781] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 02:57:02,102] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 02:57:02,112] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-15 02:57:02,112] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-10-15 02:57:02,394] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-15 02:57:02,561] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 02:57:02,561] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 02:57:02,741] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 02:57:02,881] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-15 02:57:02,912] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
[2025-10-15 02:57:03,569] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 02:57:04,189] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251015_025704-e0f4jrbp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.3-1
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/e0f4jrbp
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.19s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.58s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.91s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.25s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.26s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.16s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.89s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.29s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.77s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.69s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.08s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.23s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.52s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.81s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.48s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.78s/it]
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.33s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.99s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.99s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.34s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.86s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.95s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.51s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.88s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.63s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.73s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.06s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.64s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.20s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.64s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.94s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.63s/it]
Formatting inputs...Skip in lazy mode
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.010982751846313477 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.009498357772827148 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.012927055358886719 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10150790214538574 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10151815414428711 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10215330123901367 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/186 [00:08<27:09,  8.81s/it]  1%|          | 2/186 [00:18<29:19,  9.56s/it]  2%|▏         | 3/186 [00:29<30:02,  9.85s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1285 > 1048). Running this sequence through the model will result in indexing errors
  2%|▏         | 4/186 [00:39<31:04, 10.25s/it]  3%|▎         | 5/186 [00:51<31:54, 10.58s/it]  3%|▎         | 6/186 [01:02<32:44, 10.92s/it]  4%|▍         | 7/186 [01:14<33:08, 11.11s/it]  4%|▍         | 8/186 [01:25<33:16, 11.22s/it]  5%|▍         | 9/186 [01:37<33:25, 11.33s/it]  5%|▌         | 10/186 [01:48<33:21, 11.37s/it]  6%|▌         | 11/186 [02:00<33:11, 11.38s/it]  6%|▋         | 12/186 [02:11<33:12, 11.45s/it]  7%|▋         | 13/186 [02:23<33:12, 11.52s/it]  8%|▊         | 14/186 [02:34<33:03, 11.53s/it]  8%|▊         | 15/186 [02:46<32:48, 11.51s/it]  9%|▊         | 16/186 [02:57<32:31, 11.48s/it]  9%|▉         | 17/186 [03:09<32:21, 11.49s/it] 10%|▉         | 18/186 [03:20<32:12, 11.50s/it] 10%|█         | 19/186 [03:32<32:07, 11.54s/it] 11%|█         | 20/186 [03:44<31:54, 11.53s/it] 11%|█▏        | 21/186 [03:55<31:42, 11.53s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1200 > 1048). Running this sequence through the model will result in indexing errors
 12%|█▏        | 22/186 [04:07<31:46, 11.62s/it] 12%|█▏        | 23/186 [04:18<31:34, 11.62s/it] 13%|█▎        | 24/186 [04:30<31:41, 11.74s/it] 13%|█▎        | 25/186 [04:42<31:22, 11.69s/it] 14%|█▍        | 26/186 [04:54<31:03, 11.64s/it] 15%|█▍        | 27/186 [05:06<31:04, 11.73s/it] 15%|█▌        | 28/186 [05:17<30:43, 11.67s/it] 16%|█▌        | 29/186 [05:29<30:24, 11.62s/it] 16%|█▌        | 30/186 [05:40<30:13, 11.62s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 17%|█▋        | 31/186 [05:52<30:00, 11.62s/it] 17%|█▋        | 32/186 [06:03<29:51, 11.63s/it] 18%|█▊        | 33/186 [06:15<29:39, 11.63s/it] 18%|█▊        | 34/186 [06:27<29:26, 11.62s/it] 19%|█▉        | 35/186 [06:38<29:22, 11.67s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1125 > 1048). Running this sequence through the model will result in indexing errors
 19%|█▉        | 36/186 [06:50<29:15, 11.70s/it] 20%|█▉        | 37/186 [07:02<29:00, 11.68s/it] 20%|██        | 38/186 [07:13<28:44, 11.66s/it] 21%|██        | 39/186 [07:25<28:33, 11.66s/it] 22%|██▏       | 40/186 [07:37<28:35, 11.75s/it] 22%|██▏       | 41/186 [07:49<28:23, 11.74s/it] 23%|██▎       | 42/186 [08:01<28:26, 11.85s/it] 23%|██▎       | 43/186 [08:13<28:11, 11.83s/it] 24%|██▎       | 44/186 [08:25<28:15, 11.94s/it] 24%|██▍       | 45/186 [08:37<28:08, 11.97s/it] 25%|██▍       | 46/186 [08:49<28:01, 12.01s/it] 25%|██▌       | 47/186 [09:01<27:42, 11.96s/it] 26%|██▌       | 48/186 [09:12<27:14, 11.85s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1160 > 1048). Running this sequence through the model will result in indexing errors
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 26%|██▋       | 49/186 [09:24<27:00, 11.83s/it] 27%|██▋       | 50/186 [09:36<26:37, 11.75s/it] 27%|██▋       | 51/186 [09:47<26:22, 11.72s/it] 28%|██▊       | 52/186 [09:59<26:06, 11.69s/it] 28%|██▊       | 53/186 [10:11<26:05, 11.77s/it] 29%|██▉       | 54/186 [10:23<26:00, 11.82s/it] 30%|██▉       | 55/186 [10:35<25:44, 11.79s/it] 30%|███       | 56/186 [10:46<25:30, 11.78s/it] 31%|███       | 57/186 [10:58<25:10, 11.71s/it] 31%|███       | 58/186 [11:10<25:01, 11.73s/it] 32%|███▏      | 59/186 [11:22<25:03, 11.84s/it] 32%|███▏      | 60/186 [11:33<24:41, 11.76s/it] 33%|███▎      | 61/186 [11:45<24:37, 11.82s/it] 33%|███▎      | 62/186 [11:57<24:20, 11.78s/it] 34%|███▍      | 63/186 [12:09<24:03, 11.74s/it] 34%|███▍      | 64/186 [12:20<23:43, 11.66s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 35%|███▍      | 65/186 [12:32<23:31, 11.66s/it] 35%|███▌      | 66/186 [12:44<23:23, 11.70s/it] 36%|███▌      | 67/186 [12:56<23:17, 11.75s/it] 37%|███▋      | 68/186 [13:07<23:04, 11.73s/it] 37%|███▋      | 69/186 [13:19<22:56, 11.77s/it] 38%|███▊      | 70/186 [13:31<22:43, 11.75s/it] 38%|███▊      | 71/186 [13:42<22:24, 11.69s/it] 39%|███▊      | 72/186 [13:54<22:08, 11.65s/it] 39%|███▉      | 73/186 [14:06<21:56, 11.65s/it] 40%|███▉      | 74/186 [14:17<21:41, 11.62s/it] 40%|████      | 75/186 [14:29<21:30, 11.63s/it] 41%|████      | 76/186 [14:40<21:13, 11.58s/it] 41%|████▏     | 77/186 [14:52<21:09, 11.65s/it] 42%|████▏     | 78/186 [15:04<20:54, 11.61s/it] 42%|████▏     | 79/186 [15:15<20:45, 11.64s/it] 43%|████▎     | 80/186 [15:27<20:35, 11.65s/it] 44%|████▎     | 81/186 [15:39<20:20, 11.62s/it] 44%|████▍     | 82/186 [15:50<20:18, 11.72s/it] 45%|████▍     | 83/186 [16:02<20:03, 11.68s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1125 > 1048). Running this sequence through the model will result in indexing errors
 45%|████▌     | 84/186 [16:14<19:59, 11.76s/it] 46%|████▌     | 85/186 [16:25<19:38, 11.67s/it] 46%|████▌     | 86/186 [16:37<19:21, 11.62s/it] 47%|████▋     | 87/186 [16:49<19:12, 11.64s/it] 47%|████▋     | 88/186 [17:00<19:07, 11.71s/it] 48%|████▊     | 89/186 [17:12<19:01, 11.77s/it] 48%|████▊     | 90/186 [17:24<18:48, 11.76s/it] 49%|████▉     | 91/186 [17:36<18:49, 11.89s/it] 49%|████▉     | 92/186 [17:48<18:30, 11.82s/it] 50%|█████     | 93/186 [18:00<18:21, 11.85s/it] 51%|█████     | 94/186 [18:11<18:02, 11.77s/it] 51%|█████     | 95/186 [18:23<17:57, 11.84s/it] 52%|█████▏    | 96/186 [18:35<17:43, 11.82s/it] 52%|█████▏    | 97/186 [18:47<17:25, 11.74s/it] 53%|█████▎    | 98/186 [18:59<17:13, 11.75s/it] 53%|█████▎    | 99/186 [19:10<16:58, 11.71s/it] 54%|█████▍    | 100/186 [19:22<16:45, 11.69s/it] 54%|█████▍    | 101/186 [19:33<16:31, 11.66s/it] 55%|█████▍    | 102/186 [19:45<16:14, 11.60s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 55%|█████▌    | 103/186 [19:57<16:11, 11.70s/it] 56%|█████▌    | 104/186 [20:08<15:54, 11.64s/it] 56%|█████▋    | 105/186 [20:20<15:48, 11.70s/it] 57%|█████▋    | 106/186 [20:32<15:32, 11.66s/it] 58%|█████▊    | 107/186 [20:43<15:17, 11.62s/it] 58%|█████▊    | 108/186 [20:55<15:14, 11.72s/it] 59%|█████▊    | 109/186 [21:07<15:00, 11.69s/it] 59%|█████▉    | 110/186 [21:19<14:53, 11.75s/it] 60%|█████▉    | 111/186 [21:30<14:39, 11.73s/it]WARNING: tokenization mismatch: 1 vs. 48. (ignored)
 60%|██████    | 112/186 [21:42<14:24, 11.68s/it] 61%|██████    | 113/186 [21:53<14:08, 11.62s/it] 61%|██████▏   | 114/186 [22:05<14:00, 11.68s/it] 62%|██████▏   | 115/186 [22:17<13:59, 11.83s/it] 62%|██████▏   | 116/186 [22:29<13:51, 11.88s/it] 63%|██████▎   | 117/186 [22:41<13:33, 11.79s/it] 63%|██████▎   | 118/186 [22:53<13:14, 11.69s/it] 64%|██████▍   | 119/186 [23:04<12:57, 11.61s/it] 65%|██████▍   | 120/186 [23:15<12:44, 11.58s/it] 65%|██████▌   | 121/186 [23:27<12:34, 11.61s/it] 66%|██████▌   | 122/186 [23:39<12:24, 11.64s/it] 66%|██████▌   | 123/186 [23:51<12:20, 11.76s/it] 67%|██████▋   | 124/186 [24:03<12:08, 11.75s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1263 > 1048). Running this sequence through the model will result in indexing errors
 67%|██████▋   | 125/186 [24:15<12:00, 11.81s/it] 68%|██████▊   | 126/186 [24:26<11:43, 11.73s/it] 68%|██████▊   | 127/186 [24:37<11:24, 11.59s/it] 69%|██████▉   | 128/186 [24:49<11:09, 11.55s/it] 69%|██████▉   | 129/186 [25:00<10:56, 11.52s/it] 70%|██████▉   | 130/186 [25:12<10:47, 11.56s/it] 70%|███████   | 131/186 [25:24<10:39, 11.63s/it] 71%|███████   | 132/186 [25:35<10:28, 11.65s/it] 72%|███████▏  | 133/186 [25:48<10:27, 11.83s/it] 72%|███████▏  | 134/186 [26:00<10:20, 11.94s/it] 73%|███████▎  | 135/186 [26:12<10:07, 11.90s/it] 73%|███████▎  | 136/186 [26:24<09:58, 11.98s/it] 74%|███████▎  | 137/186 [26:36<09:45, 11.94s/it] 74%|███████▍  | 138/186 [26:47<09:30, 11.88s/it] 75%|███████▍  | 139/186 [26:59<09:18, 11.88s/it] 75%|███████▌  | 140/186 [27:11<09:02, 11.80s/it] 76%|███████▌  | 141/186 [27:23<08:48, 11.74s/it] 76%|███████▋  | 142/186 [27:34<08:36, 11.74s/it] 77%|███████▋  | 143/186 [27:46<08:23, 11.71s/it] 77%|███████▋  | 144/186 [27:57<08:09, 11.64s/it] 78%|███████▊  | 145/186 [28:09<07:56, 11.63s/it] 78%|███████▊  | 146/186 [28:21<07:44, 11.62s/it] 79%|███████▉  | 147/186 [28:32<07:34, 11.65s/it] 80%|███████▉  | 148/186 [28:44<07:19, 11.57s/it] 80%|████████  | 149/186 [28:55<07:08, 11.57s/it] 81%|████████  | 150/186 [29:07<06:58, 11.62s/it] 81%|████████  | 151/186 [29:19<06:48, 11.67s/it] 82%|████████▏ | 152/186 [29:31<06:40, 11.78s/it] 82%|████████▏ | 153/186 [29:43<06:28, 11.76s/it] 83%|████████▎ | 154/186 [29:54<06:17, 11.79s/it] 83%|████████▎ | 155/186 [30:06<06:05, 11.80s/it] 84%|████████▍ | 156/186 [30:19<06:03, 12.10s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 84%|████████▍ | 157/186 [30:31<05:51, 12.12s/it] 85%|████████▍ | 158/186 [30:43<05:39, 12.13s/it] 85%|████████▌ | 159/186 [30:55<05:27, 12.13s/it] 86%|████████▌ | 160/186 [31:08<05:14, 12.10s/it] 87%|████████▋ | 161/186 [31:19<05:01, 12.05s/it] 87%|████████▋ | 162/186 [31:32<04:51, 12.15s/it] 88%|████████▊ | 163/186 [31:44<04:41, 12.23s/it] 88%|████████▊ | 164/186 [31:56<04:27, 12.17s/it] 89%|████████▊ | 165/186 [32:08<04:14, 12.12s/it] 89%|████████▉ | 166/186 [32:20<04:01, 12.07s/it] 90%|████████▉ | 167/186 [32:32<03:50, 12.11s/it] 90%|█████████ | 168/186 [32:45<03:40, 12.23s/it] 91%|█████████ | 169/186 [32:57<03:27, 12.20s/it] 91%|█████████▏| 170/186 [33:09<03:15, 12.19s/it] 92%|█████████▏| 171/186 [33:22<03:03, 12.24s/it] 92%|█████████▏| 172/186 [33:33<02:49, 12.12s/it] 93%|█████████▎| 173/186 [33:45<02:37, 12.11s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 94%|█████████▎| 174/186 [33:57<02:24, 12.07s/it] 94%|█████████▍| 175/186 [34:10<02:12, 12.06s/it] 95%|█████████▍| 176/186 [34:22<02:01, 12.17s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
 95%|█████████▌| 177/186 [34:34<01:48, 12.05s/it] 96%|█████████▌| 178/186 [34:46<01:35, 11.98s/it] 96%|█████████▌| 179/186 [34:58<01:23, 12.00s/it] 97%|█████████▋| 180/186 [35:10<01:11, 12.00s/it] 97%|█████████▋| 181/186 [35:22<01:00, 12.00s/it] 98%|█████████▊| 182/186 [35:34<00:48, 12.03s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
 98%|█████████▊| 183/186 [35:46<00:35, 11.99s/it] 99%|█████████▉| 184/186 [35:57<00:23, 11.97s/it]WARNING: tokenization mismatch: 1 vs. 48. (ignored)
 99%|█████████▉| 185/186 [36:10<00:11, 12.00s/it]100%|██████████| 186/186 [36:21<00:00, 11.97s/it]                                                 {'train_runtime': 2181.9578, 'train_samples_per_second': 6.157, 'train_steps_per_second': 0.085, 'train_loss': 0.7652123564033098, 'epoch': 1.0}
100%|██████████| 186/186 [36:22<00:00, 11.97s/it]100%|██████████| 186/186 [36:22<00:00, 11.74s/it]
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.3-1
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33mdirect-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.3-1[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251015_025704-e0f4jrbp/logs[0m
[2025-10-15 03:35:12,716] [INFO] [launch.py:347:main] Process 488722 exits successfully.
[2025-10-15 03:35:12,717] [INFO] [launch.py:347:main] Process 488719 exits successfully.
[2025-10-15 03:35:12,717] [INFO] [launch.py:347:main] Process 488723 exits successfully.
[2025-10-15 03:35:12,717] [INFO] [launch.py:347:main] Process 488721 exits successfully.
[2025-10-15 03:35:12,718] [INFO] [launch.py:347:main] Process 488720 exits successfully.
[2025-10-15 03:35:19,725] [INFO] [launch.py:347:main] Process 488718 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-15 03:35:22,940] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 03:35:23,496] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-15 03:35:23,496] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12354 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-1 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-1 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 1 --use_anchor False --yilin_anchor True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-15 03:35:25,153] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 03:35:25,739] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-15 03:35:25,739] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-15 03:35:25,739] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-15 03:35:25,739] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-15 03:35:25,739] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-15 03:35:29,569] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-15 03:35:29,817] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 03:35:29,890] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-15 03:35:29,890] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-10-15 03:35:29,913] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 03:35:30,140] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 03:35:30,227] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-10-15 03:35:30,378] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
[2025-10-15 03:35:30,813] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 03:35:30,868] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 03:35:31,505] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-15 03:35:31,605] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 03:35:32,187] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: creating run
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251015_033532-bbm3od9u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-1
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/bbm3od9u
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.48s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.09s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.84s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.94s/it]
to bfloat16...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.38s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.64s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.11s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.68s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.16s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.83s/it]to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.51s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.69s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.57s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.91s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.51s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.81s/it]
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.46s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.24s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.32s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.97s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.34s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.59s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.78s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.16s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.96s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.61s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.12s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.50s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.82s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.99s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.15s/it]
Formatting inputs...Skip in lazy mode
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.012990713119506836 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.012178421020507812 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.008382558822631836 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10140514373779297 seconds
Loading extension module fused_adam...
Loading extension module fused_adam...
Time to load fused_adam op: 0.10152721405029297 seconds
Time to load fused_adam op: 0.10153746604919434 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/186 [00:08<26:18,  8.53s/it]  1%|          | 2/186 [00:18<28:18,  9.23s/it]  2%|▏         | 3/186 [00:28<29:56,  9.82s/it]  2%|▏         | 4/186 [00:39<30:28, 10.05s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1089 > 1048). Running this sequence through the model will result in indexing errors
  3%|▎         | 5/186 [00:50<31:21, 10.39s/it]  3%|▎         | 6/186 [01:00<31:32, 10.52s/it]  4%|▍         | 7/186 [01:11<31:36, 10.59s/it]  4%|▍         | 8/186 [01:22<31:18, 10.56s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1089 > 1048). Running this sequence through the model will result in indexing errors
  5%|▍         | 9/186 [01:32<31:14, 10.59s/it]  5%|▌         | 10/186 [01:43<30:51, 10.52s/it]  6%|▌         | 11/186 [01:53<30:49, 10.57s/it]  6%|▋         | 12/186 [02:04<31:04, 10.71s/it]  7%|▋         | 13/186 [02:15<30:40, 10.64s/it]  8%|▊         | 14/186 [02:26<30:49, 10.75s/it]  8%|▊         | 15/186 [02:36<30:21, 10.65s/it]  9%|▊         | 16/186 [02:47<30:29, 10.76s/it]  9%|▉         | 17/186 [02:58<30:28, 10.82s/it] 10%|▉         | 18/186 [03:10<30:39, 10.95s/it] 10%|█         | 19/186 [03:20<30:17, 10.88s/it] 11%|█         | 20/186 [03:31<29:59, 10.84s/it] 11%|█▏        | 21/186 [03:42<29:55, 10.88s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1073 > 1048). Running this sequence through the model will result in indexing errors
 12%|█▏        | 22/186 [03:54<30:30, 11.16s/it] 12%|█▏        | 23/186 [04:06<30:55, 11.38s/it] 13%|█▎        | 24/186 [04:17<31:00, 11.49s/it] 13%|█▎        | 25/186 [04:29<30:45, 11.47s/it] 14%|█▍        | 26/186 [04:41<30:49, 11.56s/it] 15%|█▍        | 27/186 [04:52<30:45, 11.61s/it] 15%|█▌        | 28/186 [05:04<30:26, 11.56s/it] 16%|█▌        | 29/186 [05:15<30:05, 11.50s/it] 16%|█▌        | 30/186 [05:27<29:50, 11.48s/it] 17%|█▋        | 31/186 [05:38<29:38, 11.47s/it] 17%|█▋        | 32/186 [05:49<29:21, 11.44s/it] 18%|█▊        | 33/186 [06:01<29:08, 11.43s/it] 18%|█▊        | 34/186 [06:12<28:55, 11.41s/it] 19%|█▉        | 35/186 [06:24<29:00, 11.52s/it] 19%|█▉        | 36/186 [06:35<28:47, 11.51s/it] 20%|█▉        | 37/186 [06:47<28:33, 11.50s/it] 20%|██        | 38/186 [06:59<28:31, 11.57s/it] 21%|██        | 39/186 [07:10<28:31, 11.64s/it] 22%|██▏       | 40/186 [07:22<28:14, 11.61s/it] 22%|██▏       | 41/186 [07:34<28:10, 11.66s/it] 23%|██▎       | 42/186 [07:45<27:55, 11.63s/it] 23%|██▎       | 43/186 [07:57<27:50, 11.68s/it] 24%|██▎       | 44/186 [08:09<27:33, 11.65s/it] 24%|██▍       | 45/186 [08:20<27:22, 11.65s/it] 25%|██▍       | 46/186 [08:32<27:07, 11.62s/it] 25%|██▌       | 47/186 [08:44<27:00, 11.66s/it] 26%|██▌       | 48/186 [08:55<26:39, 11.59s/it] 26%|██▋       | 49/186 [09:07<26:32, 11.62s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 27%|██▋       | 50/186 [09:18<26:18, 11.60s/it] 27%|██▋       | 51/186 [09:30<26:02, 11.58s/it] 28%|██▊       | 52/186 [09:42<26:07, 11.69s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 28%|██▊       | 53/186 [09:54<25:57, 11.71s/it] 29%|██▉       | 54/186 [10:05<25:36, 11.64s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 30%|██▉       | 55/186 [10:17<25:26, 11.65s/it] 30%|███       | 56/186 [10:28<25:05, 11.58s/it] 31%|███       | 57/186 [10:40<24:52, 11.57s/it] 31%|███       | 58/186 [10:51<24:35, 11.52s/it] 32%|███▏      | 59/186 [11:03<24:37, 11.63s/it] 32%|███▏      | 60/186 [11:14<24:18, 11.57s/it] 33%|███▎      | 61/186 [11:26<24:16, 11.65s/it] 33%|███▎      | 62/186 [11:38<24:02, 11.64s/it] 34%|███▍      | 63/186 [11:49<23:45, 11.59s/it] 34%|███▍      | 64/186 [12:01<23:33, 11.59s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1158 > 1048). Running this sequence through the model will result in indexing errors
 35%|███▍      | 65/186 [12:13<23:29, 11.65s/it] 35%|███▌      | 66/186 [12:25<23:33, 11.78s/it] 36%|███▌      | 67/186 [12:36<23:14, 11.72s/it] 37%|███▋      | 68/186 [12:48<22:55, 11.66s/it] 37%|███▋      | 69/186 [13:00<23:02, 11.82s/it] 38%|███▊      | 70/186 [13:12<23:06, 11.96s/it] 38%|███▊      | 71/186 [13:24<22:53, 11.95s/it] 39%|███▊      | 72/186 [13:36<22:32, 11.87s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1240 > 1048). Running this sequence through the model will result in indexing errors
 39%|███▉      | 73/186 [13:48<22:20, 11.87s/it] 40%|███▉      | 74/186 [13:59<21:54, 11.74s/it] 40%|████      | 75/186 [14:11<21:39, 11.71s/it] 41%|████      | 76/186 [14:23<21:40, 11.82s/it] 41%|████▏     | 77/186 [14:35<21:30, 11.84s/it] 42%|████▏     | 78/186 [14:47<21:13, 11.79s/it] 42%|████▏     | 79/186 [14:58<20:50, 11.69s/it] 43%|████▎     | 80/186 [15:10<20:50, 11.80s/it] 44%|████▎     | 81/186 [15:22<20:36, 11.77s/it] 44%|████▍     | 82/186 [15:34<20:28, 11.81s/it] 45%|████▍     | 83/186 [15:45<20:12, 11.77s/it] 45%|████▌     | 84/186 [15:57<20:02, 11.79s/it] 46%|████▌     | 85/186 [16:09<19:45, 11.74s/it] 46%|████▌     | 86/186 [16:20<19:29, 11.69s/it] 47%|████▋     | 87/186 [16:32<19:12, 11.64s/it] 47%|████▋     | 88/186 [16:44<19:04, 11.68s/it] 48%|████▊     | 89/186 [16:55<18:50, 11.65s/it] 48%|████▊     | 90/186 [17:07<18:37, 11.65s/it] 49%|████▉     | 91/186 [17:19<18:32, 11.71s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 49%|████▉     | 92/186 [17:31<18:21, 11.72s/it] 50%|█████     | 93/186 [17:43<18:18, 11.81s/it] 51%|█████     | 94/186 [17:54<18:01, 11.75s/it] 51%|█████     | 95/186 [18:06<17:43, 11.69s/it] 52%|█████▏    | 96/186 [18:17<17:28, 11.65s/it] 52%|█████▏    | 97/186 [18:29<17:18, 11.67s/it] 53%|█████▎    | 98/186 [18:40<17:00, 11.60s/it] 53%|█████▎    | 99/186 [18:52<16:49, 11.61s/it] 54%|█████▍    | 100/186 [19:04<16:36, 11.59s/it] 54%|█████▍    | 101/186 [19:15<16:20, 11.54s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 55%|█████▍    | 102/186 [19:27<16:09, 11.54s/it] 55%|█████▌    | 103/186 [19:39<16:10, 11.69s/it] 56%|█████▌    | 104/186 [19:50<16:03, 11.75s/it] 56%|█████▋    | 105/186 [20:02<15:49, 11.72s/it] 57%|█████▋    | 106/186 [20:14<15:33, 11.67s/it] 58%|█████▊    | 107/186 [20:25<15:20, 11.65s/it] 58%|█████▊    | 108/186 [20:37<15:06, 11.63s/it] 59%|█████▊    | 109/186 [20:48<14:53, 11.61s/it] 59%|█████▉    | 110/186 [21:00<14:50, 11.71s/it] 60%|█████▉    | 111/186 [21:12<14:40, 11.75s/it] 60%|██████    | 112/186 [21:24<14:23, 11.67s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 61%|██████    | 113/186 [21:35<14:13, 11.69s/it] 61%|██████▏   | 114/186 [21:47<13:56, 11.62s/it] 62%|██████▏   | 115/186 [21:59<13:45, 11.63s/it] 62%|██████▏   | 116/186 [22:11<13:52, 11.89s/it] 63%|██████▎   | 117/186 [22:23<13:35, 11.81s/it] 63%|██████▎   | 118/186 [22:34<13:17, 11.73s/it] 64%|██████▍   | 119/186 [22:46<13:03, 11.70s/it] 65%|██████▍   | 120/186 [22:57<12:50, 11.67s/it] 65%|██████▌   | 121/186 [23:09<12:38, 11.67s/it] 66%|██████▌   | 122/186 [23:21<12:28, 11.69s/it] 66%|██████▌   | 123/186 [23:33<12:28, 11.88s/it] 67%|██████▋   | 124/186 [23:45<12:14, 11.84s/it] 67%|██████▋   | 125/186 [23:57<11:59, 11.80s/it] 68%|██████▊   | 126/186 [24:08<11:44, 11.74s/it] 68%|██████▊   | 127/186 [24:20<11:29, 11.69s/it] 69%|██████▉   | 128/186 [24:31<11:16, 11.67s/it] 69%|██████▉   | 129/186 [24:43<11:02, 11.62s/it] 70%|██████▉   | 130/186 [24:55<10:51, 11.64s/it] 70%|███████   | 131/186 [25:06<10:44, 11.72s/it] 71%|███████   | 132/186 [25:19<10:39, 11.85s/it] 72%|███████▏  | 133/186 [25:30<10:24, 11.78s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1221 > 1048). Running this sequence through the model will result in indexing errors
 72%|███████▏  | 134/186 [25:42<10:13, 11.80s/it] 73%|███████▎  | 135/186 [25:54<09:59, 11.76s/it] 73%|███████▎  | 136/186 [26:05<09:46, 11.72s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 74%|███████▎  | 137/186 [26:18<09:46, 11.97s/it] 74%|███████▍  | 138/186 [26:29<09:28, 11.84s/it] 75%|███████▍  | 139/186 [26:41<09:16, 11.85s/it] 75%|███████▌  | 140/186 [26:53<09:04, 11.83s/it] 76%|███████▌  | 141/186 [27:05<08:47, 11.72s/it] 76%|███████▋  | 142/186 [27:17<08:39, 11.80s/it] 77%|███████▋  | 143/186 [27:29<08:31, 11.89s/it] 77%|███████▋  | 144/186 [27:40<08:16, 11.83s/it] 78%|███████▊  | 145/186 [27:52<08:02, 11.78s/it] 78%|███████▊  | 146/186 [28:04<07:53, 11.83s/it] 79%|███████▉  | 147/186 [28:15<07:37, 11.72s/it] 80%|███████▉  | 148/186 [28:28<07:30, 11.86s/it] 80%|████████  | 149/186 [28:40<07:20, 11.91s/it] 81%|████████  | 150/186 [28:51<07:05, 11.82s/it] 81%|████████  | 151/186 [29:03<06:52, 11.79s/it] 82%|████████▏ | 152/186 [29:15<06:39, 11.76s/it] 82%|████████▏ | 153/186 [29:27<06:29, 11.81s/it] 83%|████████▎ | 154/186 [29:38<06:15, 11.72s/it] 83%|████████▎ | 155/186 [29:50<06:01, 11.68s/it] 84%|████████▍ | 156/186 [30:02<05:53, 11.78s/it] 84%|████████▍ | 157/186 [30:13<05:40, 11.73s/it] 85%|████████▍ | 158/186 [30:25<05:27, 11.68s/it] 85%|████████▌ | 159/186 [30:36<05:14, 11.65s/it] 86%|████████▌ | 160/186 [30:48<05:03, 11.67s/it] 87%|████████▋ | 161/186 [31:00<04:51, 11.67s/it] 87%|████████▋ | 162/186 [31:12<04:41, 11.73s/it] 88%|████████▊ | 163/186 [31:23<04:29, 11.72s/it] 88%|████████▊ | 164/186 [31:35<04:18, 11.76s/it] 89%|████████▊ | 165/186 [31:47<04:06, 11.75s/it] 89%|████████▉ | 166/186 [31:59<03:55, 11.79s/it] 90%|████████▉ | 167/186 [32:10<03:42, 11.70s/it] 90%|█████████ | 168/186 [32:22<03:29, 11.66s/it] 91%|█████████ | 169/186 [32:33<03:17, 11.62s/it] 91%|█████████▏| 170/186 [32:45<03:05, 11.61s/it] 92%|█████████▏| 171/186 [32:57<02:56, 11.78s/it] 92%|█████████▏| 172/186 [33:09<02:44, 11.78s/it] 93%|█████████▎| 173/186 [33:21<02:32, 11.74s/it] 94%|█████████▎| 174/186 [33:32<02:20, 11.73s/it] 94%|█████████▍| 175/186 [33:44<02:09, 11.76s/it] 95%|█████████▍| 176/186 [33:56<01:56, 11.69s/it] 95%|█████████▌| 177/186 [34:07<01:44, 11.65s/it] 96%|█████████▌| 178/186 [34:19<01:32, 11.60s/it] 96%|█████████▌| 179/186 [34:31<01:21, 11.65s/it] 97%|█████████▋| 180/186 [34:42<01:09, 11.58s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 97%|█████████▋| 181/186 [34:54<00:58, 11.60s/it] 98%|█████████▊| 182/186 [35:05<00:46, 11.62s/it]WARNING: tokenization mismatch: 1 vs. 47. (ignored)
 98%|█████████▊| 183/186 [35:17<00:34, 11.63s/it] 99%|█████████▉| 184/186 [35:29<00:23, 11.63s/it] 99%|█████████▉| 185/186 [35:41<00:11, 11.74s/it]100%|██████████| 186/186 [35:52<00:00, 11.73s/it]                                                 {'train_runtime': 2152.7477, 'train_samples_per_second': 6.241, 'train_steps_per_second': 0.086, 'train_loss': 0.7860286261445733, 'epoch': 1.0}
100%|██████████| 186/186 [35:53<00:00, 11.73s/it]100%|██████████| 186/186 [35:53<00:00, 11.58s/it]
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-1
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33mdirect-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-1[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251015_033532-bbm3od9u/logs[0m
[2025-10-15 04:13:10,407] [INFO] [launch.py:347:main] Process 490473 exits successfully.
[2025-10-15 04:13:10,407] [INFO] [launch.py:347:main] Process 490470 exits successfully.
[2025-10-15 04:13:10,407] [INFO] [launch.py:347:main] Process 490474 exits successfully.
[2025-10-15 04:13:10,407] [INFO] [launch.py:347:main] Process 490472 exits successfully.
[2025-10-15 04:13:10,408] [INFO] [launch.py:347:main] Process 490471 exits successfully.
[2025-10-15 04:13:14,412] [INFO] [launch.py:347:main] Process 490469 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-15 04:13:17,757] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 04:13:18,314] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-15 04:13:18,314] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12354 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-2 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-2 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 0.5 --use_anchor False --yilin_anchor True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-15 04:13:20,013] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 04:13:20,606] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-15 04:13:20,606] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-15 04:13:20,606] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-15 04:13:20,606] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-15 04:13:20,606] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-15 04:13:25,189] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-15 04:13:25,345] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 04:13:25,503] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-15 04:13:25,503] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 04:13:25,795] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-15 04:13:25,888] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 04:13:25,922] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-15 04:13:26,108] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
[2025-10-15 04:13:26,211] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 04:13:26,255] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
[2025-10-15 04:13:26,400] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 04:13:26,434] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 04:13:27,042] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: creating run
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251015_041327-v3ess6vd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-2
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/v3ess6vd
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.78s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.06s/it]
to bfloat16...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.37s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.97s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.18s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.18s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.24s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.24s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.65s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.51s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.91s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.24s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.54s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.68s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.04s/it]
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.96s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.90s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.21s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.20s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.20s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.83s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.01s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.82s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.81s/it]
Formatting inputs...Skip in lazy mode
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.73s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.10s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.55s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.89s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.98s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.87s/it]
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.011963367462158203 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Time to load fused_adam op: 0.01107025146484375 seconds
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.01072835922241211 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...

Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.011853456497192383 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10150313377380371 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.1020808219909668 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/186 [00:08<26:18,  8.53s/it]  1%|          | 2/186 [00:18<28:29,  9.29s/it]  2%|▏         | 3/186 [00:28<29:22,  9.63s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
  2%|▏         | 4/186 [00:38<30:10,  9.95s/it]  3%|▎         | 5/186 [00:49<30:30, 10.11s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1143 > 1048). Running this sequence through the model will result in indexing errors
  3%|▎         | 6/186 [01:00<31:15, 10.42s/it]  4%|▍         | 7/186 [01:11<31:33, 10.58s/it]  4%|▍         | 8/186 [01:22<32:28, 10.94s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1263 > 1048). Running this sequence through the model will result in indexing errors
  5%|▍         | 9/186 [01:34<33:03, 11.20s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1089 > 1048). Running this sequence through the model will result in indexing errors
  5%|▌         | 10/186 [01:46<33:22, 11.38s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
  6%|▌         | 11/186 [01:58<33:23, 11.45s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1150 > 1048). Running this sequence through the model will result in indexing errors
  6%|▋         | 12/186 [02:10<33:42, 11.62s/it]  7%|▋         | 13/186 [02:21<33:25, 11.59s/it]  8%|▊         | 14/186 [02:33<33:15, 11.60s/it]  8%|▊         | 15/186 [02:44<33:03, 11.60s/it]  9%|▊         | 16/186 [02:56<32:49, 11.59s/it]  9%|▉         | 17/186 [03:07<32:34, 11.56s/it] 10%|▉         | 18/186 [03:19<32:35, 11.64s/it] 10%|█         | 19/186 [03:31<32:22, 11.63s/it] 11%|█         | 20/186 [03:42<32:04, 11.60s/it] 11%|█▏        | 21/186 [03:54<31:49, 11.57s/it] 12%|█▏        | 22/186 [04:06<31:44, 11.61s/it] 12%|█▏        | 23/186 [04:17<31:29, 11.59s/it] 13%|█▎        | 24/186 [04:29<31:42, 11.75s/it] 13%|█▎        | 25/186 [04:41<31:32, 11.75s/it] 14%|█▍        | 26/186 [04:53<31:16, 11.73s/it] 15%|█▍        | 27/186 [05:05<31:16, 11.80s/it] 15%|█▌        | 28/186 [05:16<30:56, 11.75s/it] 16%|█▌        | 29/186 [05:28<30:34, 11.68s/it] 16%|█▌        | 30/186 [05:40<30:28, 11.72s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1125 > 1048). Running this sequence through the model will result in indexing errors
 17%|█▋        | 31/186 [05:52<30:44, 11.90s/it]WARNING: tokenization mismatch: 1 vs. 48. (ignored)
 17%|█▋        | 32/186 [06:04<30:20, 11.82s/it] 18%|█▊        | 33/186 [06:15<30:15, 11.86s/it] 18%|█▊        | 34/186 [06:27<29:56, 11.82s/it] 19%|█▉        | 35/186 [06:39<30:02, 11.94s/it] 19%|█▉        | 36/186 [06:51<29:47, 11.91s/it] 20%|█▉        | 37/186 [07:03<29:19, 11.81s/it] 20%|██        | 38/186 [07:14<28:55, 11.72s/it] 21%|██        | 39/186 [07:26<28:45, 11.74s/it] 22%|██▏       | 40/186 [07:38<28:30, 11.71s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 22%|██▏       | 41/186 [07:49<28:11, 11.66s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 23%|██▎       | 42/186 [08:01<27:56, 11.64s/it] 23%|██▎       | 43/186 [08:13<27:52, 11.69s/it] 24%|██▎       | 44/186 [08:24<27:38, 11.68s/it] 24%|██▍       | 45/186 [08:36<27:18, 11.62s/it] 25%|██▍       | 46/186 [08:48<27:11, 11.65s/it] 25%|██▌       | 47/186 [09:00<27:14, 11.76s/it] 26%|██▌       | 48/186 [09:11<27:03, 11.77s/it] 26%|██▋       | 49/186 [09:23<26:52, 11.77s/it] 27%|██▋       | 50/186 [09:35<26:33, 11.71s/it] 27%|██▋       | 51/186 [09:46<26:14, 11.66s/it] 28%|██▊       | 52/186 [09:58<25:55, 11.61s/it] 28%|██▊       | 53/186 [10:09<25:43, 11.61s/it] 29%|██▉       | 54/186 [10:21<25:31, 11.60s/it] 30%|██▉       | 55/186 [10:32<25:15, 11.57s/it] 30%|███       | 56/186 [10:44<25:04, 11.58s/it] 31%|███       | 57/186 [10:56<24:58, 11.62s/it] 31%|███       | 58/186 [11:08<24:51, 11.66s/it] 32%|███▏      | 59/186 [11:20<25:20, 11.98s/it] 32%|███▏      | 60/186 [11:32<25:01, 11.91s/it] 33%|███▎      | 61/186 [11:44<24:40, 11.84s/it] 33%|███▎      | 62/186 [11:55<24:15, 11.74s/it] 34%|███▍      | 63/186 [12:07<23:56, 11.68s/it] 34%|███▍      | 64/186 [12:18<23:36, 11.61s/it] 35%|███▍      | 65/186 [12:30<23:29, 11.65s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 35%|███▌      | 66/186 [12:42<23:40, 11.84s/it] 36%|███▌      | 67/186 [12:54<23:26, 11.82s/it] 37%|███▋      | 68/186 [13:06<23:07, 11.76s/it] 37%|███▋      | 69/186 [13:18<23:02, 11.82s/it] 38%|███▊      | 70/186 [13:30<23:05, 11.94s/it] 38%|███▊      | 71/186 [13:41<22:45, 11.87s/it] 39%|███▊      | 72/186 [13:53<22:26, 11.81s/it] 39%|███▉      | 73/186 [14:05<22:21, 11.87s/it] 40%|███▉      | 74/186 [14:17<22:09, 11.87s/it] 40%|████      | 75/186 [14:29<22:05, 11.94s/it] 41%|████      | 76/186 [14:41<21:44, 11.86s/it] 41%|████▏     | 77/186 [14:53<21:36, 11.89s/it] 42%|████▏     | 78/186 [15:05<21:22, 11.87s/it] 42%|████▏     | 79/186 [15:16<21:08, 11.85s/it] 43%|████▎     | 80/186 [15:28<20:54, 11.84s/it] 44%|████▎     | 81/186 [15:40<20:45, 11.86s/it] 44%|████▍     | 82/186 [15:52<20:34, 11.87s/it] 45%|████▍     | 83/186 [16:04<20:19, 11.84s/it] 45%|████▌     | 84/186 [16:15<20:02, 11.79s/it] 46%|████▌     | 85/186 [16:27<19:45, 11.74s/it] 46%|████▌     | 86/186 [16:39<19:34, 11.75s/it] 47%|████▋     | 87/186 [16:51<19:25, 11.78s/it] 47%|████▋     | 88/186 [17:02<19:08, 11.72s/it] 48%|████▊     | 89/186 [17:14<19:03, 11.79s/it] 48%|████▊     | 90/186 [17:26<18:46, 11.74s/it] 49%|████▉     | 91/186 [17:38<18:48, 11.88s/it] 49%|████▉     | 92/186 [17:50<18:32, 11.84s/it] 50%|█████     | 93/186 [18:02<18:25, 11.89s/it] 51%|█████     | 94/186 [18:13<18:08, 11.83s/it] 51%|█████     | 95/186 [18:25<18:00, 11.87s/it] 52%|█████▏    | 96/186 [18:37<17:39, 11.77s/it] 52%|█████▏    | 97/186 [18:49<17:29, 11.79s/it] 53%|█████▎    | 98/186 [19:01<17:20, 11.82s/it] 53%|█████▎    | 99/186 [19:13<17:07, 11.81s/it] 54%|█████▍    | 100/186 [19:24<16:50, 11.75s/it] 54%|█████▍    | 101/186 [19:36<16:35, 11.71s/it] 55%|█████▍    | 102/186 [19:48<16:34, 11.84s/it] 55%|█████▌    | 103/186 [20:01<16:43, 12.09s/it] 56%|█████▌    | 104/186 [20:12<16:25, 12.01s/it] 56%|█████▋    | 105/186 [20:24<16:02, 11.89s/it] 57%|█████▋    | 106/186 [20:36<15:44, 11.81s/it] 58%|█████▊    | 107/186 [20:47<15:30, 11.78s/it] 58%|█████▊    | 108/186 [20:59<15:22, 11.83s/it] 59%|█████▊    | 109/186 [21:11<15:03, 11.74s/it] 59%|█████▉    | 110/186 [21:22<14:48, 11.69s/it] 60%|█████▉    | 111/186 [21:34<14:39, 11.73s/it] 60%|██████    | 112/186 [21:46<14:32, 11.79s/it] 61%|██████    | 113/186 [21:58<14:19, 11.78s/it] 61%|██████▏   | 114/186 [22:10<14:07, 11.77s/it] 62%|██████▏   | 115/186 [22:22<13:58, 11.80s/it] 62%|██████▏   | 116/186 [22:34<13:55, 11.94s/it] 63%|██████▎   | 117/186 [22:45<13:36, 11.84s/it] 63%|██████▎   | 118/186 [22:57<13:22, 11.80s/it] 64%|██████▍   | 119/186 [23:09<13:04, 11.70s/it] 65%|██████▍   | 120/186 [23:20<12:47, 11.64s/it] 65%|██████▌   | 121/186 [23:32<12:35, 11.62s/it] 66%|██████▌   | 122/186 [23:44<12:29, 11.71s/it] 66%|██████▌   | 123/186 [23:55<12:20, 11.75s/it] 67%|██████▋   | 124/186 [24:07<12:12, 11.82s/it] 67%|██████▋   | 125/186 [24:19<12:01, 11.82s/it] 68%|██████▊   | 126/186 [24:31<11:48, 11.80s/it] 68%|██████▊   | 127/186 [24:43<11:34, 11.77s/it] 69%|██████▉   | 128/186 [24:54<11:18, 11.69s/it] 69%|██████▉   | 129/186 [25:06<11:05, 11.68s/it] 70%|██████▉   | 130/186 [25:18<11:00, 11.79s/it] 70%|███████   | 131/186 [25:29<10:45, 11.74s/it] 71%|███████   | 132/186 [25:41<10:35, 11.77s/it] 72%|███████▏  | 133/186 [25:53<10:23, 11.76s/it] 72%|███████▏  | 134/186 [26:05<10:16, 11.86s/it] 73%|███████▎  | 135/186 [26:17<09:59, 11.75s/it] 73%|███████▎  | 136/186 [26:28<09:44, 11.70s/it] 74%|███████▎  | 137/186 [26:40<09:37, 11.79s/it] 74%|███████▍  | 138/186 [26:52<09:22, 11.73s/it] 75%|███████▍  | 139/186 [27:04<09:14, 11.81s/it] 75%|███████▌  | 140/186 [27:15<08:59, 11.74s/it] 76%|███████▌  | 141/186 [27:27<08:50, 11.78s/it] 76%|███████▋  | 142/186 [27:39<08:35, 11.72s/it] 77%|███████▋  | 143/186 [27:51<08:25, 11.75s/it] 77%|███████▋  | 144/186 [28:03<08:15, 11.79s/it] 78%|███████▊  | 145/186 [28:14<08:01, 11.73s/it] 78%|███████▊  | 146/186 [28:26<07:49, 11.75s/it] 79%|███████▉  | 147/186 [28:38<07:37, 11.74s/it] 80%|███████▉  | 148/186 [28:49<07:24, 11.71s/it] 80%|████████  | 149/186 [29:01<07:11, 11.67s/it] 81%|████████  | 150/186 [29:12<06:59, 11.65s/it] 81%|████████  | 151/186 [29:24<06:45, 11.60s/it] 82%|████████▏ | 152/186 [29:36<06:35, 11.65s/it] 82%|████████▏ | 153/186 [29:47<06:23, 11.63s/it] 83%|████████▎ | 154/186 [29:59<06:10, 11.56s/it] 83%|████████▎ | 155/186 [30:10<05:59, 11.59s/it] 84%|████████▍ | 156/186 [30:22<05:51, 11.72s/it] 84%|████████▍ | 157/186 [30:34<05:39, 11.72s/it] 85%|████████▍ | 158/186 [30:46<05:26, 11.67s/it] 85%|████████▌ | 159/186 [30:57<05:13, 11.61s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
 86%|████████▌ | 160/186 [31:09<05:02, 11.63s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 87%|████████▋ | 161/186 [31:20<04:50, 11.63s/it] 87%|████████▋ | 162/186 [31:33<04:43, 11.81s/it] 88%|████████▊ | 163/186 [31:44<04:30, 11.75s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1307 > 1048). Running this sequence through the model will result in indexing errors
 88%|████████▊ | 164/186 [31:56<04:18, 11.73s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 89%|████████▊ | 165/186 [32:07<04:04, 11.65s/it] 89%|████████▉ | 166/186 [32:19<03:52, 11.60s/it] 90%|████████▉ | 167/186 [32:30<03:39, 11.56s/it] 90%|█████████ | 168/186 [32:42<03:28, 11.58s/it] 91%|█████████ | 169/186 [32:54<03:17, 11.60s/it] 91%|█████████▏| 170/186 [33:05<03:05, 11.60s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 92%|█████████▏| 171/186 [33:17<02:56, 11.76s/it] 92%|█████████▏| 172/186 [33:29<02:43, 11.66s/it] 93%|█████████▎| 173/186 [33:41<02:32, 11.72s/it] 94%|█████████▎| 174/186 [33:52<02:19, 11.66s/it] 94%|█████████▍| 175/186 [34:04<02:07, 11.60s/it] 95%|█████████▍| 176/186 [34:15<01:55, 11.58s/it] 95%|█████████▌| 177/186 [34:27<01:44, 11.56s/it] 96%|█████████▌| 178/186 [34:38<01:32, 11.55s/it] 96%|█████████▌| 179/186 [34:50<01:20, 11.49s/it] 97%|█████████▋| 180/186 [35:01<01:09, 11.56s/it] 97%|█████████▋| 181/186 [35:13<00:58, 11.64s/it] 98%|█████████▊| 182/186 [35:25<00:46, 11.63s/it] 98%|█████████▊| 183/186 [35:36<00:34, 11.61s/it] 99%|█████████▉| 184/186 [35:48<00:23, 11.67s/it] 99%|█████████▉| 185/186 [36:00<00:11, 11.73s/it]100%|██████████| 186/186 [36:11<00:00, 11.67s/it]                                                 {'train_runtime': 2171.9887, 'train_samples_per_second': 6.186, 'train_steps_per_second': 0.086, 'train_loss': 0.7611968337848622, 'epoch': 1.0}
100%|██████████| 186/186 [36:12<00:00, 11.67s/it]100%|██████████| 186/186 [36:12<00:00, 11.68s/it]
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-2
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33mdirect-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-2[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251015_041327-v3ess6vd/logs[0m
[2025-10-15 04:51:27,375] [INFO] [launch.py:347:main] Process 492203 exits successfully.
[2025-10-15 04:51:27,375] [INFO] [launch.py:347:main] Process 492200 exits successfully.
[2025-10-15 04:51:27,376] [INFO] [launch.py:347:main] Process 492204 exits successfully.
[2025-10-15 04:51:27,376] [INFO] [launch.py:347:main] Process 492202 exits successfully.
[2025-10-15 04:51:27,376] [INFO] [launch.py:347:main] Process 492201 exits successfully.
[2025-10-15 04:51:32,381] [INFO] [launch.py:347:main] Process 492199 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-15 04:51:35,600] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 04:51:36,157] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-15 04:51:36,157] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12354 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.7-2 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.7-2 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 0.7 --use_anchor False --yilin_anchor True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-15 04:51:37,837] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 04:51:38,441] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-15 04:51:38,441] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-15 04:51:38,441] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-15 04:51:38,441] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-15 04:51:38,441] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-15 04:51:42,137] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 04:51:42,208] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 04:51:42,482] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-15 04:51:42,482] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 04:51:42,549] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-15 04:51:42,631] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-15 04:51:42,827] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 04:51:42,955] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 04:51:43,148] [INFO] [comm.py:637:init_distributed] cdb=None
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
[2025-10-15 04:51:43,978] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 04:51:44,173] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 04:51:44,399] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 04:51:44,726] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
wandb: creating run
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251015_045144-fiuy0qgz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.7-2
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/fiuy0qgz
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.53s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.90s/it]
to bfloat16...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.03s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.16s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.88s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.20s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.90s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.24s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.74s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.51s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.72s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.59s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.91s/it]
to bfloat16...
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.69s/it]
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.21s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.34s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.75s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.17s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.67s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.98s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.88s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.23s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.86s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.20s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.16s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.12s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.57s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.81s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.65s/it]
Formatting inputs...Skip in lazy mode
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.013230085372924805 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.013698339462280273 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.010438919067382812 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.01098179817199707 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10117745399475098 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10180521011352539 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Token indices sequence length is longer than the specified maximum sequence length for this model (1153 > 1048). Running this sequence through the model will result in indexing errors
  1%|          | 1/186 [00:09<27:58,  9.08s/it]  1%|          | 2/186 [00:19<30:25,  9.92s/it]  2%|▏         | 3/186 [00:29<30:38, 10.05s/it]  2%|▏         | 4/186 [00:40<31:09, 10.27s/it]  3%|▎         | 5/186 [00:50<31:15, 10.36s/it]  3%|▎         | 6/186 [01:01<31:01, 10.34s/it]  4%|▍         | 7/186 [01:11<31:10, 10.45s/it]  4%|▍         | 8/186 [01:22<31:11, 10.51s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1065 > 1048). Running this sequence through the model will result in indexing errors
  5%|▍         | 9/186 [01:33<31:16, 10.60s/it]  5%|▌         | 10/186 [01:44<31:30, 10.74s/it]  6%|▌         | 11/186 [01:55<31:40, 10.86s/it]  6%|▋         | 12/186 [02:06<31:30, 10.86s/it]  7%|▋         | 13/186 [02:17<31:10, 10.81s/it]  8%|▊         | 14/186 [02:27<31:00, 10.82s/it]  8%|▊         | 15/186 [02:38<30:50, 10.82s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
  9%|▊         | 16/186 [02:49<30:41, 10.83s/it]  9%|▉         | 17/186 [03:00<30:39, 10.88s/it] 10%|▉         | 18/186 [03:11<30:17, 10.82s/it] 10%|█         | 19/186 [03:21<29:57, 10.76s/it] 11%|█         | 20/186 [03:32<29:45, 10.76s/it] 11%|█▏        | 21/186 [03:43<29:26, 10.71s/it] 12%|█▏        | 22/186 [03:54<29:21, 10.74s/it] 12%|█▏        | 23/186 [04:04<29:05, 10.71s/it] 13%|█▎        | 24/186 [04:15<29:19, 10.86s/it] 13%|█▎        | 25/186 [04:26<29:13, 10.89s/it] 14%|█▍        | 26/186 [04:37<28:53, 10.83s/it] 15%|█▍        | 27/186 [04:48<28:52, 10.90s/it] 15%|█▌        | 28/186 [04:59<28:52, 10.97s/it] 16%|█▌        | 29/186 [05:10<28:32, 10.91s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
 16%|█▌        | 30/186 [05:21<28:11, 10.85s/it] 17%|█▋        | 31/186 [05:31<27:47, 10.76s/it] 17%|█▋        | 32/186 [05:42<27:32, 10.73s/it] 18%|█▊        | 33/186 [05:53<27:16, 10.70s/it] 18%|█▊        | 34/186 [06:03<27:05, 10.70s/it] 19%|█▉        | 35/186 [06:14<27:15, 10.83s/it] 19%|█▉        | 36/186 [06:25<26:52, 10.75s/it] 20%|█▉        | 37/186 [06:35<26:26, 10.65s/it] 20%|██        | 38/186 [06:46<26:27, 10.73s/it] 21%|██        | 39/186 [06:57<26:23, 10.77s/it] 22%|██▏       | 40/186 [07:08<25:58, 10.68s/it]WARNING: tokenization mismatch: 1 vs. 48. (ignored)
 22%|██▏       | 41/186 [07:19<25:57, 10.74s/it] 23%|██▎       | 42/186 [07:29<25:44, 10.73s/it] 23%|██▎       | 43/186 [07:40<25:33, 10.72s/it] 24%|██▎       | 44/186 [07:51<25:27, 10.76s/it] 24%|██▍       | 45/186 [08:02<25:18, 10.77s/it] 25%|██▍       | 46/186 [08:12<25:08, 10.77s/it] 25%|██▌       | 47/186 [08:24<25:16, 10.91s/it] 26%|██▌       | 48/186 [08:34<24:45, 10.77s/it] 26%|██▋       | 49/186 [08:45<24:47, 10.86s/it] 27%|██▋       | 50/186 [08:56<24:34, 10.85s/it] 27%|██▋       | 51/186 [09:07<24:23, 10.84s/it] 28%|██▊       | 52/186 [09:18<24:11, 10.83s/it] 28%|██▊       | 53/186 [09:28<23:51, 10.76s/it] 29%|██▉       | 54/186 [09:39<23:44, 10.79s/it] 30%|██▉       | 55/186 [09:50<23:30, 10.77s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1125 > 1048). Running this sequence through the model will result in indexing errors
 30%|███       | 56/186 [10:01<23:22, 10.79s/it] 31%|███       | 57/186 [10:11<23:05, 10.74s/it] 31%|███       | 58/186 [10:22<22:53, 10.73s/it] 32%|███▏      | 59/186 [10:33<23:00, 10.87s/it] 32%|███▏      | 60/186 [10:44<22:55, 10.92s/it] 33%|███▎      | 61/186 [10:55<22:49, 10.95s/it] 33%|███▎      | 62/186 [11:06<22:29, 10.88s/it] 34%|███▍      | 63/186 [11:17<22:41, 11.07s/it] 34%|███▍      | 64/186 [11:29<22:55, 11.28s/it] 35%|███▍      | 65/186 [11:41<22:53, 11.36s/it] 35%|███▌      | 66/186 [11:53<23:04, 11.54s/it] 36%|███▌      | 67/186 [12:04<22:54, 11.55s/it] 37%|███▋      | 68/186 [12:16<22:44, 11.57s/it] 37%|███▋      | 69/186 [12:28<23:00, 11.80s/it] 38%|███▊      | 70/186 [12:40<23:07, 11.96s/it] 38%|███▊      | 71/186 [12:52<22:49, 11.91s/it] 39%|███▊      | 72/186 [13:04<22:30, 11.84s/it] 39%|███▉      | 73/186 [13:16<22:20, 11.86s/it] 40%|███▉      | 74/186 [13:28<22:19, 11.96s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1160 > 1048). Running this sequence through the model will result in indexing errors
 40%|████      | 75/186 [13:40<22:10, 11.99s/it] 41%|████      | 76/186 [13:52<21:46, 11.88s/it] 41%|████▏     | 77/186 [14:03<21:27, 11.82s/it] 42%|████▏     | 78/186 [14:15<21:07, 11.73s/it] 42%|████▏     | 79/186 [14:26<20:48, 11.67s/it] 43%|████▎     | 80/186 [14:38<20:33, 11.64s/it] 44%|████▎     | 81/186 [14:50<20:20, 11.62s/it] 44%|████▍     | 82/186 [15:01<20:12, 11.66s/it] 45%|████▍     | 83/186 [15:13<20:02, 11.67s/it] 45%|████▌     | 84/186 [15:25<19:56, 11.73s/it] 46%|████▌     | 85/186 [15:37<19:49, 11.78s/it] 46%|████▌     | 86/186 [15:49<19:48, 11.88s/it] 47%|████▋     | 87/186 [16:01<19:39, 11.92s/it] 47%|████▋     | 88/186 [16:13<19:30, 11.95s/it] 48%|████▊     | 89/186 [16:25<19:19, 11.95s/it] 48%|████▊     | 90/186 [16:37<19:01, 11.90s/it] 49%|████▉     | 91/186 [16:49<18:49, 11.89s/it] 49%|████▉     | 92/186 [17:00<18:28, 11.79s/it] 50%|█████     | 93/186 [17:12<18:20, 11.83s/it] 51%|█████     | 94/186 [17:24<17:59, 11.73s/it] 51%|█████     | 95/186 [17:35<17:45, 11.70s/it] 52%|█████▏    | 96/186 [17:47<17:33, 11.71s/it] 52%|█████▏    | 97/186 [17:58<17:17, 11.66s/it] 53%|█████▎    | 98/186 [18:10<17:09, 11.70s/it] 53%|█████▎    | 99/186 [18:22<17:07, 11.81s/it] 54%|█████▍    | 100/186 [18:34<17:00, 11.86s/it] 54%|█████▍    | 101/186 [18:46<16:39, 11.75s/it] 55%|█████▍    | 102/186 [18:58<16:29, 11.78s/it] 55%|█████▌    | 103/186 [19:10<16:21, 11.83s/it] 56%|█████▌    | 104/186 [19:21<16:07, 11.80s/it] 56%|█████▋    | 105/186 [19:33<15:53, 11.77s/it] 57%|█████▋    | 106/186 [19:45<15:43, 11.79s/it] 58%|█████▊    | 107/186 [19:56<15:23, 11.69s/it] 58%|█████▊    | 108/186 [20:08<15:22, 11.83s/it] 59%|█████▊    | 109/186 [20:20<15:15, 11.89s/it] 59%|█████▉    | 110/186 [20:32<14:58, 11.82s/it] 60%|█████▉    | 111/186 [20:44<14:44, 11.79s/it] 60%|██████    | 112/186 [20:55<14:28, 11.73s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 61%|██████    | 113/186 [21:07<14:11, 11.67s/it] 61%|██████▏   | 114/186 [21:19<13:57, 11.64s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 62%|██████▏   | 115/186 [21:30<13:51, 11.72s/it] 62%|██████▏   | 116/186 [21:42<13:46, 11.81s/it] 63%|██████▎   | 117/186 [21:54<13:34, 11.80s/it] 63%|██████▎   | 118/186 [22:06<13:23, 11.82s/it] 64%|██████▍   | 119/186 [22:18<13:09, 11.78s/it] 65%|██████▍   | 120/186 [22:30<12:59, 11.81s/it] 65%|██████▌   | 121/186 [22:41<12:45, 11.78s/it] 66%|██████▌   | 122/186 [22:53<12:31, 11.74s/it] 66%|██████▌   | 123/186 [23:05<12:24, 11.82s/it] 67%|██████▋   | 124/186 [23:17<12:12, 11.82s/it] 67%|██████▋   | 125/186 [23:29<12:09, 11.96s/it] 68%|██████▊   | 126/186 [23:41<11:56, 11.95s/it] 68%|██████▊   | 127/186 [23:53<11:48, 12.00s/it] 69%|██████▉   | 128/186 [24:05<11:29, 11.89s/it] 69%|██████▉   | 129/186 [24:17<11:19, 11.92s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1221 > 1048). Running this sequence through the model will result in indexing errors
 70%|██████▉   | 130/186 [24:29<11:08, 11.94s/it] 70%|███████   | 131/186 [24:40<10:48, 11.79s/it] 71%|███████   | 132/186 [24:52<10:37, 11.81s/it] 72%|███████▏  | 133/186 [25:04<10:20, 11.70s/it] 72%|███████▏  | 134/186 [25:15<10:09, 11.72s/it] 73%|███████▎  | 135/186 [25:27<09:56, 11.70s/it] 73%|███████▎  | 136/186 [25:39<09:45, 11.71s/it] 74%|███████▎  | 137/186 [25:51<09:36, 11.76s/it] 74%|███████▍  | 138/186 [26:02<09:21, 11.71s/it] 75%|███████▍  | 139/186 [26:14<09:18, 11.89s/it] 75%|███████▌  | 140/186 [26:26<09:03, 11.81s/it] 76%|███████▌  | 141/186 [26:38<08:48, 11.75s/it] 76%|███████▋  | 142/186 [26:50<08:37, 11.77s/it] 77%|███████▋  | 143/186 [27:01<08:22, 11.69s/it] 77%|███████▋  | 144/186 [27:13<08:08, 11.64s/it] 78%|███████▊  | 145/186 [27:24<07:56, 11.61s/it] 78%|███████▊  | 146/186 [27:36<07:46, 11.67s/it] 79%|███████▉  | 147/186 [27:47<07:32, 11.61s/it] 80%|███████▉  | 148/186 [27:59<07:21, 11.62s/it] 80%|████████  | 149/186 [28:10<07:07, 11.55s/it] 81%|████████  | 150/186 [28:22<06:53, 11.49s/it] 81%|████████  | 151/186 [28:33<06:43, 11.54s/it] 82%|████████▏ | 152/186 [28:45<06:31, 11.50s/it] 82%|████████▏ | 153/186 [28:56<06:19, 11.49s/it] 83%|████████▎ | 154/186 [29:08<06:07, 11.49s/it] 83%|████████▎ | 155/186 [29:19<05:55, 11.47s/it] 84%|████████▍ | 156/186 [29:31<05:47, 11.58s/it] 84%|████████▍ | 157/186 [29:42<05:33, 11.50s/it] 85%|████████▍ | 158/186 [29:54<05:22, 11.52s/it] 85%|████████▌ | 159/186 [30:05<05:10, 11.51s/it] 86%|████████▌ | 160/186 [30:17<04:58, 11.48s/it] 87%|████████▋ | 161/186 [30:28<04:47, 11.50s/it] 87%|████████▋ | 162/186 [30:40<04:39, 11.66s/it] 88%|████████▊ | 163/186 [30:52<04:28, 11.68s/it] 88%|████████▊ | 164/186 [31:04<04:16, 11.65s/it] 89%|████████▊ | 165/186 [31:16<04:05, 11.70s/it] 89%|████████▉ | 166/186 [31:27<03:52, 11.64s/it] 90%|████████▉ | 167/186 [31:39<03:40, 11.59s/it] 90%|█████████ | 168/186 [31:50<03:29, 11.63s/it] 91%|█████████ | 169/186 [32:02<03:17, 11.61s/it] 91%|█████████▏| 170/186 [32:13<03:05, 11.60s/it] 92%|█████████▏| 171/186 [32:25<02:54, 11.64s/it] 92%|█████████▏| 172/186 [32:37<02:42, 11.59s/it] 93%|█████████▎| 173/186 [32:48<02:30, 11.57s/it] 94%|█████████▎| 174/186 [33:00<02:18, 11.57s/it] 94%|█████████▍| 175/186 [33:11<02:07, 11.56s/it] 95%|█████████▍| 176/186 [33:23<01:55, 11.56s/it] 95%|█████████▌| 177/186 [33:34<01:44, 11.56s/it] 96%|█████████▌| 178/186 [33:46<01:32, 11.54s/it] 96%|█████████▌| 179/186 [33:58<01:21, 11.59s/it] 97%|█████████▋| 180/186 [34:09<01:09, 11.53s/it] 97%|█████████▋| 181/186 [34:20<00:57, 11.52s/it] 98%|█████████▊| 182/186 [34:32<00:46, 11.62s/it] 98%|█████████▊| 183/186 [34:44<00:34, 11.59s/it] 99%|█████████▉| 184/186 [34:55<00:23, 11.59s/it] 99%|█████████▉| 185/186 [35:07<00:11, 11.70s/it]100%|██████████| 186/186 [35:19<00:00, 11.63s/it]                                                 {'train_runtime': 2119.3236, 'train_samples_per_second': 6.339, 'train_steps_per_second': 0.088, 'train_loss': 0.7810526201801915, 'epoch': 1.0}
100%|██████████| 186/186 [35:20<00:00, 11.63s/it]100%|██████████| 186/186 [35:20<00:00, 11.40s/it]
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.7-2
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33mdirect-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.7-2[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251015_045144-fiuy0qgz/logs[0m
[2025-10-15 05:28:52,157] [INFO] [launch.py:347:main] Process 493936 exits successfully.
[2025-10-15 05:28:52,158] [INFO] [launch.py:347:main] Process 493940 exits successfully.
[2025-10-15 05:28:52,158] [INFO] [launch.py:347:main] Process 493938 exits successfully.
[2025-10-15 05:28:52,158] [INFO] [launch.py:347:main] Process 493937 exits successfully.
[2025-10-15 05:28:53,159] [INFO] [launch.py:347:main] Process 493939 exits successfully.
[2025-10-15 05:28:57,164] [INFO] [launch.py:347:main] Process 493935 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-15 05:29:00,773] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 05:29:01,328] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-15 05:29:01,328] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12354 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.3-2 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.3-2 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 0.3 --use_anchor False --yilin_anchor True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-15 05:29:02,768] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 05:29:03,330] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-15 05:29:03,330] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-15 05:29:03,330] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-15 05:29:03,330] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-15 05:29:03,330] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-15 05:29:07,074] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 05:29:07,082] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 05:29:07,391] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 05:29:07,408] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-15 05:29:07,653] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 05:29:07,971] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-15 05:29:08,446] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 05:29:08,524] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 05:29:08,756] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 05:29:08,835] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-15 05:29:08,835] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-10-15 05:29:08,967] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 05:29:09,464] [INFO] [comm.py:637:init_distributed] cdb=None
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251015_052909-u5xh4yla
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.3-2
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/u5xh4yla
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.48s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.22s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.41s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.14s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.06s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.89s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.19s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.48s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.92s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.73s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.08s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.96s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.75s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.12s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.73s/it]
to bfloat16...
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.73s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.92s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.19s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.20s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.11s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.01s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.05s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.03s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.67s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.05s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.55s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.93s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.45s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.83s/it]
Formatting inputs...Skip in lazy mode
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.63s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.99s/it]
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...

Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.010760307312011719 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Time to load fused_adam op: 0.012280464172363281 seconds
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.010519266128540039 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.009557247161865234 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10103368759155273 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10170316696166992 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/186 [00:08<26:46,  8.68s/it]  1%|          | 2/186 [00:18<28:56,  9.44s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1089 > 1048). Running this sequence through the model will result in indexing errors
  2%|▏         | 3/186 [00:29<30:35, 10.03s/it]  2%|▏         | 4/186 [00:40<31:21, 10.34s/it]  3%|▎         | 5/186 [00:50<31:34, 10.46s/it]  3%|▎         | 6/186 [01:01<31:34, 10.52s/it]  4%|▍         | 7/186 [01:12<31:45, 10.64s/it]  4%|▍         | 8/186 [01:23<32:24, 10.92s/it]  5%|▍         | 9/186 [01:35<32:47, 11.11s/it]  5%|▌         | 10/186 [01:46<32:53, 11.21s/it]  6%|▌         | 11/186 [01:58<33:06, 11.35s/it]  6%|▋         | 12/186 [02:10<33:09, 11.43s/it]  7%|▋         | 13/186 [02:21<33:03, 11.47s/it]  8%|▊         | 14/186 [02:33<32:48, 11.44s/it]  8%|▊         | 15/186 [02:45<33:09, 11.63s/it]  9%|▊         | 16/186 [02:56<32:59, 11.64s/it]  9%|▉         | 17/186 [03:08<32:39, 11.60s/it] 10%|▉         | 18/186 [03:19<32:29, 11.61s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1160 > 1048). Running this sequence through the model will result in indexing errors
 10%|█         | 19/186 [03:31<32:32, 11.69s/it] 11%|█         | 20/186 [03:43<32:15, 11.66s/it] 11%|█▏        | 21/186 [03:54<31:48, 11.57s/it] 12%|█▏        | 22/186 [04:06<31:34, 11.55s/it] 12%|█▏        | 23/186 [04:17<31:22, 11.55s/it] 13%|█▎        | 24/186 [04:30<31:52, 11.81s/it] 13%|█▎        | 25/186 [04:42<31:44, 11.83s/it] 14%|█▍        | 26/186 [04:53<31:14, 11.72s/it] 15%|█▍        | 27/186 [05:05<31:13, 11.78s/it] 15%|█▌        | 28/186 [05:16<30:45, 11.68s/it] 16%|█▌        | 29/186 [05:28<30:23, 11.61s/it] 16%|█▌        | 30/186 [05:39<30:07, 11.58s/it] 17%|█▋        | 31/186 [05:51<29:55, 11.58s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1075 > 1048). Running this sequence through the model will result in indexing errors
 17%|█▋        | 32/186 [06:03<29:51, 11.64s/it] 18%|█▊        | 33/186 [06:14<29:33, 11.59s/it] 18%|█▊        | 34/186 [06:26<29:12, 11.53s/it] 19%|█▉        | 35/186 [06:38<29:14, 11.62s/it] 19%|█▉        | 36/186 [06:50<29:19, 11.73s/it] 20%|█▉        | 37/186 [07:01<28:59, 11.68s/it] 20%|██        | 38/186 [07:13<28:46, 11.66s/it] 21%|██        | 39/186 [07:24<28:36, 11.68s/it] 22%|██▏       | 40/186 [07:36<28:33, 11.73s/it] 22%|██▏       | 41/186 [07:48<28:15, 11.69s/it] 23%|██▎       | 42/186 [07:59<27:58, 11.66s/it] 23%|██▎       | 43/186 [08:11<27:51, 11.69s/it] 24%|██▎       | 44/186 [08:23<27:25, 11.59s/it] 24%|██▍       | 45/186 [08:34<27:06, 11.54s/it] 25%|██▍       | 46/186 [08:45<26:52, 11.52s/it] 25%|██▌       | 47/186 [08:58<27:12, 11.74s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 26%|██▌       | 48/186 [09:09<26:54, 11.70s/it] 26%|██▋       | 49/186 [09:21<26:47, 11.74s/it] 27%|██▋       | 50/186 [09:33<26:38, 11.75s/it] 27%|██▋       | 51/186 [09:45<26:20, 11.71s/it] 28%|██▊       | 52/186 [09:56<26:09, 11.71s/it] 28%|██▊       | 53/186 [10:08<25:48, 11.64s/it] 29%|██▉       | 54/186 [10:19<25:39, 11.66s/it] 30%|██▉       | 55/186 [10:31<25:24, 11.64s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1158 > 1048). Running this sequence through the model will result in indexing errors
 30%|███       | 56/186 [10:43<25:25, 11.73s/it] 31%|███       | 57/186 [10:54<25:05, 11.67s/it] 31%|███       | 58/186 [11:06<24:52, 11.66s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1150 > 1048). Running this sequence through the model will result in indexing errors
 32%|███▏      | 59/186 [11:18<25:01, 11.82s/it] 32%|███▏      | 60/186 [11:30<24:41, 11.76s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1102 > 1048). Running this sequence through the model will result in indexing errors
 33%|███▎      | 61/186 [11:42<24:28, 11.75s/it] 33%|███▎      | 62/186 [11:53<24:11, 11.70s/it] 34%|███▍      | 63/186 [12:05<23:52, 11.65s/it] 34%|███▍      | 64/186 [12:16<23:38, 11.63s/it] 35%|███▍      | 65/186 [12:28<23:42, 11.76s/it] 35%|███▌      | 66/186 [12:41<23:47, 11.89s/it] 36%|███▌      | 67/186 [12:52<23:23, 11.79s/it] 37%|███▋      | 68/186 [13:04<23:07, 11.76s/it] 37%|███▋      | 69/186 [13:16<23:00, 11.80s/it] 38%|███▊      | 70/186 [13:28<22:49, 11.81s/it] 38%|███▊      | 71/186 [13:39<22:25, 11.70s/it] 39%|███▊      | 72/186 [13:51<22:10, 11.67s/it] 39%|███▉      | 73/186 [14:03<22:09, 11.76s/it] 40%|███▉      | 74/186 [14:14<22:00, 11.79s/it] 40%|████      | 75/186 [14:26<21:48, 11.79s/it] 41%|████      | 76/186 [14:38<21:28, 11.71s/it] 41%|████▏     | 77/186 [14:49<21:12, 11.67s/it] 42%|████▏     | 78/186 [15:01<20:59, 11.66s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 42%|████▏     | 79/186 [15:13<20:54, 11.73s/it] 43%|████▎     | 80/186 [15:25<20:51, 11.81s/it] 44%|████▎     | 81/186 [15:37<20:37, 11.79s/it] 44%|████▍     | 82/186 [15:48<20:22, 11.76s/it] 45%|████▍     | 83/186 [16:00<20:05, 11.71s/it] 45%|████▌     | 84/186 [16:11<19:49, 11.66s/it] 46%|████▌     | 85/186 [16:23<19:38, 11.66s/it] 46%|████▌     | 86/186 [16:35<19:21, 11.62s/it] 47%|████▋     | 87/186 [16:46<19:11, 11.64s/it] 47%|████▋     | 88/186 [16:58<19:05, 11.69s/it] 48%|████▊     | 89/186 [17:10<19:00, 11.75s/it] 48%|████▊     | 90/186 [17:22<18:41, 11.69s/it] 49%|████▉     | 91/186 [17:34<18:50, 11.90s/it] 49%|████▉     | 92/186 [17:46<18:33, 11.85s/it] 50%|█████     | 93/186 [17:58<18:28, 11.92s/it] 51%|█████     | 94/186 [18:10<18:16, 11.92s/it] 51%|█████     | 95/186 [18:21<17:55, 11.82s/it] 52%|█████▏    | 96/186 [18:33<17:34, 11.72s/it] 52%|█████▏    | 97/186 [18:44<17:19, 11.68s/it] 53%|█████▎    | 98/186 [18:56<17:04, 11.64s/it] 53%|█████▎    | 99/186 [19:08<16:52, 11.64s/it] 54%|█████▍    | 100/186 [19:19<16:41, 11.65s/it] 54%|█████▍    | 101/186 [19:31<16:33, 11.69s/it] 55%|█████▍    | 102/186 [19:43<16:21, 11.69s/it] 55%|█████▌    | 103/186 [19:55<16:14, 11.74s/it] 56%|█████▌    | 104/186 [20:06<16:04, 11.76s/it] 56%|█████▋    | 105/186 [20:18<15:46, 11.69s/it] 57%|█████▋    | 106/186 [20:30<15:35, 11.70s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 58%|█████▊    | 107/186 [20:41<15:25, 11.72s/it] 58%|█████▊    | 108/186 [20:53<15:12, 11.70s/it] 59%|█████▊    | 109/186 [21:05<14:57, 11.66s/it] 59%|█████▉    | 110/186 [21:17<14:53, 11.75s/it] 60%|█████▉    | 111/186 [21:28<14:38, 11.71s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 60%|██████    | 112/186 [21:40<14:27, 11.73s/it] 61%|██████    | 113/186 [21:52<14:25, 11.86s/it] 61%|██████▏   | 114/186 [22:04<14:20, 11.95s/it] 62%|██████▏   | 115/186 [22:16<14:09, 11.96s/it] 62%|██████▏   | 116/186 [22:28<14:03, 12.05s/it] 63%|██████▎   | 117/186 [22:40<13:44, 11.95s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 63%|██████▎   | 118/186 [22:52<13:32, 11.94s/it] 64%|██████▍   | 119/186 [23:04<13:11, 11.82s/it] 65%|██████▍   | 120/186 [23:15<12:56, 11.76s/it] 65%|██████▌   | 121/186 [23:27<12:45, 11.78s/it] 66%|██████▌   | 122/186 [23:39<12:31, 11.75s/it] 66%|██████▌   | 123/186 [23:51<12:22, 11.79s/it] 67%|██████▋   | 124/186 [24:02<12:10, 11.79s/it] 67%|██████▋   | 125/186 [24:14<11:55, 11.73s/it] 68%|██████▊   | 126/186 [24:26<11:43, 11.73s/it] 68%|██████▊   | 127/186 [24:37<11:30, 11.70s/it] 69%|██████▉   | 128/186 [24:49<11:23, 11.79s/it] 69%|██████▉   | 129/186 [25:01<11:09, 11.74s/it] 70%|██████▉   | 130/186 [25:13<10:56, 11.73s/it] 70%|███████   | 131/186 [25:24<10:40, 11.65s/it] 71%|███████   | 132/186 [25:36<10:36, 11.79s/it] 72%|███████▏  | 133/186 [25:48<10:21, 11.73s/it] 72%|███████▏  | 134/186 [26:00<10:09, 11.71s/it] 73%|███████▎  | 135/186 [26:11<09:54, 11.65s/it] 73%|███████▎  | 136/186 [26:23<09:46, 11.72s/it] 74%|███████▎  | 137/186 [26:35<09:37, 11.79s/it] 74%|███████▍  | 138/186 [26:46<09:22, 11.72s/it] 75%|███████▍  | 139/186 [26:59<09:21, 11.94s/it] 75%|███████▌  | 140/186 [27:11<09:07, 11.90s/it] 76%|███████▌  | 141/186 [27:23<08:55, 11.89s/it] 76%|███████▋  | 142/186 [27:34<08:42, 11.88s/it] 77%|███████▋  | 143/186 [27:47<08:34, 11.97s/it] 77%|███████▋  | 144/186 [27:59<08:22, 11.96s/it] 78%|███████▊  | 145/186 [28:11<08:13, 12.04s/it] 78%|███████▊  | 146/186 [28:22<07:56, 11.92s/it] 79%|███████▉  | 147/186 [28:34<07:42, 11.85s/it] 80%|███████▉  | 148/186 [28:46<07:28, 11.79s/it] 80%|████████  | 149/186 [28:58<07:15, 11.78s/it] 81%|████████  | 150/186 [29:09<07:03, 11.76s/it] 81%|████████  | 151/186 [29:21<06:52, 11.78s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 82%|████████▏ | 152/186 [29:33<06:41, 11.80s/it] 82%|████████▏ | 153/186 [29:45<06:27, 11.74s/it] 83%|████████▎ | 154/186 [29:56<06:14, 11.70s/it] 83%|████████▎ | 155/186 [30:08<06:03, 11.74s/it] 84%|████████▍ | 156/186 [30:19<05:50, 11.67s/it] 84%|████████▍ | 157/186 [30:31<05:38, 11.66s/it] 85%|████████▍ | 158/186 [30:43<05:28, 11.73s/it] 85%|████████▌ | 159/186 [30:55<05:16, 11.74s/it] 86%|████████▌ | 160/186 [31:06<05:03, 11.67s/it] 87%|████████▋ | 161/186 [31:18<04:51, 11.65s/it] 87%|████████▋ | 162/186 [31:30<04:43, 11.79s/it] 88%|████████▊ | 163/186 [31:42<04:30, 11.77s/it] 88%|████████▊ | 164/186 [31:53<04:18, 11.77s/it] 89%|████████▊ | 165/186 [32:06<04:08, 11.85s/it] 89%|████████▉ | 166/186 [32:17<03:56, 11.80s/it] 90%|████████▉ | 167/186 [32:29<03:44, 11.84s/it] 90%|█████████ | 168/186 [32:41<03:34, 11.92s/it] 91%|█████████ | 169/186 [32:53<03:21, 11.86s/it] 91%|█████████▏| 170/186 [33:05<03:10, 11.89s/it] 92%|█████████▏| 171/186 [33:17<02:58, 11.92s/it] 92%|█████████▏| 172/186 [33:29<02:46, 11.89s/it] 93%|█████████▎| 173/186 [33:41<02:34, 11.88s/it] 94%|█████████▎| 174/186 [33:53<02:23, 12.00s/it] 94%|█████████▍| 175/186 [34:04<02:10, 11.88s/it] 95%|█████████▍| 176/186 [34:16<01:59, 11.91s/it] 95%|█████████▌| 177/186 [34:28<01:46, 11.84s/it] 96%|█████████▌| 178/186 [34:40<01:34, 11.80s/it] 96%|█████████▌| 179/186 [34:52<01:22, 11.78s/it] 97%|█████████▋| 180/186 [35:03<01:10, 11.72s/it] 97%|█████████▋| 181/186 [35:15<00:59, 11.88s/it] 98%|█████████▊| 182/186 [35:27<00:47, 11.82s/it] 98%|█████████▊| 183/186 [35:39<00:35, 11.71s/it] 99%|█████████▉| 184/186 [35:50<00:23, 11.68s/it] 99%|█████████▉| 185/186 [36:02<00:11, 11.83s/it]100%|██████████| 186/186 [36:14<00:00, 11.88s/it]                                                 {'train_runtime': 2174.8337, 'train_samples_per_second': 6.177, 'train_steps_per_second': 0.086, 'train_loss': 0.7645581973496304, 'epoch': 1.0}
100%|██████████| 186/186 [36:15<00:00, 11.88s/it]100%|██████████| 186/186 [36:15<00:00, 11.70s/it]
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.3-2
[2025-10-15 06:07:13,142] [INFO] [launch.py:347:main] Process 495675 exits successfully.
[2025-10-15 06:07:13,143] [INFO] [launch.py:347:main] Process 495676 exits successfully.
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33mdirect-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.3-2[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251015_052909-u5xh4yla/logs[0m
[2025-10-15 06:07:15,145] [INFO] [launch.py:347:main] Process 495678 exits successfully.
[2025-10-15 06:07:15,146] [INFO] [launch.py:347:main] Process 495679 exits successfully.
[2025-10-15 06:07:15,146] [INFO] [launch.py:347:main] Process 495677 exits successfully.
[2025-10-15 06:07:22,154] [INFO] [launch.py:347:main] Process 495674 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-15 06:07:25,389] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 06:07:25,944] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-15 06:07:25,944] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12354 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-2 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-2 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 1 --use_anchor False --yilin_anchor True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-15 06:07:27,597] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 06:07:28,173] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-15 06:07:28,173] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-15 06:07:28,173] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-15 06:07:28,173] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-15 06:07:28,173] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-15 06:07:31,843] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-15 06:07:32,101] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 06:07:32,180] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 06:07:32,419] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-15 06:07:33,028] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 06:07:33,060] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 06:07:33,128] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 06:07:33,344] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 06:07:33,378] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-15 06:07:33,413] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 06:07:33,444] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-15 06:07:33,444] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 06:07:33,749] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: creating run
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251015_060734-e65k2txz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-2
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/e65k2txz
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.94s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.13s/it]
to bfloat16...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.24s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.95s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.29s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.71s/it]to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.74s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.81s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.26s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.74s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.61s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.93s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.66s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.98s/it]
to bfloat16...
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.71s/it]
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.12s/it]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.26s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.54s/it]
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.69s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.26s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.86s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.16s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.96s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.37s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.71s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.09s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.90s/it]
Formatting inputs...Skip in lazy mode
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.86s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.76s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.10s/it]
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.012411832809448242 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.014154672622680664 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10132765769958496 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.01256251335144043 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.015194892883300781 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10145950317382812 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/186 [00:16<52:24, 17.00s/it]  1%|          | 2/186 [00:26<39:29, 12.88s/it]  2%|▏         | 3/186 [00:37<36:19, 11.91s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1074 > 1048). Running this sequence through the model will result in indexing errors
  2%|▏         | 4/186 [00:49<36:00, 11.87s/it]  3%|▎         | 5/186 [01:01<35:39, 11.82s/it]  3%|▎         | 6/186 [01:13<35:24, 11.80s/it]  4%|▍         | 7/186 [01:24<35:15, 11.82s/it]  4%|▍         | 8/186 [01:36<34:44, 11.71s/it]  5%|▍         | 9/186 [01:48<34:29, 11.69s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
  5%|▌         | 10/186 [01:59<34:19, 11.70s/it]  6%|▌         | 11/186 [02:11<33:59, 11.66s/it]  6%|▋         | 12/186 [02:22<33:45, 11.64s/it]  7%|▋         | 13/186 [02:34<33:29, 11.61s/it]  8%|▊         | 14/186 [02:46<33:23, 11.65s/it]  8%|▊         | 15/186 [02:57<33:01, 11.59s/it]  9%|▊         | 16/186 [03:09<33:02, 11.66s/it]  9%|▉         | 17/186 [03:21<32:48, 11.65s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1240 > 1048). Running this sequence through the model will result in indexing errors
 10%|▉         | 18/186 [03:33<32:53, 11.75s/it] 10%|█         | 19/186 [03:44<32:43, 11.76s/it] 11%|█         | 20/186 [03:56<32:34, 11.77s/it] 11%|█▏        | 21/186 [04:08<32:17, 11.74s/it] 12%|█▏        | 22/186 [04:19<31:53, 11.67s/it] 12%|█▏        | 23/186 [04:31<31:38, 11.65s/it] 13%|█▎        | 24/186 [04:43<31:40, 11.73s/it] 13%|█▎        | 25/186 [04:54<31:22, 11.69s/it] 14%|█▍        | 26/186 [05:06<30:49, 11.56s/it] 15%|█▍        | 27/186 [05:17<30:26, 11.49s/it] 15%|█▌        | 28/186 [05:28<29:34, 11.23s/it] 16%|█▌        | 29/186 [05:38<28:54, 11.05s/it] 16%|█▌        | 30/186 [05:49<28:38, 11.01s/it] 17%|█▋        | 31/186 [06:00<28:36, 11.07s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 17%|█▋        | 32/186 [06:12<28:51, 11.24s/it] 18%|█▊        | 33/186 [06:24<29:00, 11.37s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1080 > 1048). Running this sequence through the model will result in indexing errors
 18%|█▊        | 34/186 [06:36<29:11, 11.52s/it] 19%|█▉        | 35/186 [06:47<29:13, 11.61s/it] 19%|█▉        | 36/186 [06:59<29:00, 11.60s/it] 20%|█▉        | 37/186 [07:11<28:51, 11.62s/it] 20%|██        | 38/186 [07:22<28:41, 11.63s/it] 21%|██        | 39/186 [07:34<28:36, 11.68s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 22%|██▏       | 40/186 [07:46<28:20, 11.64s/it] 22%|██▏       | 41/186 [07:57<28:10, 11.66s/it] 23%|██▎       | 42/186 [08:09<27:53, 11.62s/it] 23%|██▎       | 43/186 [08:21<27:42, 11.63s/it] 24%|██▎       | 44/186 [08:32<27:30, 11.62s/it] 24%|██▍       | 45/186 [08:44<27:13, 11.59s/it] 25%|██▍       | 46/186 [08:56<27:13, 11.67s/it] 25%|██▌       | 47/186 [09:07<27:11, 11.73s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 26%|██▌       | 48/186 [09:19<26:55, 11.71s/it] 26%|██▋       | 49/186 [09:31<26:41, 11.69s/it] 27%|██▋       | 50/186 [09:42<26:26, 11.66s/it] 27%|██▋       | 51/186 [09:54<26:07, 11.61s/it] 28%|██▊       | 52/186 [10:05<25:51, 11.57s/it] 28%|██▊       | 53/186 [10:17<25:46, 11.63s/it] 29%|██▉       | 54/186 [10:29<25:49, 11.74s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 30%|██▉       | 55/186 [10:41<25:36, 11.73s/it] 30%|███       | 56/186 [10:53<25:26, 11.74s/it] 31%|███       | 57/186 [11:04<25:06, 11.68s/it] 31%|███       | 58/186 [11:16<24:48, 11.63s/it] 32%|███▏      | 59/186 [11:28<24:53, 11.76s/it] 32%|███▏      | 60/186 [11:39<24:40, 11.75s/it] 33%|███▎      | 61/186 [11:51<24:23, 11.71s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1125 > 1048). Running this sequence through the model will result in indexing errors
 33%|███▎      | 62/186 [12:03<24:19, 11.77s/it] 34%|███▍      | 63/186 [12:15<24:10, 11.79s/it] 34%|███▍      | 64/186 [12:27<24:04, 11.84s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1073 > 1048). Running this sequence through the model will result in indexing errors
 35%|███▍      | 65/186 [12:39<23:53, 11.84s/it] 35%|███▌      | 66/186 [12:51<23:49, 11.91s/it] 36%|███▌      | 67/186 [13:02<23:35, 11.89s/it] 37%|███▋      | 68/186 [13:14<23:15, 11.82s/it] 37%|███▋      | 69/186 [13:26<23:04, 11.83s/it] 38%|███▊      | 70/186 [13:38<22:52, 11.83s/it] 38%|███▊      | 71/186 [13:49<22:32, 11.76s/it] 39%|███▊      | 72/186 [14:01<22:22, 11.78s/it] 39%|███▉      | 73/186 [14:13<22:05, 11.73s/it] 40%|███▉      | 74/186 [14:25<22:00, 11.79s/it] 40%|████      | 75/186 [14:36<21:46, 11.77s/it] 41%|████      | 76/186 [14:48<21:34, 11.77s/it] 41%|████▏     | 77/186 [15:00<21:27, 11.81s/it]WARNING: tokenization mismatch: 1 vs. 47. (ignored)
 42%|████▏     | 78/186 [15:12<21:17, 11.83s/it] 42%|████▏     | 79/186 [15:24<20:59, 11.77s/it] 43%|████▎     | 80/186 [15:36<20:53, 11.82s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 44%|████▎     | 81/186 [15:47<20:32, 11.74s/it] 44%|████▍     | 82/186 [15:59<20:17, 11.70s/it] 45%|████▍     | 83/186 [16:10<20:04, 11.70s/it] 45%|████▌     | 84/186 [16:22<19:49, 11.66s/it] 46%|████▌     | 85/186 [16:34<19:39, 11.68s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 46%|████▌     | 86/186 [16:46<19:36, 11.76s/it] 47%|████▋     | 87/186 [16:57<19:24, 11.76s/it] 47%|████▋     | 88/186 [17:09<19:07, 11.71s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 48%|████▊     | 89/186 [17:21<18:52, 11.68s/it] 48%|████▊     | 90/186 [17:32<18:40, 11.67s/it] 49%|████▉     | 91/186 [17:45<18:44, 11.84s/it] 49%|████▉     | 92/186 [17:56<18:30, 11.81s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 50%|█████     | 93/186 [18:08<18:25, 11.89s/it] 51%|█████     | 94/186 [18:20<18:03, 11.78s/it] 51%|█████     | 95/186 [18:31<17:46, 11.72s/it] 52%|█████▏    | 96/186 [18:43<17:35, 11.72s/it] 52%|█████▏    | 97/186 [18:55<17:18, 11.67s/it] 53%|█████▎    | 98/186 [19:06<17:06, 11.66s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1233 > 1048). Running this sequence through the model will result in indexing errors
 53%|█████▎    | 99/186 [19:18<17:02, 11.76s/it] 54%|█████▍    | 100/186 [19:30<16:49, 11.73s/it] 54%|█████▍    | 101/186 [19:42<16:34, 11.71s/it] 55%|█████▍    | 102/186 [19:54<16:27, 11.76s/it] 55%|█████▌    | 103/186 [20:05<16:17, 11.77s/it] 56%|█████▌    | 104/186 [20:17<16:08, 11.81s/it] 56%|█████▋    | 105/186 [20:29<15:58, 11.83s/it] 57%|█████▋    | 106/186 [20:41<15:39, 11.75s/it] 58%|█████▊    | 107/186 [20:52<15:26, 11.73s/it] 58%|█████▊    | 108/186 [21:04<15:18, 11.77s/it] 59%|█████▊    | 109/186 [21:16<15:01, 11.70s/it] 59%|█████▉    | 110/186 [21:28<14:51, 11.73s/it] 60%|█████▉    | 111/186 [21:39<14:37, 11.70s/it] 60%|██████    | 112/186 [21:51<14:29, 11.75s/it] 61%|██████    | 113/186 [22:03<14:18, 11.76s/it] 61%|██████▏   | 114/186 [22:14<14:00, 11.68s/it] 62%|██████▏   | 115/186 [22:26<13:45, 11.63s/it] 62%|██████▏   | 116/186 [22:38<13:47, 11.82s/it] 63%|██████▎   | 117/186 [22:50<13:31, 11.76s/it] 63%|██████▎   | 118/186 [23:01<13:16, 11.71s/it] 64%|██████▍   | 119/186 [23:13<13:02, 11.67s/it] 65%|██████▍   | 120/186 [23:24<12:46, 11.61s/it] 65%|██████▌   | 121/186 [23:36<12:35, 11.62s/it] 66%|██████▌   | 122/186 [23:48<12:24, 11.63s/it] 66%|██████▌   | 123/186 [24:00<12:20, 11.75s/it] 67%|██████▋   | 124/186 [24:11<12:04, 11.69s/it] 67%|██████▋   | 125/186 [24:23<11:51, 11.66s/it] 68%|██████▊   | 126/186 [24:35<11:39, 11.66s/it] 68%|██████▊   | 127/186 [24:46<11:29, 11.69s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 69%|██████▉   | 128/186 [24:58<11:22, 11.76s/it] 69%|██████▉   | 129/186 [25:10<11:07, 11.72s/it] 70%|██████▉   | 130/186 [25:22<10:56, 11.72s/it] 70%|███████   | 131/186 [25:34<10:48, 11.79s/it] 71%|███████   | 132/186 [25:46<10:42, 11.90s/it] 72%|███████▏  | 133/186 [25:58<10:38, 12.05s/it] 72%|███████▏  | 134/186 [26:10<10:24, 12.00s/it] 73%|███████▎  | 135/186 [26:22<10:06, 11.89s/it] 73%|███████▎  | 136/186 [26:33<09:53, 11.87s/it] 74%|███████▎  | 137/186 [26:45<09:42, 11.90s/it] 74%|███████▍  | 138/186 [26:57<09:28, 11.84s/it] 75%|███████▍  | 139/186 [27:09<09:21, 11.95s/it] 75%|███████▌  | 140/186 [27:21<09:09, 11.95s/it] 76%|███████▌  | 141/186 [27:33<08:53, 11.85s/it] 76%|███████▋  | 142/186 [27:44<08:38, 11.79s/it] 77%|███████▋  | 143/186 [27:56<08:29, 11.84s/it] 77%|███████▋  | 144/186 [28:08<08:16, 11.81s/it] 78%|███████▊  | 145/186 [28:20<08:05, 11.85s/it] 78%|███████▊  | 146/186 [28:32<07:52, 11.81s/it] 79%|███████▉  | 147/186 [28:43<07:36, 11.71s/it] 80%|███████▉  | 148/186 [28:55<07:25, 11.71s/it] 80%|████████  | 149/186 [29:07<07:14, 11.73s/it] 81%|████████  | 150/186 [29:19<07:04, 11.79s/it] 81%|████████  | 151/186 [29:31<06:56, 11.90s/it] 82%|████████▏ | 152/186 [29:42<06:41, 11.80s/it] 82%|████████▏ | 153/186 [29:55<06:31, 11.87s/it] 83%|████████▎ | 154/186 [30:06<06:18, 11.82s/it] 83%|████████▎ | 155/186 [30:18<06:04, 11.74s/it] 84%|████████▍ | 156/186 [30:30<05:54, 11.82s/it] 84%|████████▍ | 157/186 [30:42<05:43, 11.86s/it] 85%|████████▍ | 158/186 [30:53<05:29, 11.78s/it] 85%|████████▌ | 159/186 [31:05<05:18, 11.80s/it] 86%|████████▌ | 160/186 [31:17<05:04, 11.72s/it] 87%|████████▋ | 161/186 [31:28<04:51, 11.65s/it] 87%|████████▋ | 162/186 [31:40<04:44, 11.84s/it] 88%|████████▊ | 163/186 [31:52<04:29, 11.74s/it] 88%|████████▊ | 164/186 [32:04<04:17, 11.70s/it] 89%|████████▊ | 165/186 [32:15<04:04, 11.64s/it] 89%|████████▉ | 166/186 [32:27<03:53, 11.68s/it] 90%|████████▉ | 167/186 [32:39<03:42, 11.72s/it] 90%|█████████ | 168/186 [32:50<03:30, 11.69s/it] 91%|█████████ | 169/186 [33:02<03:17, 11.60s/it] 91%|█████████▏| 170/186 [33:13<03:05, 11.58s/it] 92%|█████████▏| 171/186 [33:25<02:54, 11.65s/it] 92%|█████████▏| 172/186 [33:37<02:42, 11.64s/it] 93%|█████████▎| 173/186 [33:48<02:30, 11.59s/it] 94%|█████████▎| 174/186 [34:00<02:19, 11.59s/it] 94%|█████████▍| 175/186 [34:11<02:07, 11.58s/it] 95%|█████████▍| 176/186 [34:23<01:55, 11.51s/it] 95%|█████████▌| 177/186 [34:34<01:43, 11.54s/it] 96%|█████████▌| 178/186 [34:46<01:31, 11.48s/it] 96%|█████████▌| 179/186 [34:57<01:20, 11.55s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 97%|█████████▋| 180/186 [35:09<01:09, 11.56s/it] 97%|█████████▋| 181/186 [35:21<00:57, 11.59s/it] 98%|█████████▊| 182/186 [35:32<00:46, 11.58s/it] 98%|█████████▊| 183/186 [35:44<00:34, 11.57s/it] 99%|█████████▉| 184/186 [35:56<00:23, 11.69s/it] 99%|█████████▉| 185/186 [36:07<00:11, 11.74s/it]100%|██████████| 186/186 [36:19<00:00, 11.69s/it]                                                 {'train_runtime': 2179.5678, 'train_samples_per_second': 6.164, 'train_steps_per_second': 0.085, 'train_loss': 0.7933877924437164, 'epoch': 1.0}
100%|██████████| 186/186 [36:20<00:00, 11.69s/it]100%|██████████| 186/186 [36:20<00:00, 11.72s/it]
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-2
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33mdirect-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-2[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251015_060734-e65k2txz/logs[0m
[2025-10-15 06:45:33,877] [INFO] [launch.py:347:main] Process 497410 exits successfully.
[2025-10-15 06:45:33,877] [INFO] [launch.py:347:main] Process 497412 exits successfully.
[2025-10-15 06:45:33,877] [INFO] [launch.py:347:main] Process 497409 exits successfully.
[2025-10-15 06:45:33,877] [INFO] [launch.py:347:main] Process 497413 exits successfully.
[2025-10-15 06:45:33,878] [INFO] [launch.py:347:main] Process 497411 exits successfully.
[2025-10-15 06:45:37,882] [INFO] [launch.py:347:main] Process 497408 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-15 06:45:41,105] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 06:45:41,662] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-15 06:45:41,662] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12354 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-3 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-3 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 0.5 --use_anchor False --yilin_anchor True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-15 06:45:43,341] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 06:45:43,918] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-15 06:45:43,918] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-15 06:45:43,918] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-15 06:45:43,918] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-15 06:45:43,918] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-15 06:45:47,969] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 06:45:48,113] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-15 06:45:48,276] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-15 06:45:48,389] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 06:45:48,423] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-15 06:45:48,477] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-15 06:45:48,583] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 06:45:48,693] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 06:45:48,794] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 06:45:48,990] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-15 06:45:48,990] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-10-15 06:45:49,028] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 06:45:49,362] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251015_064549-buzj1t1w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-3
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/buzj1t1w
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.67s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.73s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.69s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.55s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.87s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.59s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.91s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.06s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.46s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.47s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.62s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.58s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.91s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.84s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.90s/it]
to bfloat16...
to bfloat16...
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.93s/it]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.17s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.43s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.00s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.21s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.16s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.86s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.22s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.88s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.87s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.87s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.63s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.69s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.01s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.74s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.06s/it]
Formatting inputs...Skip in lazy mode
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.010495901107788086 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.010092496871948242 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.1014244556427002 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.013221979141235352 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10098624229431152 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10135245323181152 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/186 [00:09<29:52,  9.69s/it]  1%|          | 2/186 [00:20<31:13, 10.18s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1160 > 1048). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1233 > 1048). Running this sequence through the model will result in indexing errors
  2%|▏         | 3/186 [00:31<32:24, 10.62s/it]  2%|▏         | 4/186 [00:41<32:05, 10.58s/it]  3%|▎         | 5/186 [00:52<31:40, 10.50s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1075 > 1048). Running this sequence through the model will result in indexing errors
  3%|▎         | 6/186 [01:02<31:44, 10.58s/it]  4%|▍         | 7/186 [01:13<31:37, 10.60s/it]  4%|▍         | 8/186 [01:24<31:37, 10.66s/it]  5%|▍         | 9/186 [01:35<31:31, 10.69s/it]  5%|▌         | 10/186 [01:45<31:23, 10.70s/it]  6%|▌         | 11/186 [01:56<31:27, 10.78s/it]  6%|▋         | 12/186 [02:07<31:03, 10.71s/it]  7%|▋         | 13/186 [02:17<30:45, 10.67s/it]  8%|▊         | 14/186 [02:28<30:54, 10.78s/it]  8%|▊         | 15/186 [02:40<31:25, 11.03s/it]  9%|▊         | 16/186 [02:52<31:56, 11.27s/it]  9%|▉         | 17/186 [03:04<32:06, 11.40s/it] 10%|▉         | 18/186 [03:15<32:11, 11.50s/it] 10%|█         | 19/186 [03:27<32:09, 11.55s/it] 11%|█         | 20/186 [03:39<31:57, 11.55s/it] 11%|█▏        | 21/186 [03:50<31:56, 11.61s/it] 12%|█▏        | 22/186 [04:02<31:58, 11.70s/it] 12%|█▏        | 23/186 [04:14<31:50, 11.72s/it] 13%|█▎        | 24/186 [04:26<31:44, 11.76s/it] 13%|█▎        | 25/186 [04:38<31:32, 11.76s/it] 14%|█▍        | 26/186 [04:49<31:13, 11.71s/it] 15%|█▍        | 27/186 [05:01<31:05, 11.73s/it] 15%|█▌        | 28/186 [05:13<30:53, 11.73s/it] 16%|█▌        | 29/186 [05:24<30:31, 11.66s/it] 16%|█▌        | 30/186 [05:36<30:11, 11.61s/it] 17%|█▋        | 31/186 [05:47<29:54, 11.58s/it] 17%|█▋        | 32/186 [05:59<29:38, 11.55s/it] 18%|█▊        | 33/186 [06:10<29:29, 11.57s/it]WARNING: tokenization mismatch: 1 vs. 48. (ignored)
 18%|█▊        | 34/186 [06:22<29:23, 11.60s/it] 19%|█▉        | 35/186 [06:34<29:27, 11.70s/it] 19%|█▉        | 36/186 [06:46<29:09, 11.66s/it] 20%|█▉        | 37/186 [06:57<28:59, 11.67s/it] 20%|██        | 38/186 [07:09<28:48, 11.68s/it] 21%|██        | 39/186 [07:21<28:42, 11.72s/it] 22%|██▏       | 40/186 [07:33<28:44, 11.81s/it] 22%|██▏       | 41/186 [07:45<28:49, 11.93s/it] 23%|██▎       | 42/186 [07:56<28:20, 11.81s/it] 23%|██▎       | 43/186 [08:07<27:33, 11.56s/it] 24%|██▎       | 44/186 [08:18<26:37, 11.25s/it] 24%|██▍       | 45/186 [08:29<25:56, 11.04s/it] 25%|██▍       | 46/186 [08:40<25:47, 11.05s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 25%|██▌       | 47/186 [08:51<25:42, 11.09s/it] 26%|██▌       | 48/186 [09:01<25:05, 10.91s/it] 26%|██▋       | 49/186 [09:12<24:40, 10.81s/it] 27%|██▋       | 50/186 [09:23<24:59, 11.03s/it] 27%|██▋       | 51/186 [09:39<27:52, 12.39s/it] 28%|██▊       | 52/186 [09:55<29:48, 13.34s/it] 28%|██▊       | 53/186 [10:10<31:07, 14.04s/it] 29%|██▉       | 54/186 [10:26<32:20, 14.70s/it] 30%|██▉       | 55/186 [10:42<32:48, 15.03s/it] 30%|███       | 56/186 [10:59<33:29, 15.46s/it] 31%|███       | 57/186 [11:15<33:36, 15.63s/it] 31%|███       | 58/186 [11:31<33:31, 15.71s/it] 32%|███▏      | 59/186 [11:43<31:25, 14.85s/it] 32%|███▏      | 60/186 [11:54<28:11, 13.43s/it] 33%|███▎      | 61/186 [12:04<26:08, 12.55s/it] 33%|███▎      | 62/186 [12:15<24:50, 12.02s/it] 34%|███▍      | 63/186 [12:26<24:19, 11.86s/it] 34%|███▍      | 64/186 [12:38<24:02, 11.82s/it] 35%|███▍      | 65/186 [12:50<23:42, 11.76s/it] 35%|███▌      | 66/186 [13:02<23:37, 11.81s/it] 36%|███▌      | 67/186 [13:14<23:34, 11.88s/it] 37%|███▋      | 68/186 [13:25<23:14, 11.82s/it] 37%|███▋      | 69/186 [13:37<22:47, 11.69s/it] 38%|███▊      | 70/186 [13:47<21:51, 11.31s/it] 38%|███▊      | 71/186 [13:58<21:33, 11.24s/it] 39%|███▊      | 72/186 [14:10<21:27, 11.29s/it] 39%|███▉      | 73/186 [14:20<20:51, 11.08s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 40%|███▉      | 74/186 [14:31<20:32, 11.01s/it] 40%|████      | 75/186 [14:42<20:05, 10.86s/it] 41%|████      | 76/186 [14:52<19:39, 10.72s/it] 41%|████▏     | 77/186 [15:03<19:32, 10.75s/it] 42%|████▏     | 78/186 [15:14<19:42, 10.95s/it] 42%|████▏     | 79/186 [15:25<19:15, 10.80s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1240 > 1048). Running this sequence through the model will result in indexing errors
 43%|████▎     | 80/186 [15:36<19:23, 10.98s/it] 44%|████▎     | 81/186 [15:48<19:33, 11.17s/it]WARNING: tokenization mismatch: 1 vs. 48. (ignored)
 44%|████▍     | 82/186 [16:03<21:24, 12.35s/it] 45%|████▍     | 83/186 [16:18<22:50, 13.30s/it] 45%|████▌     | 84/186 [16:34<23:47, 14.00s/it] 46%|████▌     | 85/186 [16:50<24:48, 14.74s/it] 46%|████▌     | 86/186 [17:06<25:11, 15.12s/it]WARNING: tokenization mismatch: 1 vs. 47. (ignored)
 47%|████▋     | 87/186 [17:22<25:09, 15.25s/it] 47%|████▋     | 88/186 [17:38<25:10, 15.42s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
Token indices sequence length is longer than the specified maximum sequence length for this model (1074 > 1048). Running this sequence through the model will result in indexing errors
 48%|████▊     | 89/186 [17:53<25:02, 15.49s/it] 48%|████▊     | 90/186 [18:10<25:14, 15.78s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 49%|████▉     | 91/186 [18:26<25:07, 15.87s/it] 49%|████▉     | 92/186 [18:42<24:59, 15.95s/it] 50%|█████     | 93/186 [18:58<24:52, 16.05s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 51%|█████     | 94/186 [19:14<24:29, 15.97s/it] 51%|█████     | 95/186 [19:30<24:13, 15.97s/it] 52%|█████▏    | 96/186 [19:46<23:52, 15.91s/it] 52%|█████▏    | 97/186 [20:02<23:29, 15.83s/it] 53%|█████▎    | 98/186 [20:18<23:16, 15.87s/it] 53%|█████▎    | 99/186 [20:34<23:12, 16.01s/it] 54%|█████▍    | 100/186 [20:50<22:55, 15.99s/it] 54%|█████▍    | 101/186 [21:06<22:38, 15.99s/it] 55%|█████▍    | 102/186 [21:22<22:25, 16.02s/it] 55%|█████▌    | 103/186 [21:39<22:29, 16.26s/it] 56%|█████▌    | 104/186 [21:55<22:06, 16.18s/it] 56%|█████▋    | 105/186 [22:11<21:49, 16.16s/it] 57%|█████▋    | 106/186 [22:27<21:26, 16.08s/it] 58%|█████▊    | 107/186 [22:42<21:00, 15.95s/it] 58%|█████▊    | 108/186 [22:58<20:47, 15.99s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 59%|█████▊    | 109/186 [23:14<20:31, 15.99s/it] 59%|█████▉    | 110/186 [23:30<20:13, 15.97s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 60%|█████▉    | 111/186 [23:46<19:53, 15.91s/it] 60%|██████    | 112/186 [24:01<19:12, 15.57s/it] 61%|██████    | 113/186 [24:16<18:42, 15.38s/it] 61%|██████▏   | 114/186 [24:31<18:16, 15.23s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 62%|██████▏   | 115/186 [24:45<17:51, 15.09s/it] 62%|██████▏   | 116/186 [25:01<17:41, 15.16s/it] 63%|██████▎   | 117/186 [25:17<17:42, 15.40s/it] 63%|██████▎   | 118/186 [25:27<15:52, 14.00s/it] 64%|██████▍   | 119/186 [25:38<14:27, 12.95s/it] 65%|██████▍   | 120/186 [25:49<13:27, 12.23s/it] 65%|██████▌   | 121/186 [25:59<12:37, 11.65s/it] 66%|██████▌   | 122/186 [26:09<11:59, 11.24s/it] 66%|██████▌   | 123/186 [26:21<11:54, 11.35s/it] 67%|██████▋   | 124/186 [26:31<11:25, 11.06s/it] 67%|██████▋   | 125/186 [26:42<11:16, 11.09s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 68%|██████▊   | 126/186 [26:54<11:11, 11.20s/it] 68%|██████▊   | 127/186 [27:05<10:57, 11.15s/it] 69%|██████▉   | 128/186 [27:15<10:38, 11.00s/it] 69%|██████▉   | 129/186 [27:27<10:34, 11.13s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 70%|██████▉   | 130/186 [27:38<10:31, 11.28s/it] 70%|███████   | 131/186 [27:49<10:11, 11.11s/it] 71%|███████   | 132/186 [28:00<09:54, 11.01s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1263 > 1048). Running this sequence through the model will result in indexing errors
 72%|███████▏  | 133/186 [28:10<09:34, 10.84s/it] 72%|███████▏  | 134/186 [28:25<10:23, 11.99s/it] 73%|███████▎  | 135/186 [28:41<11:07, 13.09s/it] 73%|███████▎  | 136/186 [28:56<11:32, 13.85s/it] 74%|███████▎  | 137/186 [29:12<11:49, 14.48s/it] 74%|███████▍  | 138/186 [29:28<11:52, 14.85s/it] 75%|███████▍  | 139/186 [29:44<11:56, 15.24s/it] 75%|███████▌  | 140/186 [30:00<11:50, 15.44s/it] 76%|███████▌  | 141/186 [30:16<11:38, 15.53s/it] 76%|███████▋  | 142/186 [30:31<11:23, 15.54s/it] 77%|███████▋  | 143/186 [30:47<11:14, 15.68s/it] 77%|███████▋  | 144/186 [31:03<11:00, 15.73s/it] 78%|███████▊  | 145/186 [31:19<10:47, 15.78s/it] 78%|███████▊  | 146/186 [31:35<10:32, 15.80s/it] 79%|███████▉  | 147/186 [31:51<10:17, 15.83s/it] 80%|███████▉  | 148/186 [32:06<09:59, 15.78s/it] 80%|████████  | 149/186 [32:22<09:45, 15.83s/it] 81%|████████  | 150/186 [32:38<09:28, 15.78s/it] 81%|████████  | 151/186 [32:54<09:12, 15.80s/it] 82%|████████▏ | 152/186 [33:10<08:58, 15.83s/it] 82%|████████▏ | 153/186 [33:26<08:49, 16.05s/it] 83%|████████▎ | 154/186 [33:42<08:30, 15.94s/it] 83%|████████▎ | 155/186 [33:58<08:16, 16.01s/it] 84%|████████▍ | 156/186 [34:14<07:58, 15.94s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 84%|████████▍ | 157/186 [34:30<07:40, 15.89s/it] 85%|████████▍ | 158/186 [34:46<07:26, 15.94s/it] 85%|████████▌ | 159/186 [35:02<07:08, 15.87s/it] 86%|████████▌ | 160/186 [35:18<06:55, 15.98s/it] 87%|████████▋ | 161/186 [35:34<06:39, 16.00s/it] 87%|████████▋ | 162/186 [35:50<06:24, 16.03s/it] 88%|████████▊ | 163/186 [36:06<06:10, 16.13s/it] 88%|████████▊ | 164/186 [36:21<05:48, 15.83s/it] 89%|████████▊ | 165/186 [36:36<05:25, 15.51s/it] 89%|████████▉ | 166/186 [36:51<05:07, 15.35s/it] 90%|████████▉ | 167/186 [37:06<04:46, 15.10s/it] 90%|█████████ | 168/186 [37:20<04:29, 14.99s/it] 91%|█████████ | 169/186 [37:35<04:15, 15.01s/it] 91%|█████████▏| 170/186 [37:49<03:51, 14.49s/it] 92%|█████████▏| 171/186 [37:59<03:20, 13.34s/it] 92%|█████████▏| 172/186 [38:10<02:55, 12.52s/it] 93%|█████████▎| 173/186 [38:20<02:34, 11.90s/it] 94%|█████████▎| 174/186 [38:31<02:18, 11.53s/it] 94%|█████████▍| 175/186 [38:42<02:04, 11.29s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 95%|█████████▍| 176/186 [38:53<01:53, 11.32s/it] 95%|█████████▌| 177/186 [39:04<01:40, 11.19s/it] 96%|█████████▌| 178/186 [39:15<01:29, 11.23s/it] 96%|█████████▌| 179/186 [39:26<01:18, 11.16s/it] 97%|█████████▋| 180/186 [39:38<01:06, 11.16s/it] 97%|█████████▋| 181/186 [39:48<00:54, 10.96s/it] 98%|█████████▊| 182/186 [39:59<00:43, 10.99s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 98%|█████████▊| 183/186 [40:11<00:33, 11.22s/it] 99%|█████████▉| 184/186 [40:22<00:22, 11.03s/it] 99%|█████████▉| 185/186 [40:32<00:10, 10.95s/it]WARNING: tokenization mismatch: 1 vs. 48. (ignored)
100%|██████████| 186/186 [40:44<00:00, 11.28s/it]                                                 {'train_runtime': 2444.8443, 'train_samples_per_second': 5.495, 'train_steps_per_second': 0.076, 'train_loss': 0.7921677456107191, 'epoch': 1.0}
100%|██████████| 186/186 [40:45<00:00, 11.28s/it]100%|██████████| 186/186 [40:45<00:00, 13.15s/it]
[2025-10-15 07:28:18,920] [INFO] [launch.py:347:main] Process 499246 exits successfully.
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-3
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33mdirect-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-3[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251015_064549-buzj1t1w/logs[0m
[2025-10-15 07:28:22,925] [INFO] [launch.py:347:main] Process 499248 exits successfully.
[2025-10-15 07:28:22,925] [INFO] [launch.py:347:main] Process 499245 exits successfully.
[2025-10-15 07:28:22,925] [INFO] [launch.py:347:main] Process 499249 exits successfully.
[2025-10-15 07:28:22,925] [INFO] [launch.py:347:main] Process 499247 exits successfully.
[2025-10-15 07:28:26,930] [INFO] [launch.py:347:main] Process 499244 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-15 07:28:30,209] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 07:28:30,771] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-15 07:28:30,771] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12354 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.7-3 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.7-3 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 0.7 --use_anchor False --yilin_anchor True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-15 07:28:32,533] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 07:28:33,135] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-15 07:28:33,135] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-15 07:28:33,135] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-15 07:28:33,135] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-15 07:28:33,135] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-15 07:28:36,964] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-15 07:28:37,182] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 07:28:37,315] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-15 07:28:37,335] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-15 07:28:37,417] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-15 07:28:37,495] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-15 07:28:37,495] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 07:28:37,658] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 07:28:37,658] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-15 07:28:37,678] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-15 07:28:37,831] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 07:28:37,984] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 07:28:38,020] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: creating run
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251015_072838-ykkljafp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.7-3
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/ykkljafp
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.54s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.50s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.44s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.36s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.35s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.01s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.22s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.23s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.86s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.23s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.66s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.02s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.87s/it]
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.26s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.48s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.82s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.06s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.42s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.39s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.07s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.02s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.96s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.28s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.25s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.89s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.21s/it]
Formatting inputs...Skip in lazy mode
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.012831687927246094 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.011818885803222656 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10105443000793457 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10102295875549316 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10165286064147949 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.01044607162475586 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/186 [00:15<47:27, 15.39s/it]  1%|          | 2/186 [00:30<47:32, 15.50s/it]  2%|▏         | 3/186 [00:46<47:30, 15.57s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
  2%|▏         | 4/186 [01:02<47:45, 15.75s/it]  3%|▎         | 5/186 [01:18<47:38, 15.79s/it]  3%|▎         | 6/186 [01:34<47:31, 15.84s/it]  4%|▍         | 7/186 [01:50<47:20, 15.87s/it]  4%|▍         | 8/186 [02:05<46:47, 15.77s/it]  5%|▍         | 9/186 [02:21<46:42, 15.83s/it]  5%|▌         | 10/186 [02:38<46:41, 15.92s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1073 > 1048). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1069 > 1048). Running this sequence through the model will result in indexing errors
  6%|▌         | 11/186 [02:55<47:50, 16.40s/it]  6%|▋         | 12/186 [03:11<47:10, 16.27s/it]  7%|▋         | 13/186 [03:27<47:03, 16.32s/it]  8%|▊         | 14/186 [03:43<46:31, 16.23s/it]  8%|▊         | 15/186 [04:00<46:15, 16.23s/it]  9%|▊         | 16/186 [04:16<45:43, 16.14s/it]  9%|▉         | 17/186 [04:31<44:58, 15.96s/it] 10%|▉         | 18/186 [04:46<43:41, 15.60s/it] 10%|█         | 19/186 [05:01<42:59, 15.45s/it] 11%|█         | 20/186 [05:16<42:16, 15.28s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1065 > 1048). Running this sequence through the model will result in indexing errors
 11%|█▏        | 21/186 [05:31<41:54, 15.24s/it] 12%|█▏        | 22/186 [05:46<41:26, 15.16s/it] 12%|█▏        | 23/186 [06:02<41:41, 15.35s/it] 13%|█▎        | 24/186 [06:13<37:45, 13.99s/it] 13%|█▎        | 25/186 [06:23<34:46, 12.96s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1160 > 1048). Running this sequence through the model will result in indexing errors
 14%|█▍        | 26/186 [06:34<32:48, 12.30s/it] 15%|█▍        | 27/186 [06:45<31:22, 11.84s/it] 15%|█▌        | 28/186 [06:55<30:09, 11.45s/it] 16%|█▌        | 29/186 [07:07<30:14, 11.56s/it] 16%|█▌        | 30/186 [07:18<29:11, 11.23s/it] 17%|█▋        | 31/186 [07:28<28:42, 11.11s/it] 17%|█▋        | 32/186 [07:39<28:32, 11.12s/it] 18%|█▊        | 33/186 [07:50<28:14, 11.08s/it] 18%|█▊        | 34/186 [08:01<27:29, 10.85s/it] 19%|█▉        | 35/186 [08:13<28:10, 11.20s/it] 19%|█▉        | 36/186 [08:24<28:17, 11.32s/it] 20%|█▉        | 37/186 [08:35<27:27, 11.06s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
 20%|██        | 38/186 [08:45<26:49, 10.88s/it] 21%|██        | 39/186 [08:56<26:27, 10.80s/it] 22%|██▏       | 40/186 [09:09<28:14, 11.61s/it] 22%|██▏       | 41/186 [09:25<31:05, 12.86s/it] 23%|██▎       | 42/186 [09:42<33:20, 13.89s/it] 23%|██▎       | 43/186 [09:57<34:34, 14.51s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 24%|██▎       | 44/186 [10:13<35:21, 14.94s/it] 24%|██▍       | 45/186 [10:29<35:53, 15.27s/it] 25%|██▍       | 46/186 [10:45<36:07, 15.49s/it] 25%|██▌       | 47/186 [11:01<36:14, 15.65s/it] 26%|██▌       | 48/186 [11:19<37:02, 16.10s/it] 26%|██▋       | 49/186 [11:34<36:26, 15.96s/it] 27%|██▋       | 50/186 [11:50<36:01, 15.89s/it] 27%|██▋       | 51/186 [12:06<35:49, 15.92s/it] 28%|██▊       | 52/186 [12:23<36:04, 16.15s/it] 28%|██▊       | 53/186 [12:39<35:51, 16.18s/it] 29%|██▉       | 54/186 [12:55<35:37, 16.19s/it] 30%|██▉       | 55/186 [13:11<35:13, 16.13s/it] 30%|███       | 56/186 [13:27<35:02, 16.17s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1075 > 1048). Running this sequence through the model will result in indexing errors
 31%|███       | 57/186 [13:44<34:59, 16.27s/it] 31%|███       | 58/186 [14:00<34:50, 16.34s/it] 32%|███▏      | 59/186 [14:16<34:19, 16.22s/it] 32%|███▏      | 60/186 [14:33<34:11, 16.28s/it] 33%|███▎      | 61/186 [14:49<34:10, 16.40s/it] 33%|███▎      | 62/186 [15:06<34:00, 16.45s/it] 34%|███▍      | 63/186 [15:23<33:54, 16.54s/it] 34%|███▍      | 64/186 [15:39<33:29, 16.47s/it] 35%|███▍      | 65/186 [15:55<33:09, 16.44s/it] 35%|███▌      | 66/186 [16:11<32:36, 16.31s/it] 36%|███▌      | 67/186 [16:27<32:08, 16.21s/it] 37%|███▋      | 68/186 [16:43<31:46, 16.16s/it] 37%|███▋      | 69/186 [16:59<31:08, 15.97s/it] 38%|███▊      | 70/186 [17:14<30:16, 15.66s/it] 38%|███▊      | 71/186 [17:29<29:48, 15.55s/it] 39%|███▊      | 72/186 [17:44<29:08, 15.33s/it] 39%|███▉      | 73/186 [18:00<29:23, 15.61s/it] 40%|███▉      | 74/186 [18:16<29:00, 15.54s/it] 40%|████      | 75/186 [18:31<28:44, 15.53s/it] 41%|████      | 76/186 [18:42<25:46, 14.06s/it] 41%|████▏     | 77/186 [18:53<23:43, 13.06s/it] 42%|████▏     | 78/186 [19:03<22:14, 12.36s/it] 42%|████▏     | 79/186 [19:14<21:04, 11.82s/it] 43%|████▎     | 80/186 [19:25<20:27, 11.58s/it] 44%|████▎     | 81/186 [19:36<20:16, 11.59s/it] 44%|████▍     | 82/186 [19:47<19:38, 11.33s/it] 45%|████▍     | 83/186 [19:59<19:28, 11.35s/it] 45%|████▌     | 84/186 [20:09<18:56, 11.14s/it] 46%|████▌     | 85/186 [20:20<18:27, 10.96s/it] 46%|████▌     | 86/186 [20:30<18:01, 10.81s/it] 47%|████▋     | 87/186 [20:42<18:06, 10.98s/it] 47%|████▋     | 88/186 [20:53<18:08, 11.11s/it] 48%|████▊     | 89/186 [21:03<17:35, 10.88s/it] 48%|████▊     | 90/186 [21:14<17:19, 10.82s/it] 49%|████▉     | 91/186 [21:26<17:44, 11.20s/it] 49%|████▉     | 92/186 [21:42<19:45, 12.61s/it] 50%|█████     | 93/186 [21:58<21:07, 13.63s/it] 51%|█████     | 94/186 [22:14<21:49, 14.23s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 51%|█████     | 95/186 [22:29<22:16, 14.69s/it] 52%|█████▏    | 96/186 [22:45<22:28, 14.98s/it]WARNING: tokenization mismatch: 1 vs. 48. (ignored)
 52%|█████▏    | 97/186 [23:01<22:36, 15.24s/it] 53%|█████▎    | 98/186 [23:17<22:48, 15.55s/it] 53%|█████▎    | 99/186 [23:33<22:44, 15.69s/it] 54%|█████▍    | 100/186 [23:49<22:36, 15.78s/it] 54%|█████▍    | 101/186 [24:06<22:39, 15.99s/it] 55%|█████▍    | 102/186 [24:22<22:20, 15.96s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 55%|█████▌    | 103/186 [24:38<22:13, 16.07s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 56%|█████▌    | 104/186 [24:54<21:52, 16.00s/it] 56%|█████▋    | 105/186 [25:10<21:38, 16.04s/it] 57%|█████▋    | 106/186 [25:26<21:33, 16.17s/it] 58%|█████▊    | 107/186 [25:42<21:11, 16.10s/it] 58%|█████▊    | 108/186 [25:58<20:56, 16.11s/it] 59%|█████▊    | 109/186 [26:15<20:42, 16.14s/it] 59%|█████▉    | 110/186 [26:31<20:21, 16.08s/it] 60%|█████▉    | 111/186 [26:46<19:52, 15.91s/it] 60%|██████    | 112/186 [27:02<19:46, 16.03s/it] 61%|██████    | 113/186 [27:18<19:28, 16.01s/it] 61%|██████▏   | 114/186 [27:35<19:19, 16.11s/it] 62%|██████▏   | 115/186 [27:51<18:59, 16.05s/it] 62%|██████▏   | 116/186 [28:07<18:50, 16.15s/it] 63%|██████▎   | 117/186 [28:23<18:30, 16.09s/it] 63%|██████▎   | 118/186 [28:39<18:12, 16.07s/it] 64%|██████▍   | 119/186 [28:55<18:01, 16.15s/it] 65%|██████▍   | 120/186 [29:12<17:51, 16.24s/it] 65%|██████▌   | 121/186 [29:26<17:04, 15.77s/it] 66%|██████▌   | 122/186 [29:41<16:29, 15.47s/it] 66%|██████▌   | 123/186 [29:56<15:59, 15.23s/it] 67%|██████▋   | 124/186 [30:10<15:30, 15.01s/it] 67%|██████▋   | 125/186 [30:25<15:15, 15.00s/it] 68%|██████▊   | 126/186 [30:41<15:06, 15.10s/it] 68%|██████▊   | 127/186 [30:55<14:40, 14.92s/it] 69%|██████▉   | 128/186 [31:06<13:06, 13.57s/it] 69%|██████▉   | 129/186 [31:16<11:57, 12.59s/it] 70%|██████▉   | 130/186 [31:26<11:06, 11.91s/it] 70%|███████   | 131/186 [31:36<10:28, 11.43s/it] 71%|███████   | 132/186 [31:47<10:02, 11.15s/it] 72%|███████▏  | 133/186 [32:00<10:17, 11.66s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
 72%|███████▏  | 134/186 [32:10<09:47, 11.30s/it] 73%|███████▎  | 135/186 [32:21<09:34, 11.26s/it] 73%|███████▎  | 136/186 [32:33<09:33, 11.48s/it] 74%|███████▎  | 137/186 [32:45<09:20, 11.44s/it] 74%|███████▍  | 138/186 [32:56<08:59, 11.25s/it] 75%|███████▍  | 139/186 [33:07<08:46, 11.21s/it] 75%|███████▌  | 140/186 [33:18<08:35, 11.22s/it] 76%|███████▌  | 141/186 [33:28<08:14, 10.99s/it] 76%|███████▋  | 142/186 [33:39<07:57, 10.85s/it] 77%|███████▋  | 143/186 [33:51<07:58, 11.12s/it] 77%|███████▋  | 144/186 [34:06<08:38, 12.34s/it] 78%|███████▊  | 145/186 [34:22<09:07, 13.35s/it] 78%|███████▊  | 146/186 [34:37<09:22, 14.07s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 79%|███████▉  | 147/186 [34:53<09:28, 14.59s/it] 80%|███████▉  | 148/186 [35:09<09:26, 14.91s/it] 80%|████████  | 149/186 [35:25<09:27, 15.34s/it] 81%|████████  | 150/186 [35:41<09:17, 15.49s/it] 81%|████████  | 151/186 [35:56<09:00, 15.45s/it] 82%|████████▏ | 152/186 [36:12<08:44, 15.44s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 82%|████████▏ | 153/186 [36:28<08:33, 15.55s/it] 83%|████████▎ | 154/186 [36:43<08:16, 15.52s/it] 83%|████████▎ | 155/186 [36:59<08:03, 15.59s/it] 84%|████████▍ | 156/186 [37:15<07:51, 15.72s/it] 84%|████████▍ | 157/186 [37:32<07:47, 16.12s/it] 85%|████████▍ | 158/186 [37:48<07:28, 16.03s/it] 85%|████████▌ | 159/186 [38:03<07:09, 15.91s/it] 86%|████████▌ | 160/186 [38:20<06:58, 16.08s/it] 87%|████████▋ | 161/186 [38:36<06:40, 16.01s/it] 87%|████████▋ | 162/186 [38:52<06:25, 16.08s/it] 88%|████████▊ | 163/186 [39:08<06:12, 16.19s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 88%|████████▊ | 164/186 [39:24<05:54, 16.12s/it] 89%|████████▊ | 165/186 [39:40<05:38, 16.10s/it] 89%|████████▉ | 166/186 [39:56<05:21, 16.08s/it] 90%|████████▉ | 167/186 [40:12<05:05, 16.07s/it] 90%|█████████ | 168/186 [40:28<04:48, 16.03s/it] 91%|█████████ | 169/186 [40:44<04:31, 15.96s/it] 91%|█████████▏| 170/186 [41:00<04:15, 15.95s/it] 92%|█████████▏| 171/186 [41:16<04:00, 16.02s/it] 92%|█████████▏| 172/186 [41:32<03:44, 16.05s/it] 93%|█████████▎| 173/186 [41:48<03:25, 15.81s/it] 94%|█████████▎| 174/186 [42:02<03:05, 15.47s/it] 94%|█████████▍| 175/186 [42:17<02:47, 15.21s/it] 95%|█████████▍| 176/186 [42:32<02:30, 15.10s/it] 95%|█████████▌| 177/186 [42:47<02:16, 15.18s/it] 96%|█████████▌| 178/186 [43:02<02:01, 15.17s/it] 96%|█████████▌| 179/186 [43:18<01:48, 15.44s/it] 97%|█████████▋| 180/186 [43:30<01:25, 14.22s/it] 97%|█████████▋| 181/186 [43:40<01:05, 13.09s/it] 98%|█████████▊| 182/186 [43:51<00:49, 12.35s/it] 98%|█████████▊| 183/186 [44:01<00:35, 11.76s/it] 99%|█████████▉| 184/186 [44:12<00:22, 11.40s/it] 99%|█████████▉| 185/186 [44:25<00:11, 11.92s/it]100%|██████████| 186/186 [44:35<00:00, 11.44s/it]                                                 {'train_runtime': 2675.6801, 'train_samples_per_second': 5.021, 'train_steps_per_second': 0.07, 'train_loss': 0.8330043669669859, 'epoch': 1.0}
100%|██████████| 186/186 [44:36<00:00, 11.44s/it]100%|██████████| 186/186 [44:36<00:00, 14.39s/it]
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.7-3
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33mdirect-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.7-3[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251015_072838-ykkljafp/logs[0m
[2025-10-15 08:15:16,321] [INFO] [launch.py:347:main] Process 512781 exits successfully.
[2025-10-15 08:15:18,323] [INFO] [launch.py:347:main] Process 512780 exits successfully.
[2025-10-15 08:15:18,324] [INFO] [launch.py:347:main] Process 512777 exits successfully.
[2025-10-15 08:15:18,324] [INFO] [launch.py:347:main] Process 512779 exits successfully.
[2025-10-15 08:15:18,324] [INFO] [launch.py:347:main] Process 512778 exits successfully.
[2025-10-15 08:15:20,326] [INFO] [launch.py:347:main] Process 512776 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-15 08:15:23,829] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 08:15:24,453] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-15 08:15:24,453] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12354 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.3-3 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.3-3 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 0.3 --use_anchor False --yilin_anchor True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-15 08:15:27,028] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 08:15:28,050] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-15 08:15:28,050] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-15 08:15:28,050] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-15 08:15:28,050] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-15 08:15:28,050] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-15 08:15:31,926] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 08:15:32,011] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-15 08:15:32,173] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 08:15:32,247] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-15 08:15:32,332] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-15 08:15:32,332] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-10-15 08:15:32,457] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 08:15:32,494] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 08:15:32,806] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
[2025-10-15 08:15:33,693] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 08:15:33,947] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 08:15:34,248] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 08:15:34,387] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251015_081534-k7zqm3w4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.3-3
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/k7zqm3w4
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.74s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.66s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.68s/it]
to bfloat16...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.40s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.30s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.38s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.32s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.25s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.21s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.29s/it]
to bfloat16...
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.61s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.94s/it]
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.23s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.01s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.04s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.99s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.91s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.23s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.81s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.18s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.58s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.66s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.99s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.18s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.29s/it]
Formatting inputs...Skip in lazy mode
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.013353347778320312 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.012677431106567383 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10120224952697754 seconds
Loading extension module fused_adam...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Time to load fused_adam op: 0.10109782218933105 seconds
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.01512765884399414 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.2012495994567871 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/186 [00:15<47:44, 15.48s/it]  1%|          | 2/186 [00:30<47:15, 15.41s/it]  2%|▏         | 3/186 [00:46<47:06, 15.45s/it]  2%|▏         | 4/186 [01:01<47:03, 15.52s/it]  3%|▎         | 5/186 [01:17<46:56, 15.56s/it]  3%|▎         | 6/186 [01:33<46:52, 15.63s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
  4%|▍         | 7/186 [01:49<47:08, 15.80s/it]  4%|▍         | 8/186 [02:05<46:47, 15.77s/it]  5%|▍         | 9/186 [02:21<46:42, 15.83s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1158 > 1048). Running this sequence through the model will result in indexing errors
  5%|▌         | 10/186 [02:37<47:13, 16.10s/it]  6%|▌         | 11/186 [02:53<46:57, 16.10s/it]  6%|▋         | 12/186 [03:10<46:43, 16.11s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1285 > 1048). Running this sequence through the model will result in indexing errors
  7%|▋         | 13/186 [03:26<46:42, 16.20s/it]  8%|▊         | 14/186 [03:42<46:03, 16.06s/it]  8%|▊         | 15/186 [03:58<45:37, 16.01s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1102 > 1048). Running this sequence through the model will result in indexing errors
  9%|▊         | 16/186 [04:14<45:34, 16.09s/it]  9%|▉         | 17/186 [04:30<45:24, 16.12s/it] 10%|▉         | 18/186 [04:46<44:57, 16.06s/it] 10%|█         | 19/186 [05:02<44:20, 15.93s/it] 11%|█         | 20/186 [05:17<43:46, 15.82s/it] 11%|█▏        | 21/186 [05:33<43:46, 15.92s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 12%|█▏        | 22/186 [05:50<44:02, 16.12s/it] 12%|█▏        | 23/186 [06:06<43:30, 16.02s/it] 13%|█▎        | 24/186 [06:22<43:40, 16.18s/it] 13%|█▎        | 25/186 [06:38<43:18, 16.14s/it] 14%|█▍        | 26/186 [06:54<42:44, 16.03s/it] 15%|█▍        | 27/186 [07:10<42:12, 15.93s/it] 15%|█▌        | 28/186 [07:24<40:54, 15.54s/it] 16%|█▌        | 29/186 [07:39<39:57, 15.27s/it] 16%|█▌        | 30/186 [07:54<39:18, 15.12s/it] 17%|█▋        | 31/186 [08:08<38:40, 14.97s/it] 17%|█▋        | 32/186 [08:24<38:30, 15.00s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 18%|█▊        | 33/186 [08:39<38:22, 15.05s/it] 18%|█▊        | 34/186 [08:49<34:38, 13.67s/it] 19%|█▉        | 35/186 [09:00<32:09, 12.78s/it] 19%|█▉        | 36/186 [09:10<30:08, 12.06s/it] 20%|█▉        | 37/186 [09:21<28:49, 11.61s/it] 20%|██        | 38/186 [09:31<27:43, 11.24s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 21%|██        | 39/186 [09:44<28:57, 11.82s/it] 22%|██▏       | 40/186 [09:55<27:39, 11.37s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1263 > 1048). Running this sequence through the model will result in indexing errors
 22%|██▏       | 41/186 [10:06<27:08, 11.23s/it] 23%|██▎       | 42/186 [10:18<27:36, 11.50s/it] 23%|██▎       | 43/186 [10:29<26:58, 11.32s/it] 24%|██▎       | 44/186 [10:39<26:18, 11.12s/it] 24%|██▍       | 45/186 [10:50<25:35, 10.89s/it] 25%|██▍       | 46/186 [11:01<25:29, 10.93s/it] 25%|██▌       | 47/186 [11:12<25:48, 11.14s/it] 26%|██▌       | 48/186 [11:23<25:00, 10.88s/it] 26%|██▋       | 49/186 [11:33<24:41, 10.81s/it] 27%|██▋       | 50/186 [11:46<25:46, 11.37s/it] 27%|██▋       | 51/186 [12:02<28:30, 12.67s/it] 28%|██▊       | 52/186 [12:17<30:21, 13.59s/it] 28%|██▊       | 53/186 [12:33<31:25, 14.17s/it] 29%|██▉       | 54/186 [12:49<32:15, 14.66s/it] 30%|██▉       | 55/186 [13:05<32:56, 15.09s/it] 30%|███       | 56/186 [13:21<33:16, 15.36s/it] 31%|███       | 57/186 [13:37<33:22, 15.52s/it] 31%|███       | 58/186 [13:53<33:32, 15.72s/it] 32%|███▏      | 59/186 [14:09<33:28, 15.81s/it] 32%|███▏      | 60/186 [14:25<33:10, 15.80s/it] 33%|███▎      | 61/186 [14:40<32:55, 15.80s/it] 33%|███▎      | 62/186 [14:56<32:41, 15.82s/it] 34%|███▍      | 63/186 [15:12<32:20, 15.78s/it] 34%|███▍      | 64/186 [15:28<32:26, 15.95s/it] 35%|███▍      | 65/186 [15:45<32:22, 16.05s/it] 35%|███▌      | 66/186 [16:01<32:05, 16.05s/it] 36%|███▌      | 67/186 [16:17<31:46, 16.02s/it] 37%|███▋      | 68/186 [16:32<31:22, 15.96s/it] 37%|███▋      | 69/186 [16:49<31:43, 16.27s/it] 38%|███▊      | 70/186 [17:06<31:29, 16.29s/it] 38%|███▊      | 71/186 [17:22<31:07, 16.24s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1240 > 1048). Running this sequence through the model will result in indexing errors
 39%|███▊      | 72/186 [17:38<30:46, 16.20s/it] 39%|███▉      | 73/186 [17:54<30:23, 16.14s/it] 40%|███▉      | 74/186 [18:10<30:06, 16.13s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 40%|████      | 75/186 [18:26<29:47, 16.10s/it] 41%|████      | 76/186 [18:42<29:26, 16.06s/it] 41%|████▏     | 77/186 [18:58<28:54, 15.91s/it] 42%|████▏     | 78/186 [19:14<28:45, 15.98s/it] 42%|████▏     | 79/186 [19:30<28:44, 16.12s/it] 43%|████▎     | 80/186 [19:45<27:51, 15.77s/it] 44%|████▎     | 81/186 [20:00<27:10, 15.53s/it] 44%|████▍     | 82/186 [20:15<26:30, 15.29s/it] 45%|████▍     | 83/186 [20:30<26:07, 15.22s/it] 45%|████▌     | 84/186 [20:45<25:48, 15.18s/it] 46%|████▌     | 85/186 [21:01<25:48, 15.34s/it] 46%|████▌     | 86/186 [21:14<24:30, 14.70s/it] 47%|████▋     | 87/186 [21:24<22:05, 13.39s/it] 47%|████▋     | 88/186 [21:35<20:21, 12.47s/it] 48%|████▊     | 89/186 [21:45<19:07, 11.83s/it] 48%|████▊     | 90/186 [21:56<18:29, 11.56s/it] 49%|████▉     | 91/186 [22:06<17:46, 11.22s/it] 49%|████▉     | 92/186 [22:19<18:24, 11.75s/it] 50%|█████     | 93/186 [22:30<17:53, 11.55s/it] 51%|█████     | 94/186 [22:41<17:18, 11.29s/it] 51%|█████     | 95/186 [22:51<16:45, 11.05s/it] 52%|█████▏    | 96/186 [23:02<16:19, 10.88s/it] 52%|█████▏    | 97/186 [23:12<15:52, 10.70s/it] 53%|█████▎    | 98/186 [23:23<15:47, 10.77s/it] 53%|█████▎    | 99/186 [23:34<15:42, 10.84s/it] 54%|█████▍    | 100/186 [23:45<15:21, 10.71s/it] 54%|█████▍    | 101/186 [23:55<15:06, 10.67s/it]WARNING: tokenization mismatch: 1 vs. 48. (ignored)
 55%|█████▍    | 102/186 [24:07<15:14, 10.89s/it] 55%|█████▌    | 103/186 [24:22<17:04, 12.34s/it] 56%|█████▌    | 104/186 [24:38<18:03, 13.21s/it] 56%|█████▋    | 105/186 [24:54<18:58, 14.06s/it] 57%|█████▋    | 106/186 [25:10<19:51, 14.89s/it] 58%|█████▊    | 107/186 [25:26<19:50, 15.07s/it] 58%|█████▊    | 108/186 [25:42<19:52, 15.29s/it] 59%|█████▊    | 109/186 [25:57<19:41, 15.35s/it] 59%|█████▉    | 110/186 [26:13<19:28, 15.38s/it] 60%|█████▉    | 111/186 [26:29<19:32, 15.63s/it] 60%|██████    | 112/186 [26:45<19:27, 15.78s/it] 61%|██████    | 113/186 [27:01<19:13, 15.81s/it] 61%|██████▏   | 114/186 [27:17<19:01, 15.86s/it] 62%|██████▏   | 115/186 [27:33<18:52, 15.95s/it] 62%|██████▏   | 116/186 [27:49<18:38, 15.99s/it] 63%|██████▎   | 117/186 [28:05<18:13, 15.86s/it] 63%|██████▎   | 118/186 [28:21<18:07, 15.99s/it] 64%|██████▍   | 119/186 [28:38<18:05, 16.20s/it] 65%|██████▍   | 120/186 [28:54<17:47, 16.17s/it] 65%|██████▌   | 121/186 [29:10<17:31, 16.18s/it] 66%|██████▌   | 122/186 [29:26<17:08, 16.06s/it] 66%|██████▌   | 123/186 [29:41<16:46, 15.98s/it] 67%|██████▋   | 124/186 [29:57<16:23, 15.87s/it] 67%|██████▋   | 125/186 [30:13<16:04, 15.81s/it] 68%|██████▊   | 126/186 [30:29<15:50, 15.85s/it]WARNING: tokenization mismatch: 1 vs. 47. (ignored)
 68%|██████▊   | 127/186 [30:45<15:36, 15.88s/it] 69%|██████▉   | 128/186 [31:01<15:26, 15.98s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1233 > 1048). Running this sequence through the model will result in indexing errors
 69%|██████▉   | 129/186 [31:18<15:22, 16.19s/it] 70%|██████▉   | 130/186 [31:33<15:00, 16.08s/it] 70%|███████   | 131/186 [31:49<14:39, 15.99s/it] 71%|███████   | 132/186 [32:05<14:17, 15.88s/it] 72%|███████▏  | 133/186 [32:19<13:40, 15.48s/it] 72%|███████▏  | 134/186 [32:34<13:15, 15.29s/it] 73%|███████▎  | 135/186 [32:49<12:49, 15.09s/it] 73%|███████▎  | 136/186 [33:04<12:32, 15.05s/it] 74%|███████▎  | 137/186 [33:19<12:21, 15.14s/it] 74%|███████▍  | 138/186 [33:35<12:20, 15.43s/it] 75%|███████▍  | 139/186 [33:47<11:19, 14.46s/it] 75%|███████▌  | 140/186 [33:58<10:13, 13.33s/it] 76%|███████▌  | 141/186 [34:08<09:18, 12.41s/it] 76%|███████▋  | 142/186 [34:19<08:40, 11.84s/it] 77%|███████▋  | 143/186 [34:30<08:16, 11.54s/it] 77%|███████▋  | 144/186 [34:42<08:13, 11.76s/it] 78%|███████▊  | 145/186 [34:54<08:04, 11.81s/it] 78%|███████▊  | 146/186 [35:05<07:38, 11.47s/it] 79%|███████▉  | 147/186 [35:15<07:20, 11.29s/it] 80%|███████▉  | 148/186 [35:26<07:03, 11.15s/it] 80%|████████  | 149/186 [35:37<06:44, 10.93s/it] 81%|████████  | 150/186 [35:48<06:35, 10.98s/it] 81%|████████  | 151/186 [35:59<06:29, 11.14s/it] 82%|████████▏ | 152/186 [36:10<06:13, 11.00s/it] 82%|████████▏ | 153/186 [36:21<06:01, 10.97s/it] 83%|████████▎ | 154/186 [36:32<05:57, 11.17s/it] 83%|████████▎ | 155/186 [36:48<06:30, 12.60s/it] 84%|████████▍ | 156/186 [37:04<06:46, 13.56s/it] 84%|████████▍ | 157/186 [37:20<06:51, 14.18s/it] 85%|████████▍ | 158/186 [37:36<06:52, 14.72s/it] 85%|████████▌ | 159/186 [37:52<06:48, 15.12s/it] 86%|████████▌ | 160/186 [38:08<06:41, 15.44s/it] 87%|████████▋ | 161/186 [38:24<06:26, 15.47s/it] 87%|████████▋ | 162/186 [38:40<06:14, 15.62s/it] 88%|████████▊ | 163/186 [38:55<06:00, 15.67s/it] 88%|████████▊ | 164/186 [39:12<05:50, 15.92s/it] 89%|████████▊ | 165/186 [39:28<05:32, 15.85s/it] 89%|████████▉ | 166/186 [39:43<05:17, 15.85s/it] 90%|████████▉ | 167/186 [40:00<05:03, 15.99s/it] 90%|█████████ | 168/186 [40:16<04:47, 15.99s/it] 91%|█████████ | 169/186 [40:32<04:31, 15.97s/it] 91%|█████████▏| 170/186 [40:48<04:15, 15.97s/it] 92%|█████████▏| 171/186 [41:04<04:00, 16.05s/it] 92%|█████████▏| 172/186 [41:20<03:43, 15.99s/it] 93%|█████████▎| 173/186 [41:36<03:28, 16.01s/it] 94%|█████████▎| 174/186 [41:52<03:11, 15.99s/it] 94%|█████████▍| 175/186 [42:08<02:55, 15.99s/it] 95%|█████████▍| 176/186 [42:24<02:40, 16.03s/it] 95%|█████████▌| 177/186 [42:40<02:24, 16.02s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 96%|█████████▌| 178/186 [42:56<02:07, 15.97s/it] 96%|█████████▌| 179/186 [43:11<01:50, 15.85s/it] 97%|█████████▋| 180/186 [43:27<01:35, 15.93s/it] 97%|█████████▋| 181/186 [43:43<01:19, 15.97s/it] 98%|█████████▊| 182/186 [44:00<01:04, 16.12s/it] 98%|█████████▊| 183/186 [44:16<00:48, 16.24s/it] 99%|█████████▉| 184/186 [44:32<00:32, 16.03s/it] 99%|█████████▉| 185/186 [44:47<00:15, 15.69s/it]100%|██████████| 186/186 [45:02<00:00, 15.49s/it]                                                 {'train_runtime': 2702.3903, 'train_samples_per_second': 4.972, 'train_steps_per_second': 0.069, 'train_loss': 0.8200129027007729, 'epoch': 1.0}
100%|██████████| 186/186 [45:03<00:00, 15.49s/it]100%|██████████| 186/186 [45:03<00:00, 14.53s/it]
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.3-3
[2025-10-15 09:02:33,349] [INFO] [launch.py:347:main] Process 516064 exits successfully.
[2025-10-15 09:02:34,351] [INFO] [launch.py:347:main] Process 516062 exits successfully.
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33mdirect-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.3-3[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251015_081534-k7zqm3w4/logs[0m
[2025-10-15 09:02:35,352] [INFO] [launch.py:347:main] Process 516066 exits successfully.
[2025-10-15 09:02:36,354] [INFO] [launch.py:347:main] Process 516065 exits successfully.
[2025-10-15 09:02:36,354] [INFO] [launch.py:347:main] Process 516063 exits successfully.
[2025-10-15 09:02:40,359] [INFO] [launch.py:347:main] Process 516061 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-15 09:02:44,700] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 09:02:45,409] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-15 09:02:45,409] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12354 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-3 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-3 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 1 --use_anchor False --yilin_anchor True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-15 09:02:47,959] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 09:02:48,991] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-15 09:02:48,991] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-15 09:02:48,991] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-15 09:02:48,991] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-15 09:02:48,991] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-15 09:02:52,808] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 09:02:52,820] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 09:02:53,135] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 09:02:53,148] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-15 09:02:53,359] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 09:02:53,712] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-15 09:02:53,942] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-15 09:02:54,174] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 09:02:54,257] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 09:02:54,498] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-15 09:02:54,875] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 09:02:55,329] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-15 09:02:55,330] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251015_090256-so4mukuj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-3
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/so4mukuj
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.19s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.81s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.75s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.39s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.70s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.44s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.74s/it]
to bfloat16...
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.41s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  6.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.37s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.12s/it]
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.66s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.71s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.66s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.64s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.95s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.66s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.54s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.66s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.69s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.96s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.18s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.55s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.31s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.71s/it]
Formatting inputs...Skip in lazy mode
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.010834217071533203 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.012678146362304688 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.01198124885559082 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10153508186340332 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.1021270751953125 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10196852684020996 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Token indices sequence length is longer than the specified maximum sequence length for this model (1074 > 1048). Running this sequence through the model will result in indexing errors
  1%|          | 1/186 [00:12<37:31, 12.17s/it]  1%|          | 2/186 [00:22<34:22, 11.21s/it]  2%|▏         | 3/186 [00:33<33:01, 10.83s/it]  2%|▏         | 4/186 [00:43<31:50, 10.50s/it]  3%|▎         | 5/186 [00:54<32:38, 10.82s/it]  3%|▎         | 6/186 [01:05<33:03, 11.02s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
  4%|▍         | 7/186 [01:16<32:07, 10.77s/it]  4%|▍         | 8/186 [01:26<31:19, 10.56s/it]  5%|▍         | 9/186 [01:37<31:23, 10.64s/it]  5%|▌         | 10/186 [01:52<35:27, 12.09s/it]  6%|▌         | 11/186 [02:08<38:30, 13.20s/it]  6%|▋         | 12/186 [02:24<40:45, 14.05s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
  7%|▋         | 13/186 [02:39<41:54, 14.53s/it]  8%|▊         | 14/186 [02:55<42:35, 14.86s/it]  8%|▊         | 15/186 [03:11<43:39, 15.32s/it]  9%|▊         | 16/186 [03:27<43:43, 15.43s/it]  9%|▉         | 17/186 [03:43<43:40, 15.51s/it] 10%|▉         | 18/186 [03:58<43:28, 15.52s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1158 > 1048). Running this sequence through the model will result in indexing errors
 10%|█         | 19/186 [04:15<43:59, 15.81s/it] 11%|█         | 20/186 [04:30<43:35, 15.76s/it] 11%|█▏        | 21/186 [04:46<43:25, 15.79s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1233 > 1048). Running this sequence through the model will result in indexing errors
 12%|█▏        | 22/186 [05:02<43:28, 15.91s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 12%|█▏        | 23/186 [05:19<43:26, 15.99s/it] 13%|█▎        | 24/186 [05:35<43:24, 16.07s/it] 13%|█▎        | 25/186 [05:51<43:04, 16.06s/it] 14%|█▍        | 26/186 [06:07<42:35, 15.97s/it] 15%|█▍        | 27/186 [06:23<42:23, 16.00s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1240 > 1048). Running this sequence through the model will result in indexing errors
 15%|█▌        | 28/186 [06:39<42:27, 16.12s/it] 16%|█▌        | 29/186 [06:55<42:05, 16.08s/it] 16%|█▌        | 30/186 [07:11<41:40, 16.03s/it] 17%|█▋        | 31/186 [07:27<41:22, 16.02s/it] 17%|█▋        | 32/186 [07:43<40:53, 15.93s/it] 18%|█▊        | 33/186 [07:59<40:55, 16.05s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 18%|█▊        | 34/186 [08:15<40:32, 16.00s/it] 19%|█▉        | 35/186 [08:31<40:20, 16.03s/it] 19%|█▉        | 36/186 [08:47<39:57, 15.98s/it] 20%|█▉        | 37/186 [09:03<40:04, 16.13s/it] 20%|██        | 38/186 [09:19<39:40, 16.08s/it] 21%|██        | 39/186 [09:34<38:42, 15.80s/it] 22%|██▏       | 40/186 [09:49<37:48, 15.53s/it] 22%|██▏       | 41/186 [10:04<37:10, 15.38s/it] 23%|██▎       | 42/186 [10:19<36:40, 15.28s/it] 23%|██▎       | 43/186 [10:35<36:25, 15.29s/it] 24%|██▎       | 44/186 [10:50<36:03, 15.23s/it] 24%|██▍       | 45/186 [11:06<36:27, 15.51s/it] 25%|██▍       | 46/186 [11:19<34:27, 14.77s/it] 25%|██▌       | 47/186 [11:30<31:26, 13.57s/it] 26%|██▌       | 48/186 [11:41<29:17, 12.73s/it] 26%|██▋       | 49/186 [11:51<27:28, 12.03s/it] 27%|██▋       | 50/186 [12:01<26:04, 11.50s/it] 27%|██▋       | 51/186 [12:12<25:29, 11.33s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
 28%|██▊       | 52/186 [12:23<24:58, 11.18s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1307 > 1048). Running this sequence through the model will result in indexing errors
 28%|██▊       | 53/186 [12:33<24:12, 10.92s/it] 29%|██▉       | 54/186 [12:44<23:50, 10.84s/it] 30%|██▉       | 55/186 [12:55<23:29, 10.76s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 30%|███       | 56/186 [13:05<23:09, 10.69s/it] 31%|███       | 57/186 [13:15<22:48, 10.61s/it] 31%|███       | 58/186 [13:27<23:02, 10.80s/it] 32%|███▏      | 59/186 [13:38<22:58, 10.85s/it] 32%|███▏      | 60/186 [13:48<22:41, 10.81s/it] 33%|███▎      | 61/186 [13:59<22:34, 10.84s/it] 33%|███▎      | 62/186 [14:11<22:42, 10.99s/it] 34%|███▍      | 63/186 [14:22<22:51, 11.15s/it] 34%|███▍      | 64/186 [14:34<23:11, 11.41s/it] 35%|███▍      | 65/186 [14:46<23:14, 11.53s/it] 35%|███▌      | 66/186 [14:58<23:12, 11.60s/it] 36%|███▌      | 67/186 [15:10<23:13, 11.71s/it] 37%|███▋      | 68/186 [15:22<23:17, 11.84s/it] 37%|███▋      | 69/186 [15:34<23:18, 11.95s/it] 38%|███▊      | 70/186 [15:46<23:20, 12.07s/it] 38%|███▊      | 71/186 [15:58<22:56, 11.97s/it] 39%|███▊      | 72/186 [16:10<22:47, 11.99s/it] 39%|███▉      | 73/186 [16:22<22:35, 11.99s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 40%|███▉      | 74/186 [16:34<22:16, 11.93s/it] 40%|████      | 75/186 [16:46<22:01, 11.90s/it] 41%|████      | 76/186 [16:58<21:48, 11.89s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 41%|████▏     | 77/186 [17:09<21:32, 11.86s/it] 42%|████▏     | 78/186 [17:21<21:09, 11.75s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 42%|████▏     | 79/186 [17:33<21:02, 11.80s/it] 43%|████▎     | 80/186 [17:45<20:45, 11.75s/it] 44%|████▎     | 81/186 [17:56<20:31, 11.73s/it] 44%|████▍     | 82/186 [18:08<20:18, 11.72s/it] 45%|████▍     | 83/186 [18:19<20:02, 11.67s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 45%|████▌     | 84/186 [18:31<19:54, 11.71s/it] 46%|████▌     | 85/186 [18:43<19:40, 11.69s/it] 46%|████▌     | 86/186 [18:55<19:28, 11.68s/it] 47%|████▋     | 87/186 [19:06<19:14, 11.66s/it] 47%|████▋     | 88/186 [19:18<19:05, 11.69s/it] 48%|████▊     | 89/186 [19:30<18:58, 11.73s/it] 48%|████▊     | 90/186 [19:42<19:04, 11.92s/it] 49%|████▉     | 91/186 [19:54<18:52, 11.93s/it] 49%|████▉     | 92/186 [20:06<18:37, 11.89s/it] 50%|█████     | 93/186 [20:18<18:25, 11.88s/it] 51%|█████     | 94/186 [20:29<18:07, 11.82s/it] 51%|█████     | 95/186 [20:41<17:45, 11.71s/it] 52%|█████▏    | 96/186 [20:52<17:26, 11.63s/it] 52%|█████▏    | 97/186 [21:04<17:23, 11.72s/it] 53%|█████▎    | 98/186 [21:16<17:13, 11.74s/it] 53%|█████▎    | 99/186 [21:28<17:00, 11.73s/it] 54%|█████▍    | 100/186 [21:40<17:02, 11.89s/it] 54%|█████▍    | 101/186 [21:52<16:47, 11.85s/it] 55%|█████▍    | 102/186 [22:04<16:37, 11.88s/it] 55%|█████▌    | 103/186 [22:16<16:26, 11.89s/it] 56%|█████▌    | 104/186 [22:27<16:08, 11.81s/it] 56%|█████▋    | 105/186 [22:39<15:56, 11.81s/it] 57%|█████▋    | 106/186 [22:51<15:45, 11.81s/it] 58%|█████▊    | 107/186 [23:03<15:30, 11.77s/it] 58%|█████▊    | 108/186 [23:14<15:13, 11.72s/it] 59%|█████▊    | 109/186 [23:26<15:13, 11.87s/it] 59%|█████▉    | 110/186 [23:38<15:01, 11.86s/it] 60%|█████▉    | 111/186 [23:50<14:44, 11.79s/it] 60%|██████    | 112/186 [24:01<14:25, 11.69s/it] 61%|██████    | 113/186 [24:13<14:09, 11.64s/it] 61%|██████▏   | 114/186 [24:24<13:59, 11.65s/it] 62%|██████▏   | 115/186 [24:36<13:47, 11.66s/it] 62%|██████▏   | 116/186 [24:48<13:41, 11.74s/it] 63%|██████▎   | 117/186 [25:00<13:26, 11.68s/it] 63%|██████▎   | 118/186 [25:11<13:13, 11.67s/it] 64%|██████▍   | 119/186 [25:23<12:59, 11.63s/it] 65%|██████▍   | 120/186 [25:35<12:51, 11.69s/it] 65%|██████▌   | 121/186 [25:47<12:50, 11.85s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 66%|██████▌   | 122/186 [25:59<12:39, 11.87s/it] 66%|██████▌   | 123/186 [26:11<12:30, 11.92s/it] 67%|██████▋   | 124/186 [26:23<12:15, 11.87s/it] 67%|██████▋   | 125/186 [26:34<12:00, 11.81s/it] 68%|██████▊   | 126/186 [26:46<11:45, 11.76s/it] 68%|██████▊   | 127/186 [26:58<11:34, 11.77s/it] 69%|██████▉   | 128/186 [27:09<11:21, 11.75s/it]WARNING: tokenization mismatch: 1 vs. 48. (ignored)
 69%|██████▉   | 129/186 [27:21<11:07, 11.71s/it] 70%|██████▉   | 130/186 [27:33<10:58, 11.76s/it] 70%|███████   | 131/186 [27:45<10:47, 11.78s/it] 71%|███████   | 132/186 [27:57<10:40, 11.86s/it] 72%|███████▏  | 133/186 [28:08<10:25, 11.81s/it] 72%|███████▏  | 134/186 [28:20<10:10, 11.74s/it] 73%|███████▎  | 135/186 [28:31<09:52, 11.63s/it] 73%|███████▎  | 136/186 [28:43<09:38, 11.56s/it] 74%|███████▎  | 137/186 [28:55<09:34, 11.72s/it] 74%|███████▍  | 138/186 [29:06<09:20, 11.68s/it] 75%|███████▍  | 139/186 [29:19<09:16, 11.84s/it] 75%|███████▌  | 140/186 [29:30<09:01, 11.77s/it] 76%|███████▌  | 141/186 [29:42<08:51, 11.80s/it] 76%|███████▋  | 142/186 [29:54<08:37, 11.75s/it] 77%|███████▋  | 143/186 [30:06<08:26, 11.77s/it] 77%|███████▋  | 144/186 [30:17<08:13, 11.74s/it] 78%|███████▊  | 145/186 [30:29<08:03, 11.79s/it] 78%|███████▊  | 146/186 [30:41<07:50, 11.77s/it] 79%|███████▉  | 147/186 [30:53<07:40, 11.81s/it] 80%|███████▉  | 148/186 [31:05<07:29, 11.83s/it] 80%|████████  | 149/186 [31:16<07:16, 11.79s/it] 81%|████████  | 150/186 [31:28<07:02, 11.75s/it] 81%|████████  | 151/186 [31:40<06:50, 11.72s/it] 82%|████████▏ | 152/186 [31:52<06:41, 11.80s/it] 82%|████████▏ | 153/186 [32:04<06:30, 11.83s/it] 83%|████████▎ | 154/186 [32:15<06:16, 11.78s/it] 83%|████████▎ | 155/186 [32:27<06:09, 11.92s/it] 84%|████████▍ | 156/186 [32:39<05:56, 11.90s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1074 > 1048). Running this sequence through the model will result in indexing errors
 84%|████████▍ | 157/186 [32:51<05:45, 11.90s/it] 85%|████████▍ | 158/186 [33:03<05:31, 11.85s/it] 85%|████████▌ | 159/186 [33:15<05:20, 11.88s/it] 86%|████████▌ | 160/186 [33:27<05:06, 11.80s/it] 87%|████████▋ | 161/186 [33:38<04:54, 11.79s/it] 87%|████████▋ | 162/186 [33:50<04:45, 11.91s/it] 88%|████████▊ | 163/186 [34:02<04:31, 11.83s/it] 88%|████████▊ | 164/186 [34:14<04:20, 11.86s/it] 89%|████████▊ | 165/186 [34:26<04:08, 11.82s/it] 89%|████████▉ | 166/186 [34:38<03:56, 11.81s/it] 90%|████████▉ | 167/186 [34:49<03:44, 11.83s/it] 90%|█████████ | 168/186 [35:01<03:32, 11.83s/it] 91%|█████████ | 169/186 [35:13<03:21, 11.85s/it] 91%|█████████▏| 170/186 [35:25<03:09, 11.84s/it] 92%|█████████▏| 171/186 [35:37<02:59, 11.98s/it] 92%|█████████▏| 172/186 [35:49<02:47, 11.97s/it] 93%|█████████▎| 173/186 [36:01<02:35, 11.93s/it] 94%|█████████▎| 174/186 [36:13<02:23, 11.94s/it] 94%|█████████▍| 175/186 [36:25<02:10, 11.89s/it] 95%|█████████▍| 176/186 [36:37<01:58, 11.85s/it] 95%|█████████▌| 177/186 [36:48<01:46, 11.82s/it] 96%|█████████▌| 178/186 [37:00<01:34, 11.80s/it] 96%|█████████▌| 179/186 [37:12<01:22, 11.84s/it] 97%|█████████▋| 180/186 [37:24<01:10, 11.78s/it] 97%|█████████▋| 181/186 [37:35<00:58, 11.78s/it] 98%|█████████▊| 182/186 [37:47<00:47, 11.77s/it] 98%|█████████▊| 183/186 [37:59<00:35, 11.83s/it] 99%|█████████▉| 184/186 [38:11<00:23, 11.85s/it] 99%|█████████▉| 185/186 [38:22<00:11, 11.64s/it]100%|██████████| 186/186 [38:33<00:00, 11.46s/it]                                                 {'train_runtime': 2313.7012, 'train_samples_per_second': 5.807, 'train_steps_per_second': 0.08, 'train_loss': 0.8330687656197496, 'epoch': 1.0}
100%|██████████| 186/186 [38:34<00:00, 11.46s/it]100%|██████████| 186/186 [38:34<00:00, 12.44s/it]
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-3
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33mdirect-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-3[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251015_090256-so4mukuj/logs[0m
[2025-10-15 09:43:18,868] [INFO] [launch.py:347:main] Process 519870 exits successfully.
[2025-10-15 09:43:18,869] [INFO] [launch.py:347:main] Process 519869 exits successfully.
[2025-10-15 09:43:18,869] [INFO] [launch.py:347:main] Process 519871 exits successfully.
[2025-10-15 09:43:18,869] [INFO] [launch.py:347:main] Process 519868 exits successfully.
[2025-10-15 09:43:18,870] [INFO] [launch.py:347:main] Process 519872 exits successfully.
[2025-10-15 09:43:23,875] [INFO] [launch.py:347:main] Process 519867 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-15 09:43:27,261] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 09:43:27,811] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-15 09:43:27,812] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12354 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-4 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-4 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 0.5 --use_anchor False --yilin_anchor True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-15 09:43:29,517] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 09:43:30,118] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-15 09:43:30,119] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-15 09:43:30,119] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-15 09:43:30,119] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-15 09:43:30,119] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-15 09:43:33,831] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 09:43:33,888] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 09:43:33,900] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 09:43:34,160] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 09:43:34,217] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-15 09:43:34,218] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-15 09:43:34,218] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-10-15 09:43:34,346] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 09:43:34,661] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
[2025-10-15 09:43:35,263] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-15 09:43:35,453] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
[2025-10-15 09:43:35,663] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-15 09:43:35,949] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251015_094336-pkdt0uc2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run direct-yilin_anchor-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-4
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/pkdt0uc2
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.24s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.27s/it]
to bfloat16...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.12s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.20s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.77s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.13s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.96s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.98s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.63s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.02s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.51s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.88s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.00s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.75s/it]
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.46s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.68s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.99s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.36s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.08s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.32s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.59s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.80s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.89s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.34s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.87s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.90s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.99s/it]
Formatting inputs...Skip in lazy mode
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.013163089752197266 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.012937545776367188 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10151028633117676 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10127091407775879 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.013056516647338867 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10101580619812012 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/186 [00:08<26:27,  8.58s/it]  1%|          | 2/186 [00:18<28:39,  9.34s/it]  2%|▏         | 3/186 [00:29<30:10,  9.89s/it]  2%|▏         | 4/186 [00:39<30:55, 10.19s/it]  3%|▎         | 5/186 [00:49<30:41, 10.18s/it]  3%|▎         | 6/186 [01:00<30:52, 10.29s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
  4%|▍         | 7/186 [01:11<31:17, 10.49s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1074 > 1048). Running this sequence through the model will result in indexing errors
  4%|▍         | 8/186 [01:22<31:50, 10.73s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1240 > 1048). Running this sequence through the model will result in indexing errors
  5%|▍         | 9/186 [01:33<31:46, 10.77s/it]  5%|▌         | 10/186 [01:44<31:55, 10.88s/it]  6%|▌         | 11/186 [01:55<31:41, 10.87s/it]  6%|▋         | 12/186 [02:06<31:35, 10.90s/it]  7%|▋         | 13/186 [02:17<31:30, 10.93s/it]  8%|▊         | 14/186 [02:28<31:39, 11.04s/it]  8%|▊         | 15/186 [02:39<31:08, 10.93s/it]  9%|▊         | 16/186 [02:49<30:35, 10.80s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
  9%|▉         | 17/186 [03:00<30:24, 10.80s/it] 10%|▉         | 18/186 [03:11<30:16, 10.81s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1073 > 1048). Running this sequence through the model will result in indexing errors
 10%|█         | 19/186 [03:22<30:19, 10.90s/it] 11%|█         | 20/186 [03:33<30:09, 10.90s/it] 11%|█▏        | 21/186 [03:44<29:54, 10.87s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1221 > 1048). Running this sequence through the model will result in indexing errors
 12%|█▏        | 22/186 [03:55<30:17, 11.08s/it] 12%|█▏        | 23/186 [04:06<29:54, 11.01s/it] 13%|█▎        | 24/186 [04:17<29:32, 10.94s/it] 13%|█▎        | 25/186 [04:28<29:20, 10.93s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 14%|█▍        | 26/186 [04:38<28:52, 10.83s/it] 15%|█▍        | 27/186 [04:50<29:04, 10.97s/it] 15%|█▌        | 28/186 [05:01<28:58, 11.01s/it] 16%|█▌        | 29/186 [05:12<28:40, 10.96s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 16%|█▌        | 30/186 [05:22<28:25, 10.93s/it] 17%|█▋        | 31/186 [05:33<28:03, 10.86s/it] 17%|█▋        | 32/186 [05:44<27:44, 10.81s/it] 18%|█▊        | 33/186 [05:55<27:27, 10.77s/it] 18%|█▊        | 34/186 [06:05<27:00, 10.66s/it] 19%|█▉        | 35/186 [06:17<27:36, 10.97s/it] 19%|█▉        | 36/186 [06:27<27:18, 10.93s/it] 20%|█▉        | 37/186 [06:39<27:13, 10.96s/it] 20%|██        | 38/186 [06:49<26:54, 10.91s/it] 21%|██        | 39/186 [07:00<26:38, 10.87s/it] 22%|██▏       | 40/186 [07:11<26:24, 10.85s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 22%|██▏       | 41/186 [07:22<26:34, 11.00s/it] 23%|██▎       | 42/186 [07:33<26:15, 10.94s/it] 23%|██▎       | 43/186 [07:44<26:13, 11.00s/it] 24%|██▎       | 44/186 [07:55<25:46, 10.89s/it] 24%|██▍       | 45/186 [08:06<25:29, 10.85s/it] 25%|██▍       | 46/186 [08:16<25:05, 10.75s/it] 25%|██▌       | 47/186 [08:27<25:00, 10.79s/it] 26%|██▌       | 48/186 [08:38<25:01, 10.88s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 26%|██▋       | 49/186 [08:49<24:47, 10.86s/it] 27%|██▋       | 50/186 [09:00<24:33, 10.84s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 27%|██▋       | 51/186 [09:11<24:38, 10.95s/it] 28%|██▊       | 52/186 [09:22<24:29, 10.96s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1072 > 1048). Running this sequence through the model will result in indexing errors
 28%|██▊       | 53/186 [09:33<24:24, 11.01s/it] 29%|██▉       | 54/186 [09:44<24:30, 11.14s/it] 30%|██▉       | 55/186 [09:55<23:54, 10.95s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 30%|███       | 56/186 [10:06<23:40, 10.93s/it] 31%|███       | 57/186 [10:17<23:28, 10.92s/it] 31%|███       | 58/186 [10:27<23:05, 10.83s/it] 32%|███▏      | 59/186 [10:38<23:04, 10.91s/it] 32%|███▏      | 60/186 [10:49<22:48, 10.86s/it] 33%|███▎      | 61/186 [11:00<22:30, 10.81s/it] 33%|███▎      | 62/186 [11:11<22:23, 10.83s/it] 34%|███▍      | 63/186 [11:21<22:08, 10.80s/it] 34%|███▍      | 64/186 [11:32<21:56, 10.79s/it] 35%|███▍      | 65/186 [11:43<21:42, 10.76s/it] 35%|███▌      | 66/186 [11:54<21:53, 10.94s/it] 36%|███▌      | 67/186 [12:06<21:54, 11.05s/it] 37%|███▋      | 68/186 [12:16<21:33, 10.96s/it] 37%|███▋      | 69/186 [12:28<21:39, 11.10s/it] 38%|███▊      | 70/186 [12:39<21:28, 11.11s/it] 38%|███▊      | 71/186 [12:51<21:40, 11.31s/it] 39%|███▊      | 72/186 [13:02<21:19, 11.22s/it] 39%|███▉      | 73/186 [13:13<21:00, 11.15s/it] 40%|███▉      | 74/186 [13:24<20:44, 11.11s/it] 40%|████      | 75/186 [13:34<20:18, 10.98s/it] 41%|████      | 76/186 [13:45<20:04, 10.95s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 41%|████▏     | 77/186 [13:56<19:53, 10.95s/it] 42%|████▏     | 78/186 [14:07<19:33, 10.86s/it] 42%|████▏     | 79/186 [14:18<19:25, 10.90s/it][2025-10-15 09:59:35,549] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 526728
Terminated
[2025-10-15 09:59:38,990] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 526729
[2025-10-15 09:59:39,484] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 526730
[2025-10-15 09:59:39,527] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 526731
[2025-10-15 09:59:39,562] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 526732
[2025-10-15 09:59:39,637] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 526733
[2025-10-15 09:59:39,659] [INFO] [launch.py:324:sigkill_handler] Main process received SIGTERM, exiting
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-16 22:55:44,989] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-16 22:55:45,552] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-16 22:55:45,553] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12423 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-0 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-0 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 0.5 --use_anchor False --use_sample_weight True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-16 22:55:47,249] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-16 22:55:47,826] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-16 22:55:47,826] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-16 22:55:47,826] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-16 22:55:47,826] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-16 22:55:47,826] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-16 22:55:51,566] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-16 22:55:51,895] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-16 22:55:51,918] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-16 22:55:52,247] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-16 22:55:52,340] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-16 22:55:52,664] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-16 22:55:52,713] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-16 22:55:53,024] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-16 22:55:53,180] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-16 22:55:53,511] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-16 22:55:53,511] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-10-16 22:55:53,562] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-16 22:55:53,990] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251016_225554-ic4f1e9t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-0
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/ic4f1e9t
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.91s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.76s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.08s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.83s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.77s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.82s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.53s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.87s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.85s/it]to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.81s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.73s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.49s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.84s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.90s/it]
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.30s/it]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.69s/it]
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.45s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.26s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.28s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.86s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.25s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.53s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.94s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.10s/it][MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:3')
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.96s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.64s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.04s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.47s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.86s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.79s/it]
Formatting inputs...Skip in lazy mode
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:2')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:5')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:1')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:4')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:0')
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.013914108276367188 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.010281562805175781 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.011471271514892578 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10102987289428711 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10153388977050781 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10158801078796387 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/186 [00:10<30:52, 10.01s/it]  1%|          | 2/186 [00:18<28:35,  9.32s/it]  2%|▏         | 3/186 [00:27<28:08,  9.23s/it]  2%|▏         | 4/186 [00:37<28:06,  9.27s/it]  3%|▎         | 5/186 [00:46<27:54,  9.25s/it]  3%|▎         | 6/186 [00:55<27:44,  9.25s/it]  4%|▍         | 7/186 [01:05<28:05,  9.41s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
  4%|▍         | 8/186 [01:15<28:12,  9.51s/it]  5%|▍         | 9/186 [01:25<28:27,  9.65s/it]  5%|▌         | 10/186 [01:35<28:40,  9.78s/it]  6%|▌         | 11/186 [01:45<28:43,  9.85s/it]  6%|▋         | 12/186 [01:55<28:53,  9.96s/it]  7%|▋         | 13/186 [02:05<28:55, 10.03s/it]  8%|▊         | 14/186 [02:15<28:55, 10.09s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1074 > 1048). Running this sequence through the model will result in indexing errors
  8%|▊         | 15/186 [02:26<29:08, 10.22s/it]  9%|▊         | 16/186 [02:36<28:58, 10.23s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1102 > 1048). Running this sequence through the model will result in indexing errors
  9%|▉         | 17/186 [02:47<29:19, 10.41s/it] 10%|▉         | 18/186 [02:57<29:02, 10.37s/it] 10%|█         | 19/186 [03:08<28:53, 10.38s/it] 11%|█         | 20/186 [03:18<28:47, 10.40s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1233 > 1048). Running this sequence through the model will result in indexing errors
 11%|█▏        | 21/186 [03:29<28:45, 10.46s/it] 12%|█▏        | 22/186 [03:39<28:30, 10.43s/it] 12%|█▏        | 23/186 [03:49<28:15, 10.40s/it] 13%|█▎        | 24/186 [04:00<28:31, 10.57s/it] 13%|█▎        | 25/186 [04:11<28:18, 10.55s/it] 14%|█▍        | 26/186 [04:21<28:01, 10.51s/it] 15%|█▍        | 27/186 [04:32<27:39, 10.44s/it] 15%|█▌        | 28/186 [04:42<27:25, 10.42s/it] 16%|█▌        | 29/186 [04:52<27:21, 10.46s/it] 16%|█▌        | 30/186 [05:03<27:15, 10.48s/it] 17%|█▋        | 31/186 [05:13<27:02, 10.47s/it] 17%|█▋        | 32/186 [05:24<26:58, 10.51s/it] 18%|█▊        | 33/186 [05:35<26:45, 10.50s/it] 18%|█▊        | 34/186 [05:45<26:26, 10.44s/it] 19%|█▉        | 35/186 [05:55<26:19, 10.46s/it] 19%|█▉        | 36/186 [06:06<26:00, 10.40s/it] 20%|█▉        | 37/186 [06:16<25:51, 10.41s/it] 20%|██        | 38/186 [06:27<25:54, 10.50s/it] 21%|██        | 39/186 [06:37<25:49, 10.54s/it] 22%|██▏       | 40/186 [06:48<25:37, 10.53s/it] 22%|██▏       | 41/186 [06:59<25:33, 10.57s/it] 23%|██▎       | 42/186 [07:09<25:22, 10.57s/it] 23%|██▎       | 43/186 [07:20<25:08, 10.55s/it] 24%|██▎       | 44/186 [07:30<25:04, 10.60s/it] 24%|██▍       | 45/186 [07:41<24:51, 10.58s/it] 25%|██▍       | 46/186 [07:52<24:53, 10.67s/it] 25%|██▌       | 47/186 [08:03<24:52, 10.74s/it] 26%|██▌       | 48/186 [08:13<24:33, 10.68s/it] 26%|██▋       | 49/186 [08:24<24:17, 10.64s/it] 27%|██▋       | 50/186 [08:34<24:02, 10.61s/it] 27%|██▋       | 51/186 [08:45<23:48, 10.58s/it] 28%|██▊       | 52/186 [08:55<23:39, 10.59s/it] 28%|██▊       | 53/186 [09:06<23:25, 10.57s/it] 29%|██▉       | 54/186 [09:17<23:29, 10.68s/it] 30%|██▉       | 55/186 [09:27<23:12, 10.63s/it] 30%|███       | 56/186 [09:38<22:55, 10.58s/it] 31%|███       | 57/186 [09:49<22:54, 10.65s/it] 31%|███       | 58/186 [09:59<22:36, 10.60s/it] 32%|███▏      | 59/186 [10:10<22:33, 10.66s/it] 32%|███▏      | 60/186 [10:20<22:19, 10.63s/it] 33%|███▎      | 61/186 [10:31<22:11, 10.65s/it] 33%|███▎      | 62/186 [10:42<21:59, 10.64s/it] 34%|███▍      | 63/186 [10:53<21:54, 10.69s/it] 34%|███▍      | 64/186 [11:03<21:40, 10.66s/it] 35%|███▍      | 65/186 [11:14<21:31, 10.67s/it] 35%|███▌      | 66/186 [11:25<21:22, 10.69s/it] 36%|███▌      | 67/186 [11:35<21:12, 10.69s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1072 > 1048). Running this sequence through the model will result in indexing errors
 37%|███▋      | 68/186 [11:46<21:02, 10.70s/it] 37%|███▋      | 69/186 [11:57<21:01, 10.78s/it] 38%|███▊      | 70/186 [12:08<21:03, 10.89s/it] 38%|███▊      | 71/186 [12:19<20:42, 10.81s/it] 39%|███▊      | 72/186 [12:30<20:32, 10.82s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1143 > 1048). Running this sequence through the model will result in indexing errors
 39%|███▉      | 73/186 [12:41<20:27, 10.86s/it] 40%|███▉      | 74/186 [12:51<20:10, 10.81s/it] 40%|████      | 75/186 [13:02<19:56, 10.78s/it] 41%|████      | 76/186 [13:13<19:41, 10.74s/it] 41%|████▏     | 77/186 [13:23<19:29, 10.73s/it] 42%|████▏     | 78/186 [13:34<19:20, 10.74s/it] 42%|████▏     | 79/186 [13:45<19:05, 10.70s/it] 43%|████▎     | 80/186 [13:55<18:51, 10.67s/it] 44%|████▎     | 81/186 [14:06<18:41, 10.68s/it] 44%|████▍     | 82/186 [14:17<18:29, 10.67s/it] 45%|████▍     | 83/186 [14:27<18:17, 10.65s/it] 45%|████▌     | 84/186 [14:38<18:10, 10.69s/it] 46%|████▌     | 85/186 [14:49<18:02, 10.72s/it] 46%|████▌     | 86/186 [14:59<17:50, 10.70s/it] 47%|████▋     | 87/186 [15:10<17:44, 10.76s/it] 47%|████▋     | 88/186 [15:21<17:31, 10.73s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 48%|████▊     | 89/186 [15:32<17:21, 10.74s/it] 48%|████▊     | 90/186 [15:43<17:10, 10.74s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 49%|████▉     | 91/186 [15:54<17:19, 10.95s/it] 49%|████▉     | 92/186 [16:05<17:01, 10.87s/it] 50%|█████     | 93/186 [16:16<16:51, 10.88s/it] 51%|█████     | 94/186 [16:26<16:35, 10.82s/it] 51%|█████     | 95/186 [16:37<16:20, 10.77s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1075 > 1048). Running this sequence through the model will result in indexing errors
 52%|█████▏    | 96/186 [16:48<16:12, 10.81s/it] 52%|█████▏    | 97/186 [16:58<15:56, 10.75s/it] 53%|█████▎    | 98/186 [17:09<15:42, 10.71s/it] 53%|█████▎    | 99/186 [17:20<15:29, 10.69s/it] 54%|█████▍    | 100/186 [17:31<15:25, 10.76s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 54%|█████▍    | 101/186 [17:41<15:14, 10.76s/it] 55%|█████▍    | 102/186 [17:52<15:01, 10.73s/it] 55%|█████▌    | 103/186 [18:03<14:54, 10.78s/it] 56%|█████▌    | 104/186 [18:14<14:44, 10.79s/it] 56%|█████▋    | 105/186 [18:24<14:33, 10.78s/it] 57%|█████▋    | 106/186 [18:36<14:30, 10.88s/it] 58%|█████▊    | 107/186 [18:46<14:15, 10.83s/it] 58%|█████▊    | 108/186 [18:57<14:05, 10.84s/it] 59%|█████▊    | 109/186 [19:08<13:51, 10.80s/it] 59%|█████▉    | 110/186 [19:19<13:38, 10.77s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 60%|█████▉    | 111/186 [19:30<13:35, 10.87s/it] 60%|██████    | 112/186 [19:40<13:18, 10.79s/it] 61%|██████    | 113/186 [19:51<13:07, 10.79s/it] 61%|██████▏   | 114/186 [20:02<13:00, 10.84s/it] 62%|██████▏   | 115/186 [20:13<12:54, 10.92s/it] 62%|██████▏   | 116/186 [20:24<12:45, 10.94s/it] 63%|██████▎   | 117/186 [20:35<12:27, 10.84s/it] 63%|██████▎   | 118/186 [20:45<12:11, 10.76s/it] 64%|██████▍   | 119/186 [20:56<12:02, 10.79s/it] 65%|██████▍   | 120/186 [21:07<11:55, 10.85s/it] 65%|██████▌   | 121/186 [21:18<11:42, 10.81s/it] 66%|██████▌   | 122/186 [21:29<11:29, 10.77s/it] 66%|██████▌   | 123/186 [21:40<11:23, 10.84s/it] 67%|██████▋   | 124/186 [21:51<11:17, 10.93s/it] 67%|██████▋   | 125/186 [22:02<11:19, 11.14s/it] 68%|██████▊   | 126/186 [22:14<11:21, 11.35s/it] 68%|██████▊   | 127/186 [22:25<11:08, 11.33s/it] 69%|██████▉   | 128/186 [22:37<10:56, 11.32s/it] 69%|██████▉   | 129/186 [22:48<10:45, 11.33s/it] 70%|██████▉   | 130/186 [22:59<10:27, 11.21s/it] 70%|███████   | 131/186 [23:10<10:16, 11.21s/it] 71%|███████   | 132/186 [23:21<10:06, 11.22s/it] 72%|███████▏  | 133/186 [23:32<09:51, 11.16s/it] 72%|███████▏  | 134/186 [23:44<09:40, 11.16s/it] 73%|███████▎  | 135/186 [23:55<09:34, 11.26s/it] 73%|███████▎  | 136/186 [24:06<09:20, 11.22s/it] 74%|███████▎  | 137/186 [24:18<09:17, 11.38s/it] 74%|███████▍  | 138/186 [24:29<09:01, 11.27s/it] 75%|███████▍  | 139/186 [24:41<08:53, 11.36s/it] 75%|███████▌  | 140/186 [24:52<08:39, 11.30s/it] 76%|███████▌  | 141/186 [25:03<08:26, 11.25s/it] 76%|███████▋  | 142/186 [25:14<08:12, 11.19s/it] 77%|███████▋  | 143/186 [25:25<08:02, 11.22s/it] 77%|███████▋  | 144/186 [25:36<07:49, 11.19s/it] 78%|███████▊  | 145/186 [25:48<07:40, 11.24s/it] 78%|███████▊  | 146/186 [25:59<07:30, 11.27s/it] 79%|███████▉  | 147/186 [26:10<07:18, 11.24s/it] 80%|███████▉  | 148/186 [26:22<07:11, 11.36s/it] 80%|████████  | 149/186 [26:33<06:58, 11.30s/it] 81%|████████  | 150/186 [26:44<06:45, 11.27s/it] 81%|████████  | 151/186 [26:56<06:34, 11.28s/it] 82%|████████▏ | 152/186 [27:07<06:21, 11.23s/it] 82%|████████▏ | 153/186 [27:18<06:10, 11.23s/it] 83%|████████▎ | 154/186 [27:29<05:58, 11.20s/it] 83%|████████▎ | 155/186 [27:40<05:47, 11.20s/it] 84%|████████▍ | 156/186 [27:52<05:45, 11.51s/it] 84%|████████▍ | 157/186 [28:04<05:30, 11.40s/it] 85%|████████▍ | 158/186 [28:15<05:17, 11.33s/it] 85%|████████▌ | 159/186 [28:26<05:05, 11.31s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 86%|████████▌ | 160/186 [28:38<04:59, 11.50s/it][2025-10-16 23:26:08,794] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2163588
Terminated
[2025-10-16 23:26:12,398] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2163589
[2025-10-16 23:26:12,447] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2163590
[2025-10-16 23:26:12,481] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2163591
[2025-10-16 23:26:12,506] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2163592
[2025-10-16 23:26:12,527] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2163593
[2025-10-16 23:26:12,547] [INFO] [launch.py:324:sigkill_handler] Main process received SIGTERM, exiting
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-16 23:40:56,097] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-16 23:40:56,663] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-16 23:40:56,663] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12423 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-0 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-0 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 0.5 --use_anchor False --use_sample_weight True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-16 23:40:58,377] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-16 23:40:58,970] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-16 23:40:58,970] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-16 23:40:58,970] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-16 23:40:58,970] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-16 23:40:58,970] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-16 23:41:02,787] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-16 23:41:02,815] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-16 23:41:03,101] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-16 23:41:03,135] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-16 23:41:03,751] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-16 23:41:04,028] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-16 23:41:04,120] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-16 23:41:04,215] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-16 23:41:04,348] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-16 23:41:04,348] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-16 23:41:04,455] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-10-16 23:41:04,746] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-16 23:41:05,302] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251016_234105-txm5a9i6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-0
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/txm5a9i6
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.41s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.55s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.45s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.49s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.78s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.45s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.74s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.08s/it]to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.36s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.84s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.01s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.23s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.63s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.96s/it]
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.35s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.36s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.79s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.67s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.99s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.00s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.68s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.68s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.08s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.91s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.51s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.84s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.71s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.07s/it]
Formatting inputs...Skip in lazy mode
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:2')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:5')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:3')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:4')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:1')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:0')
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...

Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.010612726211547852 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10105252265930176 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.1011667251586914 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.1011805534362793 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.011676788330078125 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.013486862182617188 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/186 [00:09<29:46,  9.65s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1069 > 1048). Running this sequence through the model will result in indexing errors
  1%|          | 2/186 [00:18<28:22,  9.25s/it]  2%|▏         | 3/186 [00:27<27:57,  9.16s/it]  2%|▏         | 4/186 [00:37<28:03,  9.25s/it]  3%|▎         | 5/186 [00:46<28:15,  9.37s/it]  3%|▎         | 6/186 [00:56<28:20,  9.45s/it]  4%|▍         | 7/186 [01:06<28:38,  9.60s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1069 > 1048). Running this sequence through the model will result in indexing errors
  4%|▍         | 8/186 [01:16<29:01,  9.79s/it]  5%|▍         | 9/186 [01:26<29:28,  9.99s/it]  5%|▌         | 10/186 [01:36<29:23, 10.02s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1065 > 1048). Running this sequence through the model will result in indexing errors
  6%|▌         | 11/186 [01:47<29:41, 10.18s/it]  6%|▋         | 12/186 [01:57<29:38, 10.22s/it]  7%|▋         | 13/186 [02:08<29:32, 10.25s/it]  8%|▊         | 14/186 [02:18<29:21, 10.24s/it]  8%|▊         | 15/186 [02:28<29:16, 10.27s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
  9%|▊         | 16/186 [02:39<29:14, 10.32s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1165 > 1048). Running this sequence through the model will result in indexing errors
  9%|▉         | 17/186 [02:49<29:20, 10.41s/it] 10%|▉         | 18/186 [03:00<29:24, 10.51s/it] 10%|█         | 19/186 [03:10<29:08, 10.47s/it] 11%|█         | 20/186 [03:21<29:03, 10.51s/it] 11%|█▏        | 21/186 [03:32<29:12, 10.62s/it] 12%|█▏        | 22/186 [03:42<28:57, 10.59s/it] 12%|█▏        | 23/186 [03:53<28:54, 10.64s/it] 13%|█▎        | 24/186 [04:04<28:59, 10.74s/it] 13%|█▎        | 25/186 [04:15<28:43, 10.70s/it] 14%|█▍        | 26/186 [04:25<28:21, 10.64s/it] 15%|█▍        | 27/186 [04:36<28:16, 10.67s/it] 15%|█▌        | 28/186 [04:47<28:06, 10.67s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 16%|█▌        | 29/186 [04:57<27:53, 10.66s/it] 16%|█▌        | 30/186 [05:08<27:43, 10.66s/it] 17%|█▋        | 31/186 [05:18<27:26, 10.62s/it] 17%|█▋        | 32/186 [05:29<27:14, 10.61s/it] 18%|█▊        | 33/186 [05:40<27:02, 10.60s/it] 18%|█▊        | 34/186 [05:50<26:56, 10.64s/it] 19%|█▉        | 35/186 [06:01<26:54, 10.69s/it] 19%|█▉        | 36/186 [06:12<26:37, 10.65s/it] 20%|█▉        | 37/186 [06:22<26:21, 10.61s/it] 20%|██        | 38/186 [06:33<26:08, 10.60s/it] 21%|██        | 39/186 [06:43<25:58, 10.60s/it] 22%|██▏       | 40/186 [06:54<25:49, 10.61s/it] 22%|██▏       | 41/186 [07:05<25:42, 10.64s/it] 23%|██▎       | 42/186 [07:16<25:44, 10.72s/it] 23%|██▎       | 43/186 [07:26<25:32, 10.72s/it] 24%|██▎       | 44/186 [07:37<25:20, 10.71s/it] 24%|██▍       | 45/186 [07:48<25:17, 10.76s/it] 25%|██▍       | 46/186 [07:59<25:08, 10.77s/it] 25%|██▌       | 47/186 [08:09<24:59, 10.78s/it] 26%|██▌       | 48/186 [08:20<24:46, 10.77s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1073 > 1048). Running this sequence through the model will result in indexing errors
 26%|██▋       | 49/186 [08:31<24:34, 10.76s/it] 27%|██▋       | 50/186 [08:42<24:25, 10.78s/it] 27%|██▋       | 51/186 [08:52<24:10, 10.75s/it] 28%|██▊       | 52/186 [09:03<24:00, 10.75s/it] 28%|██▊       | 53/186 [09:14<23:51, 10.76s/it] 29%|██▉       | 54/186 [09:25<23:33, 10.71s/it] 30%|██▉       | 55/186 [09:35<23:27, 10.74s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1080 > 1048). Running this sequence through the model will result in indexing errors
 30%|███       | 56/186 [09:46<23:26, 10.82s/it] 31%|███       | 57/186 [09:57<23:14, 10.81s/it] 31%|███       | 58/186 [10:08<22:59, 10.78s/it] 32%|███▏      | 59/186 [10:19<22:52, 10.81s/it] 32%|███▏      | 60/186 [10:30<22:41, 10.80s/it] 33%|███▎      | 61/186 [10:40<22:27, 10.78s/it] 33%|███▎      | 62/186 [10:51<22:13, 10.76s/it] 34%|███▍      | 63/186 [11:02<22:02, 10.75s/it] 34%|███▍      | 64/186 [11:13<21:56, 10.79s/it] 35%|███▍      | 65/186 [11:23<21:42, 10.77s/it] 35%|███▌      | 66/186 [11:34<21:45, 10.88s/it] 36%|███▌      | 67/186 [11:45<21:30, 10.84s/it] 37%|███▋      | 68/186 [11:56<21:16, 10.82s/it] 37%|███▋      | 69/186 [12:07<21:12, 10.87s/it] 38%|███▊      | 70/186 [12:18<20:59, 10.86s/it] 38%|███▊      | 71/186 [12:29<20:46, 10.84s/it] 39%|███▊      | 72/186 [12:39<20:37, 10.85s/it] 39%|███▉      | 73/186 [12:50<20:22, 10.82s/it] 40%|███▉      | 74/186 [13:01<20:01, 10.73s/it] 40%|████      | 75/186 [13:11<19:48, 10.70s/it] 41%|████      | 76/186 [13:22<19:39, 10.72s/it] 41%|████▏     | 77/186 [13:33<19:31, 10.75s/it] 42%|████▏     | 78/186 [13:44<19:25, 10.79s/it] 42%|████▏     | 79/186 [13:55<19:20, 10.85s/it] 43%|████▎     | 80/186 [14:06<19:11, 10.86s/it] 44%|████▎     | 81/186 [14:17<19:00, 10.86s/it] 44%|████▍     | 82/186 [14:27<18:45, 10.82s/it] 45%|████▍     | 83/186 [14:38<18:32, 10.80s/it] 45%|████▌     | 84/186 [14:49<18:22, 10.81s/it] 46%|████▌     | 85/186 [15:00<18:09, 10.79s/it] 46%|████▌     | 86/186 [15:10<17:59, 10.80s/it] 47%|████▋     | 87/186 [15:21<17:48, 10.79s/it] 47%|████▋     | 88/186 [15:32<17:34, 10.76s/it] 48%|████▊     | 89/186 [15:43<17:22, 10.75s/it] 48%|████▊     | 90/186 [15:53<17:14, 10.78s/it] 49%|████▉     | 91/186 [16:04<17:10, 10.85s/it] 49%|████▉     | 92/186 [16:15<17:00, 10.86s/it] 50%|█████     | 93/186 [16:26<16:49, 10.86s/it] 51%|█████     | 94/186 [16:37<16:36, 10.83s/it] 51%|█████     | 95/186 [16:48<16:25, 10.83s/it] 52%|█████▏    | 96/186 [16:59<16:11, 10.80s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 52%|█████▏    | 97/186 [17:09<15:56, 10.75s/it] 53%|█████▎    | 98/186 [17:20<15:47, 10.77s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 53%|█████▎    | 99/186 [17:31<15:38, 10.79s/it] 54%|█████▍    | 100/186 [17:42<15:35, 10.88s/it] 54%|█████▍    | 101/186 [17:53<15:24, 10.88s/it] 55%|█████▍    | 102/186 [18:04<15:17, 10.93s/it] 55%|█████▌    | 103/186 [18:15<15:08, 10.95s/it] 56%|█████▌    | 104/186 [18:25<14:46, 10.81s/it] 56%|█████▋    | 105/186 [18:36<14:36, 10.82s/it] 57%|█████▋    | 106/186 [18:47<14:23, 10.80s/it] 58%|█████▊    | 107/186 [18:58<14:13, 10.80s/it] 58%|█████▊    | 108/186 [19:09<14:04, 10.83s/it] 59%|█████▊    | 109/186 [19:20<14:02, 10.94s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
 59%|█████▉    | 110/186 [19:31<13:48, 10.91s/it] 60%|█████▉    | 111/186 [19:41<13:33, 10.85s/it] 60%|██████    | 112/186 [19:52<13:18, 10.80s/it] 61%|██████    | 113/186 [20:03<13:06, 10.78s/it] 61%|██████▏   | 114/186 [20:13<12:53, 10.74s/it] 62%|██████▏   | 115/186 [20:24<12:41, 10.72s/it] 62%|██████▏   | 116/186 [20:35<12:37, 10.83s/it] 63%|██████▎   | 117/186 [20:46<12:27, 10.83s/it] 63%|██████▎   | 118/186 [20:57<12:15, 10.82s/it] 64%|██████▍   | 119/186 [21:07<12:02, 10.78s/it] 65%|██████▍   | 120/186 [21:18<11:54, 10.83s/it] 65%|██████▌   | 121/186 [21:29<11:44, 10.84s/it] 66%|██████▌   | 122/186 [21:40<11:34, 10.86s/it] 66%|██████▌   | 123/186 [21:51<11:29, 10.94s/it] 67%|██████▋   | 124/186 [22:02<11:16, 10.91s/it] 67%|██████▋   | 125/186 [22:13<11:04, 10.89s/it] 68%|██████▊   | 126/186 [22:24<10:50, 10.84s/it] 68%|██████▊   | 127/186 [22:35<10:38, 10.83s/it] 69%|██████▉   | 128/186 [22:45<10:29, 10.85s/it] 69%|██████▉   | 129/186 [22:56<10:16, 10.82s/it] 70%|██████▉   | 130/186 [23:07<10:07, 10.85s/it] 70%|███████   | 131/186 [23:18<09:57, 10.86s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 71%|███████   | 132/186 [23:29<09:48, 10.90s/it] 72%|███████▏  | 133/186 [23:40<09:37, 10.89s/it] 72%|███████▏  | 134/186 [23:51<09:24, 10.85s/it] 73%|███████▎  | 135/186 [24:01<09:13, 10.85s/it] 73%|███████▎  | 136/186 [24:12<08:59, 10.79s/it] 74%|███████▎  | 137/186 [24:23<08:53, 10.90s/it] 74%|███████▍  | 138/186 [24:34<08:41, 10.86s/it] 75%|███████▍  | 139/186 [24:45<08:34, 10.94s/it] 75%|███████▌  | 140/186 [24:56<08:26, 11.01s/it] 76%|███████▌  | 141/186 [25:07<08:11, 10.92s/it] 76%|███████▋  | 142/186 [25:18<07:57, 10.84s/it] 77%|███████▋  | 143/186 [25:29<07:46, 10.86s/it] 77%|███████▋  | 144/186 [25:39<07:34, 10.83s/it] 78%|███████▊  | 145/186 [25:50<07:23, 10.81s/it] 78%|███████▊  | 146/186 [26:01<07:14, 10.87s/it] 79%|███████▉  | 147/186 [26:12<07:03, 10.86s/it] 80%|███████▉  | 148/186 [26:23<06:53, 10.88s/it] 80%|████████  | 149/186 [26:34<06:44, 10.94s/it]WARNING: tokenization mismatch: 1 vs. 47. (ignored)
 81%|████████  | 150/186 [26:45<06:31, 10.87s/it] 81%|████████  | 151/186 [26:55<06:18, 10.82s/it] 82%|████████▏ | 152/186 [27:06<06:09, 10.86s/it] 82%|████████▏ | 153/186 [27:17<06:00, 10.94s/it] 83%|████████▎ | 154/186 [27:28<05:48, 10.88s/it] 83%|████████▎ | 155/186 [27:39<05:39, 10.94s/it] 84%|████████▍ | 156/186 [27:51<05:31, 11.06s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 84%|████████▍ | 157/186 [28:02<05:19, 11.00s/it] 85%|████████▍ | 158/186 [28:12<05:06, 10.93s/it] 85%|████████▌ | 159/186 [28:23<04:55, 10.93s/it] 86%|████████▌ | 160/186 [28:34<04:44, 10.95s/it] 87%|████████▋ | 161/186 [28:45<04:33, 10.92s/it] 87%|████████▋ | 162/186 [28:57<04:26, 11.10s/it] 88%|████████▊ | 163/186 [29:08<04:15, 11.11s/it] 88%|████████▊ | 164/186 [29:18<04:01, 10.99s/it] 89%|████████▊ | 165/186 [29:30<03:52, 11.06s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 89%|████████▉ | 166/186 [29:40<03:38, 10.94s/it] 90%|████████▉ | 167/186 [29:51<03:27, 10.93s/it] 90%|█████████ | 168/186 [30:02<03:15, 10.88s/it] 91%|█████████ | 169/186 [30:13<03:04, 10.87s/it] 91%|█████████▏| 170/186 [30:24<02:53, 10.87s/it] 92%|█████████▏| 171/186 [30:35<02:44, 10.97s/it] 92%|█████████▏| 172/186 [30:46<02:33, 11.00s/it] 93%|█████████▎| 173/186 [30:57<02:21, 10.91s/it] 94%|█████████▎| 174/186 [31:08<02:11, 10.94s/it] 94%|█████████▍| 175/186 [31:19<02:00, 10.94s/it] 95%|█████████▍| 176/186 [31:29<01:48, 10.89s/it] 95%|█████████▌| 177/186 [31:41<01:38, 10.97s/it] 96%|█████████▌| 178/186 [31:52<01:27, 10.97s/it] 96%|█████████▌| 179/186 [32:02<01:16, 10.95s/it] 97%|█████████▋| 180/186 [32:13<01:05, 10.93s/it] 97%|█████████▋| 181/186 [32:24<00:54, 10.89s/it] 98%|█████████▊| 182/186 [32:35<00:43, 10.89s/it] 98%|█████████▊| 183/186 [32:46<00:32, 10.94s/it] 99%|█████████▉| 184/186 [32:57<00:21, 10.89s/it] 99%|█████████▉| 185/186 [33:08<00:10, 10.88s/it]100%|██████████| 186/186 [33:19<00:00, 10.93s/it]                                                 {'train_runtime': 1999.2336, 'train_samples_per_second': 6.72, 'train_steps_per_second': 0.093, 'train_loss': 0.15407549950384325, 'epoch': 1.0}
100%|██████████| 186/186 [33:19<00:00, 10.93s/it]100%|██████████| 186/186 [33:19<00:00, 10.75s/it]
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-0
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33msliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-0[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251016_234105-txm5a9i6/logs[0m
[2025-10-17 00:16:13,388] [INFO] [launch.py:347:main] Process 2177123 exits successfully.
[2025-10-17 00:16:13,389] [INFO] [launch.py:347:main] Process 2177120 exits successfully.
[2025-10-17 00:16:13,389] [INFO] [launch.py:347:main] Process 2177124 exits successfully.
[2025-10-17 00:16:13,389] [INFO] [launch.py:347:main] Process 2177122 exits successfully.
[2025-10-17 00:16:13,389] [INFO] [launch.py:347:main] Process 2177121 exits successfully.
[2025-10-17 00:16:17,394] [INFO] [launch.py:347:main] Process 2177119 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 00:16:20,793] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 00:16:21,347] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-17 00:16:21,347] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12423 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-0 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-0 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 1 --use_anchor False --use_sample_weight True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 00:16:22,828] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 00:16:23,400] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-17 00:16:23,400] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-17 00:16:23,400] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-17 00:16:23,400] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-17 00:16:23,400] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 00:16:27,121] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 00:16:27,435] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 00:16:27,451] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 00:16:27,556] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-17 00:16:27,674] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 00:16:27,826] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 00:16:27,870] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 00:16:27,880] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 00:16:27,987] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 00:16:28,192] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 00:16:28,329] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 00:16:28,722] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 00:16:28,722] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251017_001629-vsbpi5qu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-0
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/vsbpi5qu
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.43s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.53s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.97s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.34s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.40s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.86s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.85s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.72s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.26s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.67s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  4.00s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.61s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.95s/it]
to bfloat16...
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.54s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.86s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.21s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.52s/it]
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.55s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.23s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.51s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.96s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.35s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.72s/it]
Formatting inputs...Skip in lazy mode
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.99s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.37s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.68s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.54s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.55s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.87s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.86s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.49s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.78s/it]
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:0')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:3')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:5')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:4')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:2')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:1')
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.013572931289672852 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.009855270385742188 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.014302730560302734 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10101437568664551 seconds
Loading extension module fused_adam...
Loading extension module fused_adam...
Time to load fused_adam op: 0.10168027877807617 seconds
Time to load fused_adam op: 0.10147380828857422 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/186 [00:09<30:32,  9.90s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1297 > 1048). Running this sequence through the model will result in indexing errors
  1%|          | 2/186 [00:18<28:02,  9.15s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
  2%|▏         | 3/186 [00:27<27:28,  9.01s/it]  2%|▏         | 4/186 [00:36<27:53,  9.20s/it]  3%|▎         | 5/186 [00:46<28:11,  9.34s/it]  3%|▎         | 6/186 [00:56<28:14,  9.42s/it]  4%|▍         | 7/186 [01:05<28:14,  9.46s/it]  4%|▍         | 8/186 [01:15<28:21,  9.56s/it]  5%|▍         | 9/186 [01:24<28:05,  9.52s/it]  5%|▌         | 10/186 [01:34<28:06,  9.58s/it]  6%|▌         | 11/186 [01:44<28:08,  9.65s/it]  6%|▋         | 12/186 [01:54<28:13,  9.73s/it]  7%|▋         | 13/186 [02:04<28:29,  9.88s/it]  8%|▊         | 14/186 [02:14<28:20,  9.88s/it]  8%|▊         | 15/186 [02:24<28:19,  9.94s/it]  9%|▊         | 16/186 [02:34<28:29, 10.06s/it]  9%|▉         | 17/186 [02:45<29:08, 10.35s/it] 10%|▉         | 18/186 [02:56<29:07, 10.40s/it] 10%|█         | 19/186 [03:06<28:54, 10.38s/it] 11%|█         | 20/186 [03:17<28:47, 10.40s/it] 11%|█▏        | 21/186 [03:27<28:40, 10.43s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 12%|█▏        | 22/186 [03:37<28:26, 10.40s/it] 12%|█▏        | 23/186 [03:48<28:20, 10.43s/it] 13%|█▎        | 24/186 [03:59<28:23, 10.52s/it] 13%|█▎        | 25/186 [04:09<28:02, 10.45s/it] 14%|█▍        | 26/186 [04:20<27:59, 10.50s/it] 15%|█▍        | 27/186 [04:30<28:06, 10.61s/it] 15%|█▌        | 28/186 [04:41<27:58, 10.62s/it] 16%|█▌        | 29/186 [04:52<27:59, 10.70s/it] 16%|█▌        | 30/186 [05:03<27:47, 10.69s/it] 17%|█▋        | 31/186 [05:13<27:33, 10.67s/it] 17%|█▋        | 32/186 [05:24<27:30, 10.72s/it] 18%|█▊        | 33/186 [05:35<27:20, 10.72s/it] 18%|█▊        | 34/186 [05:46<27:12, 10.74s/it] 19%|█▉        | 35/186 [05:57<27:15, 10.83s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1089 > 1048). Running this sequence through the model will result in indexing errors
 19%|█▉        | 36/186 [06:08<27:17, 10.92s/it] 20%|█▉        | 37/186 [06:18<26:45, 10.78s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 20%|██        | 38/186 [06:29<26:41, 10.82s/it] 21%|██        | 39/186 [06:40<26:33, 10.84s/it] 22%|██▏       | 40/186 [06:51<26:18, 10.81s/it] 22%|██▏       | 41/186 [07:01<26:05, 10.79s/it] 23%|██▎       | 42/186 [07:12<25:51, 10.78s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1307 > 1048). Running this sequence through the model will result in indexing errors
 23%|██▎       | 43/186 [07:23<25:42, 10.79s/it] 24%|██▎       | 44/186 [07:34<25:23, 10.73s/it] 24%|██▍       | 45/186 [07:44<25:06, 10.68s/it] 25%|██▍       | 46/186 [07:55<24:51, 10.66s/it] 25%|██▌       | 47/186 [08:05<24:43, 10.68s/it] 26%|██▌       | 48/186 [08:16<24:32, 10.67s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1074 > 1048). Running this sequence through the model will result in indexing errors
 26%|██▋       | 49/186 [08:27<24:19, 10.66s/it] 27%|██▋       | 50/186 [08:38<24:17, 10.72s/it] 27%|██▋       | 51/186 [08:48<24:03, 10.69s/it] 28%|██▊       | 52/186 [08:59<23:50, 10.67s/it] 28%|██▊       | 53/186 [09:09<23:36, 10.65s/it] 29%|██▉       | 54/186 [09:20<23:27, 10.66s/it] 30%|██▉       | 55/186 [09:31<23:16, 10.66s/it] 30%|███       | 56/186 [09:42<23:09, 10.69s/it] 31%|███       | 57/186 [09:52<23:04, 10.73s/it] 31%|███       | 58/186 [10:03<22:47, 10.68s/it] 32%|███▏      | 59/186 [10:14<22:48, 10.78s/it] 32%|███▏      | 60/186 [10:25<22:34, 10.75s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1125 > 1048). Running this sequence through the model will result in indexing errors
 33%|███▎      | 61/186 [10:36<22:36, 10.85s/it] 33%|███▎      | 62/186 [10:46<22:10, 10.73s/it] 34%|███▍      | 63/186 [10:57<21:55, 10.70s/it] 34%|███▍      | 64/186 [11:08<21:53, 10.77s/it] 35%|███▍      | 65/186 [11:18<21:37, 10.73s/it] 35%|███▌      | 66/186 [11:29<21:39, 10.83s/it] 36%|███▌      | 67/186 [11:40<21:22, 10.78s/it] 37%|███▋      | 68/186 [11:51<21:02, 10.70s/it] 37%|███▋      | 69/186 [12:02<21:00, 10.78s/it] 38%|███▊      | 70/186 [12:12<20:44, 10.73s/it] 38%|███▊      | 71/186 [12:23<20:46, 10.84s/it] 39%|███▊      | 72/186 [12:34<20:28, 10.78s/it] 39%|███▉      | 73/186 [12:45<20:12, 10.73s/it] 40%|███▉      | 74/186 [12:55<19:57, 10.69s/it] 40%|████      | 75/186 [13:06<19:45, 10.68s/it] 41%|████      | 76/186 [13:17<19:41, 10.74s/it] 41%|████▏     | 77/186 [13:27<19:29, 10.73s/it] 42%|████▏     | 78/186 [13:38<19:16, 10.71s/it] 42%|████▏     | 79/186 [13:49<19:01, 10.67s/it] 43%|████▎     | 80/186 [14:00<19:00, 10.76s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1143 > 1048). Running this sequence through the model will result in indexing errors
 44%|████▎     | 81/186 [14:11<18:54, 10.80s/it] 44%|████▍     | 82/186 [14:21<18:40, 10.77s/it] 45%|████▍     | 83/186 [14:32<18:25, 10.73s/it] 45%|████▌     | 84/186 [14:43<18:14, 10.73s/it] 46%|████▌     | 85/186 [14:53<18:08, 10.77s/it] 46%|████▌     | 86/186 [15:04<17:56, 10.77s/it] 47%|████▋     | 87/186 [15:15<17:44, 10.75s/it] 47%|████▋     | 88/186 [15:26<17:29, 10.71s/it] 48%|████▊     | 89/186 [15:36<17:14, 10.67s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 48%|████▊     | 90/186 [15:47<17:03, 10.66s/it] 49%|████▉     | 91/186 [15:58<16:56, 10.70s/it] 49%|████▉     | 92/186 [16:08<16:47, 10.72s/it] 50%|█████     | 93/186 [16:20<16:50, 10.87s/it] 51%|█████     | 94/186 [16:30<16:34, 10.81s/it] 51%|█████     | 95/186 [16:41<16:22, 10.80s/it] 52%|█████▏    | 96/186 [16:52<16:12, 10.80s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 52%|█████▏    | 97/186 [17:03<16:02, 10.82s/it] 53%|█████▎    | 98/186 [17:13<15:51, 10.81s/it] 53%|█████▎    | 99/186 [17:24<15:41, 10.83s/it] 54%|█████▍    | 100/186 [17:35<15:29, 10.81s/it] 54%|█████▍    | 101/186 [17:46<15:20, 10.83s/it] 55%|█████▍    | 102/186 [17:57<15:04, 10.77s/it] 55%|█████▌    | 103/186 [18:08<14:59, 10.84s/it] 56%|█████▌    | 104/186 [18:18<14:39, 10.73s/it] 56%|█████▋    | 105/186 [18:29<14:35, 10.80s/it] 57%|█████▋    | 106/186 [18:40<14:23, 10.80s/it] 58%|█████▊    | 107/186 [18:50<14:10, 10.77s/it] 58%|█████▊    | 108/186 [19:01<13:58, 10.75s/it] 59%|█████▊    | 109/186 [19:12<13:53, 10.83s/it] 59%|█████▉    | 110/186 [19:23<13:40, 10.79s/it] 60%|█████▉    | 111/186 [19:34<13:29, 10.79s/it] 60%|██████    | 112/186 [19:45<13:20, 10.81s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 61%|██████    | 113/186 [19:55<13:06, 10.78s/it] 61%|██████▏   | 114/186 [20:06<12:55, 10.78s/it] 62%|██████▏   | 115/186 [20:17<12:45, 10.78s/it] 62%|██████▏   | 116/186 [20:28<12:37, 10.82s/it] 63%|██████▎   | 117/186 [20:38<12:23, 10.77s/it] 63%|██████▎   | 118/186 [20:49<12:13, 10.79s/it] 64%|██████▍   | 119/186 [21:00<12:05, 10.83s/it] 65%|██████▍   | 120/186 [21:11<11:48, 10.74s/it] 65%|██████▌   | 121/186 [21:21<11:38, 10.74s/it] 66%|██████▌   | 122/186 [21:32<11:27, 10.75s/it] 66%|██████▌   | 123/186 [21:43<11:21, 10.81s/it] 67%|██████▋   | 124/186 [21:54<11:08, 10.79s/it] 67%|██████▋   | 125/186 [22:05<10:56, 10.77s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 68%|██████▊   | 126/186 [22:16<10:50, 10.84s/it] 68%|██████▊   | 127/186 [22:26<10:38, 10.83s/it] 69%|██████▉   | 128/186 [22:37<10:26, 10.80s/it] 69%|██████▉   | 129/186 [22:48<10:13, 10.77s/it] 70%|██████▉   | 130/186 [22:59<10:02, 10.76s/it] 70%|███████   | 131/186 [23:09<09:48, 10.70s/it] 71%|███████   | 132/186 [23:20<09:46, 10.87s/it] 72%|███████▏  | 133/186 [23:31<09:35, 10.86s/it] 72%|███████▏  | 134/186 [23:42<09:19, 10.77s/it] 73%|███████▎  | 135/186 [23:52<09:07, 10.73s/it] 73%|███████▎  | 136/186 [24:03<08:56, 10.74s/it] 74%|███████▎  | 137/186 [24:14<08:51, 10.85s/it] 74%|███████▍  | 138/186 [24:25<08:39, 10.81s/it] 75%|███████▍  | 139/186 [24:36<08:36, 10.99s/it] 75%|███████▌  | 140/186 [24:47<08:23, 10.94s/it] 76%|███████▌  | 141/186 [24:58<08:08, 10.86s/it] 76%|███████▋  | 142/186 [25:09<07:56, 10.83s/it] 77%|███████▋  | 143/186 [25:20<07:47, 10.87s/it] 77%|███████▋  | 144/186 [25:30<07:35, 10.85s/it] 78%|███████▊  | 145/186 [25:41<07:23, 10.82s/it] 78%|███████▊  | 146/186 [25:52<07:13, 10.83s/it] 79%|███████▉  | 147/186 [26:03<07:01, 10.82s/it] 80%|███████▉  | 148/186 [26:14<06:51, 10.83s/it] 80%|████████  | 149/186 [26:25<06:41, 10.84s/it] 81%|████████  | 150/186 [26:35<06:30, 10.84s/it] 81%|████████  | 151/186 [26:46<06:19, 10.84s/it] 82%|████████▏ | 152/186 [26:57<06:07, 10.81s/it] 82%|████████▏ | 153/186 [27:08<05:58, 10.86s/it] 83%|████████▎ | 154/186 [27:19<05:47, 10.87s/it] 83%|████████▎ | 155/186 [27:30<05:37, 10.88s/it] 84%|████████▍ | 156/186 [27:41<05:26, 10.88s/it] 84%|████████▍ | 157/186 [27:51<05:14, 10.84s/it] 85%|████████▍ | 158/186 [28:02<05:04, 10.88s/it] 85%|████████▌ | 159/186 [28:13<04:54, 10.90s/it] 86%|████████▌ | 160/186 [28:24<04:43, 10.90s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 87%|████████▋ | 161/186 [28:35<04:32, 10.90s/it] 87%|████████▋ | 162/186 [28:46<04:23, 11.00s/it] 88%|████████▊ | 163/186 [28:57<04:11, 10.95s/it] 88%|████████▊ | 164/186 [29:08<04:00, 10.91s/it] 89%|████████▊ | 165/186 [29:19<03:48, 10.87s/it] 89%|████████▉ | 166/186 [29:30<03:38, 10.90s/it] 90%|████████▉ | 167/186 [29:41<03:28, 10.97s/it] 90%|█████████ | 168/186 [29:51<03:15, 10.87s/it] 91%|█████████ | 169/186 [30:02<03:04, 10.84s/it] 91%|█████████▏| 170/186 [30:13<02:54, 10.90s/it] 92%|█████████▏| 171/186 [30:24<02:44, 10.98s/it] 92%|█████████▏| 172/186 [30:35<02:33, 10.96s/it] 93%|█████████▎| 173/186 [30:46<02:22, 10.94s/it] 94%|█████████▎| 174/186 [30:57<02:11, 10.94s/it] 94%|█████████▍| 175/186 [31:08<02:00, 10.99s/it] 95%|█████████▍| 176/186 [31:19<01:50, 11.03s/it] 95%|█████████▌| 177/186 [31:30<01:38, 10.99s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 96%|█████████▌| 178/186 [31:41<01:28, 11.02s/it] 96%|█████████▌| 179/186 [31:53<01:17, 11.04s/it] 97%|█████████▋| 180/186 [32:03<01:06, 11.02s/it] 97%|█████████▋| 181/186 [32:14<00:54, 11.00s/it] 98%|█████████▊| 182/186 [32:25<00:43, 10.97s/it]WARNING: tokenization mismatch: 1 vs. 48. (ignored)
 98%|█████████▊| 183/186 [32:36<00:32, 10.98s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 99%|█████████▉| 184/186 [32:47<00:22, 11.02s/it] 99%|█████████▉| 185/186 [32:58<00:11, 11.03s/it]100%|██████████| 186/186 [33:09<00:00, 10.97s/it]                                                 {'train_runtime': 1989.8106, 'train_samples_per_second': 6.752, 'train_steps_per_second': 0.093, 'train_loss': 0.16083258967245778, 'epoch': 1.0}
100%|██████████| 186/186 [33:09<00:00, 10.97s/it]100%|██████████| 186/186 [33:09<00:00, 10.70s/it]
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-0
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33msliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-0[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251017_001629-vsbpi5qu/logs[0m
[2025-10-17 00:51:26,925] [INFO] [launch.py:347:main] Process 2180694 exits successfully.
[2025-10-17 00:51:26,925] [INFO] [launch.py:347:main] Process 2180691 exits successfully.
[2025-10-17 00:51:26,926] [INFO] [launch.py:347:main] Process 2180695 exits successfully.
[2025-10-17 00:51:26,926] [INFO] [launch.py:347:main] Process 2180692 exits successfully.
[2025-10-17 00:51:27,927] [INFO] [launch.py:347:main] Process 2180693 exits successfully.
[2025-10-17 00:51:31,932] [INFO] [launch.py:347:main] Process 2180690 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 00:51:35,121] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 00:51:35,696] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-17 00:51:35,696] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12423 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-0 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-0 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 2 --use_anchor False --use_sample_weight True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 00:51:37,356] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 00:51:37,946] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-17 00:51:37,946] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-17 00:51:37,946] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-17 00:51:37,946] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-17 00:51:37,946] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 00:51:41,620] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 00:51:41,947] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 00:51:42,146] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-17 00:51:42,361] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 00:51:42,467] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-17 00:51:42,736] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 00:51:42,832] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-17 00:51:43,029] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 00:51:43,065] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 00:51:43,346] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 00:51:43,346] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-10-17 00:51:43,716] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 00:51:44,171] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: creating run
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251017_005144-yez7q2jm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-0
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/yez7q2jm
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.64s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.59s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.73s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.79s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.02s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.99s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.77s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.79s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.75s/it]to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.54s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.92s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.45s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.83s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.51s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.85s/it]
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.36s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.72s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.65s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.50s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.72s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  4.00s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.75s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.73s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.66s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.98s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.67s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.98s/it]
Formatting inputs...Skip in lazy mode
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.74s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.06s/it]
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:4')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:1')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:3')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:0')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:2')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:5')
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...

Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.014723062515258789 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.010710954666137695 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10151958465576172 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.010961055755615234 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10152935981750488 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10213017463684082 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/186 [00:11<35:48, 11.61s/it]  1%|          | 2/186 [00:20<30:47, 10.04s/it]  2%|▏         | 3/186 [00:30<29:58,  9.83s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1143 > 1048). Running this sequence through the model will result in indexing errors
  2%|▏         | 4/186 [00:40<29:52,  9.85s/it]  3%|▎         | 5/186 [00:50<29:55,  9.92s/it]  3%|▎         | 6/186 [01:00<29:48,  9.94s/it]  4%|▍         | 7/186 [01:10<29:48,  9.99s/it]  4%|▍         | 8/186 [01:20<29:41, 10.01s/it]  5%|▍         | 9/186 [01:30<29:45, 10.09s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
  5%|▌         | 10/186 [01:40<29:42, 10.13s/it]  6%|▌         | 11/186 [01:50<29:41, 10.18s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1073 > 1048). Running this sequence through the model will result in indexing errors
  6%|▋         | 12/186 [02:01<29:48, 10.28s/it]  7%|▋         | 13/186 [02:11<29:50, 10.35s/it]  8%|▊         | 14/186 [02:22<29:45, 10.38s/it]  8%|▊         | 15/186 [02:32<29:33, 10.37s/it]  9%|▊         | 16/186 [02:43<29:17, 10.34s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1125 > 1048). Running this sequence through the model will result in indexing errors
WARNING: tokenization mismatch: 1 vs. 47. (ignored)
  9%|▉         | 17/186 [02:53<29:23, 10.43s/it] 10%|▉         | 18/186 [03:03<29:05, 10.39s/it] 10%|█         | 19/186 [03:14<28:54, 10.39s/it] 11%|█         | 20/186 [03:24<28:49, 10.42s/it] 11%|█▏        | 21/186 [03:35<28:41, 10.43s/it] 12%|█▏        | 22/186 [03:45<28:32, 10.44s/it] 12%|█▏        | 23/186 [03:56<28:25, 10.46s/it] 13%|█▎        | 24/186 [04:07<28:37, 10.60s/it] 13%|█▎        | 25/186 [04:17<28:19, 10.55s/it] 14%|█▍        | 26/186 [04:28<28:12, 10.58s/it] 15%|█▍        | 27/186 [04:38<27:58, 10.56s/it] 15%|█▌        | 28/186 [04:49<27:46, 10.55s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1072 > 1048). Running this sequence through the model will result in indexing errors
 16%|█▌        | 29/186 [05:00<27:49, 10.64s/it] 16%|█▌        | 30/186 [05:10<27:35, 10.61s/it] 17%|█▋        | 31/186 [05:21<27:16, 10.56s/it] 17%|█▋        | 32/186 [05:31<27:01, 10.53s/it] 18%|█▊        | 33/186 [05:41<26:37, 10.44s/it] 18%|█▊        | 34/186 [05:52<26:36, 10.50s/it] 19%|█▉        | 35/186 [06:03<26:33, 10.55s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 19%|█▉        | 36/186 [06:13<26:15, 10.50s/it] 20%|█▉        | 37/186 [06:23<25:56, 10.45s/it] 20%|██        | 38/186 [06:34<26:01, 10.55s/it] 21%|██        | 39/186 [06:45<25:48, 10.53s/it] 22%|██▏       | 40/186 [06:55<25:36, 10.52s/it] 22%|██▏       | 41/186 [07:06<25:31, 10.56s/it] 23%|██▎       | 42/186 [07:16<25:22, 10.57s/it] 23%|██▎       | 43/186 [07:27<25:11, 10.57s/it] 24%|██▎       | 44/186 [07:38<25:03, 10.59s/it] 24%|██▍       | 45/186 [07:48<24:46, 10.55s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1072 > 1048). Running this sequence through the model will result in indexing errors
 25%|██▍       | 46/186 [07:59<25:03, 10.74s/it] 25%|██▌       | 47/186 [08:10<24:43, 10.68s/it] 26%|██▌       | 48/186 [08:20<24:27, 10.64s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 26%|██▋       | 49/186 [08:31<24:13, 10.61s/it]WARNING: tokenization mismatch: 1 vs. 48. (ignored)
 27%|██▋       | 50/186 [08:41<24:02, 10.61s/it] 27%|██▋       | 51/186 [08:52<23:46, 10.57s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
 28%|██▊       | 52/186 [09:03<23:43, 10.62s/it] 28%|██▊       | 53/186 [09:13<23:29, 10.60s/it] 29%|██▉       | 54/186 [09:24<23:21, 10.62s/it] 30%|██▉       | 55/186 [09:35<23:11, 10.63s/it] 30%|███       | 56/186 [09:45<23:07, 10.67s/it] 31%|███       | 57/186 [09:56<22:48, 10.61s/it] 31%|███       | 58/186 [10:06<22:34, 10.58s/it]WARNING: tokenization mismatch: 1 vs. 47. (ignored)
 32%|███▏      | 59/186 [10:17<22:29, 10.63s/it] 32%|███▏      | 60/186 [10:28<22:21, 10.65s/it] 33%|███▎      | 61/186 [10:38<22:02, 10.58s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 33%|███▎      | 62/186 [10:49<21:49, 10.56s/it] 34%|███▍      | 63/186 [10:59<21:46, 10.62s/it] 34%|███▍      | 64/186 [11:10<21:31, 10.58s/it] 35%|███▍      | 65/186 [11:20<21:18, 10.57s/it] 35%|███▌      | 66/186 [11:31<21:00, 10.50s/it] 36%|███▌      | 67/186 [11:42<20:58, 10.57s/it] 37%|███▋      | 68/186 [11:52<20:48, 10.58s/it] 37%|███▋      | 69/186 [12:03<20:47, 10.66s/it] 38%|███▊      | 70/186 [12:14<20:41, 10.70s/it] 38%|███▊      | 71/186 [12:24<20:20, 10.62s/it] 39%|███▊      | 72/186 [12:35<20:14, 10.66s/it] 39%|███▉      | 73/186 [12:45<19:55, 10.58s/it] 40%|███▉      | 74/186 [12:56<19:44, 10.57s/it] 40%|████      | 75/186 [13:06<19:30, 10.55s/it] 41%|████      | 76/186 [13:17<19:20, 10.55s/it] 41%|████▏     | 77/186 [13:28<19:11, 10.56s/it] 42%|████▏     | 78/186 [13:38<19:02, 10.58s/it] 42%|████▏     | 79/186 [13:49<18:50, 10.57s/it] 43%|████▎     | 80/186 [13:59<18:39, 10.56s/it] 44%|████▎     | 81/186 [14:10<18:29, 10.57s/it] 44%|████▍     | 82/186 [14:21<18:24, 10.62s/it] 45%|████▍     | 83/186 [14:31<18:13, 10.61s/it] 45%|████▌     | 84/186 [14:42<18:01, 10.60s/it] 46%|████▌     | 85/186 [14:53<17:55, 10.65s/it] 46%|████▌     | 86/186 [15:03<17:40, 10.60s/it] 47%|████▋     | 87/186 [15:14<17:26, 10.57s/it] 47%|████▋     | 88/186 [15:24<17:14, 10.55s/it] 48%|████▊     | 89/186 [15:35<17:08, 10.60s/it] 48%|████▊     | 90/186 [15:45<16:51, 10.54s/it] 49%|████▉     | 91/186 [15:56<16:55, 10.69s/it] 49%|████▉     | 92/186 [16:07<16:44, 10.69s/it] 50%|█████     | 93/186 [16:18<16:35, 10.71s/it] 51%|█████     | 94/186 [16:28<16:21, 10.67s/it] 51%|█████     | 95/186 [16:39<16:05, 10.61s/it] 52%|█████▏    | 96/186 [16:49<15:56, 10.62s/it] 52%|█████▏    | 97/186 [17:00<15:46, 10.63s/it] 53%|█████▎    | 98/186 [17:11<15:35, 10.63s/it] 53%|█████▎    | 99/186 [17:21<15:25, 10.63s/it] 54%|█████▍    | 100/186 [17:32<15:14, 10.64s/it] 54%|█████▍    | 101/186 [17:43<15:04, 10.64s/it] 55%|█████▍    | 102/186 [17:53<14:57, 10.69s/it] 55%|█████▌    | 103/186 [18:04<14:49, 10.72s/it] 56%|█████▌    | 104/186 [18:15<14:36, 10.69s/it] 56%|█████▋    | 105/186 [18:25<14:23, 10.66s/it] 57%|█████▋    | 106/186 [18:36<14:12, 10.66s/it] 58%|█████▊    | 107/186 [18:47<14:02, 10.67s/it] 58%|█████▊    | 108/186 [18:58<13:56, 10.72s/it] 59%|█████▊    | 109/186 [19:08<13:45, 10.72s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 59%|█████▉    | 110/186 [19:19<13:31, 10.67s/it] 60%|█████▉    | 111/186 [19:30<13:20, 10.68s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 60%|██████    | 112/186 [19:40<13:07, 10.65s/it] 61%|██████    | 113/186 [19:51<12:57, 10.65s/it] 61%|██████▏   | 114/186 [20:01<12:49, 10.68s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 62%|██████▏   | 115/186 [20:12<12:37, 10.67s/it] 62%|██████▏   | 116/186 [20:23<12:35, 10.80s/it] 63%|██████▎   | 117/186 [20:34<12:27, 10.83s/it] 63%|██████▎   | 118/186 [20:45<12:15, 10.82s/it] 64%|██████▍   | 119/186 [20:56<12:01, 10.76s/it] 65%|██████▍   | 120/186 [21:06<11:49, 10.75s/it] 65%|██████▌   | 121/186 [21:17<11:41, 10.79s/it] 66%|██████▌   | 122/186 [21:28<11:33, 10.84s/it] 66%|██████▌   | 123/186 [21:39<11:19, 10.79s/it] 67%|██████▋   | 124/186 [21:50<11:13, 10.87s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1143 > 1048). Running this sequence through the model will result in indexing errors
 67%|██████▋   | 125/186 [22:01<11:03, 10.87s/it] 68%|██████▊   | 126/186 [22:12<10:57, 10.96s/it] 68%|██████▊   | 127/186 [22:23<10:42, 10.89s/it] 69%|██████▉   | 128/186 [22:33<10:28, 10.84s/it] 69%|██████▉   | 129/186 [22:44<10:16, 10.82s/it] 70%|██████▉   | 130/186 [22:55<10:03, 10.78s/it] 70%|███████   | 131/186 [23:06<09:53, 10.79s/it] 71%|███████   | 132/186 [23:16<09:43, 10.80s/it] 72%|███████▏  | 133/186 [23:27<09:31, 10.79s/it] 72%|███████▏  | 134/186 [23:38<09:20, 10.78s/it] 73%|███████▎  | 135/186 [23:49<09:08, 10.75s/it] 73%|███████▎  | 136/186 [23:59<08:58, 10.77s/it] 74%|███████▎  | 137/186 [24:11<08:52, 10.87s/it] 74%|███████▍  | 138/186 [24:21<08:41, 10.87s/it] 75%|███████▍  | 139/186 [24:32<08:32, 10.91s/it] 75%|███████▌  | 140/186 [24:43<08:17, 10.81s/it] 76%|███████▌  | 141/186 [24:54<08:04, 10.78s/it] 76%|███████▋  | 142/186 [25:05<07:56, 10.84s/it] 77%|███████▋  | 143/186 [25:15<07:44, 10.80s/it] 77%|███████▋  | 144/186 [25:26<07:33, 10.80s/it] 78%|███████▊  | 145/186 [25:37<07:25, 10.87s/it] 78%|███████▊  | 146/186 [25:48<07:12, 10.82s/it] 79%|███████▉  | 147/186 [25:59<07:05, 10.91s/it] 80%|███████▉  | 148/186 [26:10<06:54, 10.92s/it] 80%|████████  | 149/186 [26:21<06:44, 10.92s/it] 81%|████████  | 150/186 [26:32<06:31, 10.88s/it] 81%|████████  | 151/186 [26:43<06:21, 10.90s/it] 82%|████████▏ | 152/186 [26:53<06:09, 10.86s/it] 82%|████████▏ | 153/186 [27:04<05:57, 10.84s/it] 83%|████████▎ | 154/186 [27:15<05:46, 10.84s/it] 83%|████████▎ | 155/186 [27:26<05:37, 10.89s/it] 84%|████████▍ | 156/186 [27:37<05:28, 10.96s/it] 84%|████████▍ | 157/186 [27:48<05:17, 10.96s/it] 85%|████████▍ | 158/186 [27:59<05:06, 10.93s/it] 85%|████████▌ | 159/186 [28:10<04:54, 10.89s/it] 86%|████████▌ | 160/186 [28:21<04:45, 10.99s/it] 87%|████████▋ | 161/186 [28:32<04:34, 10.96s/it] 87%|████████▋ | 162/186 [28:43<04:24, 11.02s/it] 88%|████████▊ | 163/186 [28:54<04:14, 11.07s/it] 88%|████████▊ | 164/186 [29:05<04:03, 11.05s/it] 89%|████████▊ | 165/186 [29:16<03:49, 10.94s/it] 89%|████████▉ | 166/186 [29:27<03:38, 10.94s/it] 90%|████████▉ | 167/186 [29:38<03:27, 10.91s/it] 90%|█████████ | 168/186 [29:48<03:14, 10.83s/it] 91%|█████████ | 169/186 [29:59<03:03, 10.81s/it] 91%|█████████▏| 170/186 [30:10<02:53, 10.84s/it] 92%|█████████▏| 171/186 [30:21<02:44, 10.94s/it] 92%|█████████▏| 172/186 [30:32<02:32, 10.90s/it] 93%|█████████▎| 173/186 [30:43<02:21, 10.91s/it] 94%|█████████▎| 174/186 [30:54<02:10, 10.91s/it] 94%|█████████▍| 175/186 [31:05<01:59, 10.89s/it] 95%|█████████▍| 176/186 [31:15<01:48, 10.85s/it] 95%|█████████▌| 177/186 [31:26<01:37, 10.82s/it] 96%|█████████▌| 178/186 [31:37<01:26, 10.80s/it] 96%|█████████▌| 179/186 [31:48<01:16, 10.91s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 97%|█████████▋| 180/186 [31:59<01:05, 10.89s/it] 97%|█████████▋| 181/186 [32:10<00:54, 10.92s/it] 98%|█████████▊| 182/186 [32:21<00:43, 10.94s/it] 98%|█████████▊| 183/186 [32:32<00:32, 10.92s/it] 99%|█████████▉| 184/186 [32:43<00:21, 10.93s/it] 99%|█████████▉| 185/186 [32:54<00:11, 11.03s/it]100%|██████████| 186/186 [33:05<00:00, 11.07s/it]                                                 {'train_runtime': 1985.7211, 'train_samples_per_second': 6.766, 'train_steps_per_second': 0.094, 'train_loss': 0.16393211323727844, 'epoch': 1.0}
100%|██████████| 186/186 [33:05<00:00, 11.07s/it]100%|██████████| 186/186 [33:05<00:00, 10.68s/it]
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-0
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33msliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-0[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251017_005144-yez7q2jm/logs[0m
[2025-10-17 01:26:33,443] [INFO] [launch.py:347:main] Process 2183777 exits successfully.
[2025-10-17 01:26:33,443] [INFO] [launch.py:347:main] Process 2183775 exits successfully.
[2025-10-17 01:26:34,445] [INFO] [launch.py:347:main] Process 2183776 exits successfully.
[2025-10-17 01:26:34,445] [INFO] [launch.py:347:main] Process 2183773 exits successfully.
[2025-10-17 01:26:34,445] [INFO] [launch.py:347:main] Process 2183774 exits successfully.
[2025-10-17 01:26:38,450] [INFO] [launch.py:347:main] Process 2183772 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 01:26:41,804] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 01:26:42,359] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-17 01:26:42,360] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12423 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1.5e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-0 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-0 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 0.5 --use_anchor False --use_sample_weight True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 01:26:44,064] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 01:26:44,659] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-17 01:26:44,659] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-17 01:26:44,659] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-17 01:26:44,659] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-17 01:26:44,659] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 01:26:48,265] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 01:26:48,326] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 01:26:48,591] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 01:26:48,644] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 01:26:48,813] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 01:26:48,862] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 01:26:49,171] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 01:26:49,292] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 01:26:49,292] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-10-17 01:26:49,477] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 01:26:49,785] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-10-17 01:26:49,983] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 01:26:50,300] [INFO] [comm.py:637:init_distributed] cdb=None
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251017_012650-d8aoncng
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-0
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/d8aoncng
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.23s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.37s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.49s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.91s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.26s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.90s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.27s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.99s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.36s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.24s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.36s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.61s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.69s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.93s/it]
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.76s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.26s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.57s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.90s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.97s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.21s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.76s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.09s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.37s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.02s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.96s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.67s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.78s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.12s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.73s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.07s/it]
Formatting inputs...Skip in lazy mode
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:4')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:5')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:3')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:2')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:0')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:1')
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.008980512619018555 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.011124849319458008 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.01166534423828125 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.012845516204833984 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10171890258789062 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10237693786621094 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/186 [00:10<31:14, 10.13s/it]  1%|          | 2/186 [00:19<29:11,  9.52s/it]  2%|▏         | 3/186 [00:28<29:08,  9.56s/it]  2%|▏         | 4/186 [00:38<29:18,  9.66s/it]  3%|▎         | 5/186 [00:48<29:23,  9.74s/it]  3%|▎         | 6/186 [00:58<29:30,  9.84s/it]  4%|▍         | 7/186 [01:08<29:53, 10.02s/it]  4%|▍         | 8/186 [01:19<29:50, 10.06s/it]  5%|▍         | 9/186 [01:29<29:50, 10.11s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1069 > 1048). Running this sequence through the model will result in indexing errors
  5%|▌         | 10/186 [01:39<29:58, 10.22s/it]  6%|▌         | 11/186 [01:50<30:03, 10.31s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1143 > 1048). Running this sequence through the model will result in indexing errors
  6%|▋         | 12/186 [02:00<29:57, 10.33s/it]  7%|▋         | 13/186 [02:10<29:46, 10.33s/it]  8%|▊         | 14/186 [02:21<29:36, 10.33s/it]  8%|▊         | 15/186 [02:31<29:27, 10.34s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
  9%|▊         | 16/186 [02:41<29:15, 10.32s/it]  9%|▉         | 17/186 [02:52<29:00, 10.30s/it] 10%|▉         | 18/186 [03:02<28:54, 10.32s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
Token indices sequence length is longer than the specified maximum sequence length for this model (1075 > 1048). Running this sequence through the model will result in indexing errors
 10%|█         | 19/186 [03:13<28:56, 10.40s/it] 11%|█         | 20/186 [03:23<28:49, 10.42s/it] 11%|█▏        | 21/186 [03:34<28:41, 10.43s/it] 12%|█▏        | 22/186 [03:44<28:40, 10.49s/it] 12%|█▏        | 23/186 [03:55<28:39, 10.55s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1285 > 1048). Running this sequence through the model will result in indexing errors
 13%|█▎        | 24/186 [04:06<28:41, 10.62s/it] 13%|█▎        | 25/186 [04:16<28:26, 10.60s/it] 14%|█▍        | 26/186 [04:27<28:03, 10.52s/it] 15%|█▍        | 27/186 [04:37<27:53, 10.53s/it] 15%|█▌        | 28/186 [04:48<27:47, 10.55s/it] 16%|█▌        | 29/186 [04:58<27:31, 10.52s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1125 > 1048). Running this sequence through the model will result in indexing errors
 16%|█▌        | 30/186 [05:09<27:31, 10.59s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
Token indices sequence length is longer than the specified maximum sequence length for this model (1102 > 1048). Running this sequence through the model will result in indexing errors
 17%|█▋        | 31/186 [05:20<27:36, 10.69s/it] 17%|█▋        | 32/186 [05:30<27:15, 10.62s/it] 18%|█▊        | 33/186 [05:41<27:10, 10.66s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
 18%|█▊        | 34/186 [05:51<26:46, 10.57s/it] 19%|█▉        | 35/186 [06:02<26:47, 10.64s/it]WARNING: tokenization mismatch: 1 vs. 47. (ignored)
 19%|█▉        | 36/186 [06:13<26:33, 10.62s/it] 20%|█▉        | 37/186 [06:23<26:22, 10.62s/it] 20%|██        | 38/186 [06:34<26:06, 10.58s/it] 21%|██        | 39/186 [06:45<25:55, 10.58s/it] 22%|██▏       | 40/186 [06:55<25:52, 10.63s/it] 22%|██▏       | 41/186 [07:06<25:38, 10.61s/it] 23%|██▎       | 42/186 [07:16<25:21, 10.57s/it] 23%|██▎       | 43/186 [07:27<25:16, 10.61s/it] 24%|██▎       | 44/186 [07:38<25:16, 10.68s/it] 24%|██▍       | 45/186 [07:48<24:59, 10.64s/it] 25%|██▍       | 46/186 [07:59<24:43, 10.60s/it] 25%|██▌       | 47/186 [08:10<24:42, 10.67s/it] 26%|██▌       | 48/186 [08:20<24:27, 10.64s/it] 26%|██▋       | 49/186 [08:31<24:17, 10.64s/it] 27%|██▋       | 50/186 [08:41<24:02, 10.61s/it] 27%|██▋       | 51/186 [08:52<23:51, 10.61s/it] 28%|██▊       | 52/186 [09:03<23:41, 10.61s/it] 28%|██▊       | 53/186 [09:14<23:41, 10.68s/it] 29%|██▉       | 54/186 [09:24<23:38, 10.75s/it] 30%|██▉       | 55/186 [09:35<23:22, 10.71s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 30%|███       | 56/186 [09:46<23:12, 10.72s/it] 31%|███       | 57/186 [09:57<23:12, 10.80s/it] 31%|███       | 58/186 [10:08<23:04, 10.82s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 32%|███▏      | 59/186 [10:18<22:50, 10.79s/it] 32%|███▏      | 60/186 [10:29<22:34, 10.75s/it] 33%|███▎      | 61/186 [10:40<22:23, 10.75s/it] 33%|███▎      | 62/186 [10:50<22:07, 10.71s/it] 34%|███▍      | 63/186 [11:01<21:53, 10.68s/it] 34%|███▍      | 64/186 [11:12<21:41, 10.67s/it] 35%|███▍      | 65/186 [11:22<21:29, 10.66s/it] 35%|███▌      | 66/186 [11:33<21:28, 10.73s/it] 36%|███▌      | 67/186 [11:44<21:20, 10.76s/it] 37%|███▋      | 68/186 [11:55<21:04, 10.72s/it] 37%|███▋      | 69/186 [12:06<21:11, 10.87s/it] 38%|███▊      | 70/186 [12:17<21:07, 10.93s/it] 38%|███▊      | 71/186 [12:28<20:57, 10.93s/it] 39%|███▊      | 72/186 [12:39<20:39, 10.87s/it] 39%|███▉      | 73/186 [12:49<20:26, 10.85s/it] 40%|███▉      | 74/186 [13:00<20:18, 10.88s/it] 40%|████      | 75/186 [13:11<20:02, 10.83s/it] 41%|████      | 76/186 [13:22<19:46, 10.78s/it] 41%|████▏     | 77/186 [13:32<19:35, 10.78s/it] 42%|████▏     | 78/186 [13:43<19:27, 10.81s/it] 42%|████▏     | 79/186 [13:54<19:22, 10.86s/it] 43%|████▎     | 80/186 [14:05<19:12, 10.87s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 44%|████▎     | 81/186 [14:16<19:01, 10.87s/it] 44%|████▍     | 82/186 [14:27<18:49, 10.86s/it] 45%|████▍     | 83/186 [14:38<18:38, 10.86s/it] 45%|████▌     | 84/186 [14:49<18:23, 10.82s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 46%|████▌     | 85/186 [15:00<18:19, 10.88s/it] 46%|████▌     | 86/186 [15:10<18:04, 10.84s/it] 47%|████▋     | 87/186 [15:21<17:53, 10.85s/it] 47%|████▋     | 88/186 [15:32<17:45, 10.87s/it] 48%|████▊     | 89/186 [15:43<17:32, 10.85s/it] 48%|████▊     | 90/186 [15:54<17:21, 10.85s/it] 49%|████▉     | 91/186 [16:05<17:23, 10.99s/it] 49%|████▉     | 92/186 [16:16<17:17, 11.03s/it] 50%|█████     | 93/186 [16:27<17:08, 11.06s/it] 51%|█████     | 94/186 [16:38<17:00, 11.09s/it] 51%|█████     | 95/186 [16:50<16:51, 11.12s/it] 52%|█████▏    | 96/186 [17:01<16:39, 11.11s/it] 52%|█████▏    | 97/186 [17:12<16:25, 11.07s/it] 53%|█████▎    | 98/186 [17:23<16:15, 11.09s/it] 53%|█████▎    | 99/186 [17:34<15:59, 11.03s/it] 54%|█████▍    | 100/186 [17:45<15:48, 11.03s/it] 54%|█████▍    | 101/186 [17:56<15:30, 10.95s/it] 55%|█████▍    | 102/186 [18:06<15:15, 10.89s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 55%|█████▌    | 103/186 [18:18<15:16, 11.04s/it] 56%|█████▌    | 104/186 [18:29<15:13, 11.14s/it] 56%|█████▋    | 105/186 [18:40<14:58, 11.09s/it] 57%|█████▋    | 106/186 [18:51<14:46, 11.08s/it] 58%|█████▊    | 107/186 [19:02<14:35, 11.08s/it] 58%|█████▊    | 108/186 [19:13<14:21, 11.05s/it] 59%|█████▊    | 109/186 [19:24<14:10, 11.05s/it] 59%|█████▉    | 110/186 [19:35<14:02, 11.09s/it] 60%|█████▉    | 111/186 [19:46<13:51, 11.08s/it] 60%|██████    | 112/186 [19:58<13:42, 11.11s/it] 61%|██████    | 113/186 [20:09<13:31, 11.12s/it] 61%|██████▏   | 114/186 [20:20<13:21, 11.13s/it] 62%|██████▏   | 115/186 [20:31<13:14, 11.19s/it] 62%|██████▏   | 116/186 [20:42<13:04, 11.21s/it] 63%|██████▎   | 117/186 [20:54<12:56, 11.25s/it] 63%|██████▎   | 118/186 [21:05<12:43, 11.22s/it] 64%|██████▍   | 119/186 [21:16<12:25, 11.12s/it] 65%|██████▍   | 120/186 [21:27<12:08, 11.04s/it] 65%|██████▌   | 121/186 [21:38<11:58, 11.05s/it] 66%|██████▌   | 122/186 [21:49<11:47, 11.05s/it] 66%|██████▌   | 123/186 [22:00<11:38, 11.09s/it] 67%|██████▋   | 124/186 [22:11<11:30, 11.14s/it] 67%|██████▋   | 125/186 [22:22<11:17, 11.11s/it] 68%|██████▊   | 126/186 [22:33<11:06, 11.11s/it] 68%|██████▊   | 127/186 [22:44<10:52, 11.06s/it] 69%|██████▉   | 128/186 [22:55<10:39, 11.02s/it] 69%|██████▉   | 129/186 [23:06<10:30, 11.06s/it] 70%|██████▉   | 130/186 [23:18<10:20, 11.08s/it] 70%|███████   | 131/186 [23:29<10:11, 11.12s/it] 71%|███████   | 132/186 [23:40<10:03, 11.18s/it] 72%|███████▏  | 133/186 [23:51<09:49, 11.12s/it] 72%|███████▏  | 134/186 [24:02<09:38, 11.13s/it] 73%|███████▎  | 135/186 [24:13<09:28, 11.15s/it] 73%|███████▎  | 136/186 [24:25<09:15, 11.11s/it] 74%|███████▎  | 137/186 [24:36<09:07, 11.18s/it] 74%|███████▍  | 138/186 [24:47<08:56, 11.18s/it] 75%|███████▍  | 139/186 [24:58<08:45, 11.18s/it] 75%|███████▌  | 140/186 [25:09<08:34, 11.19s/it] 76%|███████▌  | 141/186 [25:21<08:22, 11.18s/it] 76%|███████▋  | 142/186 [25:32<08:09, 11.13s/it] 77%|███████▋  | 143/186 [25:43<07:59, 11.15s/it] 77%|███████▋  | 144/186 [25:54<07:47, 11.13s/it] 78%|███████▊  | 145/186 [26:05<07:35, 11.11s/it] 78%|███████▊  | 146/186 [26:16<07:21, 11.03s/it] 79%|███████▉  | 147/186 [26:27<07:08, 10.98s/it] 80%|███████▉  | 148/186 [26:37<06:56, 10.95s/it] 80%|████████  | 149/186 [26:48<06:45, 10.95s/it] 81%|████████  | 150/186 [26:59<06:34, 10.96s/it] 81%|████████  | 151/186 [27:11<06:25, 11.02s/it] 82%|████████▏ | 152/186 [27:22<06:13, 10.99s/it] 82%|████████▏ | 153/186 [27:32<06:01, 10.96s/it] 83%|████████▎ | 154/186 [27:43<05:50, 10.95s/it] 83%|████████▎ | 155/186 [27:54<05:39, 10.94s/it] 84%|████████▍ | 156/186 [28:06<05:32, 11.09s/it] 84%|████████▍ | 157/186 [28:17<05:21, 11.07s/it] 85%|████████▍ | 158/186 [28:28<05:09, 11.04s/it]WARNING: tokenization mismatch: 1 vs. 48. (ignored)
 85%|████████▌ | 159/186 [28:38<04:55, 10.95s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 86%|████████▌ | 160/186 [28:49<04:43, 10.92s/it] 87%|████████▋ | 161/186 [29:00<04:33, 10.95s/it] 87%|████████▋ | 162/186 [29:12<04:27, 11.16s/it] 88%|████████▊ | 163/186 [29:23<04:16, 11.14s/it] 88%|████████▊ | 164/186 [29:34<04:04, 11.09s/it] 89%|████████▊ | 165/186 [29:45<03:50, 10.99s/it] 89%|████████▉ | 166/186 [29:56<03:41, 11.07s/it] 90%|████████▉ | 167/186 [30:07<03:29, 11.01s/it] 90%|█████████ | 168/186 [30:18<03:17, 10.98s/it] 91%|█████████ | 169/186 [30:29<03:06, 10.96s/it] 91%|█████████▏| 170/186 [30:40<02:55, 10.96s/it] 92%|█████████▏| 171/186 [30:51<02:45, 11.02s/it] 92%|█████████▏| 172/186 [31:02<02:34, 11.03s/it] 93%|█████████▎| 173/186 [31:13<02:23, 11.05s/it] 94%|█████████▎| 174/186 [31:24<02:12, 11.07s/it] 94%|█████████▍| 175/186 [31:35<02:01, 11.01s/it] 95%|█████████▍| 176/186 [31:46<01:49, 10.97s/it] 95%|█████████▌| 177/186 [31:57<01:38, 10.97s/it] 96%|█████████▌| 178/186 [32:08<01:28, 11.02s/it] 96%|█████████▌| 179/186 [32:19<01:17, 11.01s/it] 97%|█████████▋| 180/186 [32:30<01:06, 11.13s/it] 97%|█████████▋| 181/186 [32:41<00:55, 11.10s/it] 98%|█████████▊| 182/186 [32:52<00:44, 11.01s/it] 98%|█████████▊| 183/186 [33:03<00:33, 11.01s/it] 99%|█████████▉| 184/186 [33:14<00:21, 10.99s/it] 99%|█████████▉| 185/186 [33:26<00:11, 11.23s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
100%|██████████| 186/186 [33:37<00:00, 11.13s/it]                                                 {'train_runtime': 2017.3376, 'train_samples_per_second': 6.66, 'train_steps_per_second': 0.092, 'train_loss': 0.11715244990523144, 'epoch': 1.0}
100%|██████████| 186/186 [33:37<00:00, 11.13s/it]100%|██████████| 186/186 [33:37<00:00, 10.85s/it]
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-0
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33msliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-0[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251017_012650-d8aoncng/logs[0m
[2025-10-17 02:02:16,170] [INFO] [launch.py:347:main] Process 2185517 exits successfully.
[2025-10-17 02:02:16,170] [INFO] [launch.py:347:main] Process 2185515 exits successfully.
[2025-10-17 02:02:17,172] [INFO] [launch.py:347:main] Process 2185516 exits successfully.
[2025-10-17 02:02:17,172] [INFO] [launch.py:347:main] Process 2185513 exits successfully.
[2025-10-17 02:02:17,172] [INFO] [launch.py:347:main] Process 2185514 exits successfully.
[2025-10-17 02:02:21,176] [INFO] [launch.py:347:main] Process 2185512 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 02:02:24,392] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 02:02:24,950] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-17 02:02:24,950] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12423 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1.5e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-0 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-0 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 1 --use_anchor False --use_sample_weight True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 02:02:26,652] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 02:02:27,244] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-17 02:02:27,244] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-17 02:02:27,244] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-17 02:02:27,244] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-17 02:02:27,244] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 02:02:31,379] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 02:02:31,398] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 02:02:31,706] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 02:02:31,717] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 02:02:31,972] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 02:02:32,463] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 02:02:32,554] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-17 02:02:32,779] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 02:02:32,873] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 02:02:32,874] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-17 02:02:33,048] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 02:02:33,155] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 02:02:33,387] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
wandb: creating run
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251017_020233-ks3grx36
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-0
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/ks3grx36
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.72s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.75s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.66s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.97s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.50s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.57s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.30s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.76s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.69s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.58s/it]to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.23s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.54s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.86s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.44s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.76s/it]
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.90s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.06s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.12s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.13s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.76s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.58s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.54s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.88s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.50s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.81s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.38s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.67s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.19s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.48s/it]
Formatting inputs...Skip in lazy mode
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:4')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:1')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:2')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:3')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:0')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:5')
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.012167215347290039 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.009999752044677734 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.01201486587524414 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10109829902648926 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10276126861572266 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.1019444465637207 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/186 [00:09<29:31,  9.58s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
  1%|          | 2/186 [00:18<27:43,  9.04s/it]  2%|▏         | 3/186 [00:26<27:09,  8.90s/it]  2%|▏         | 4/186 [00:36<27:28,  9.06s/it]  3%|▎         | 5/186 [00:45<27:18,  9.05s/it]  3%|▎         | 6/186 [00:55<27:50,  9.28s/it]  4%|▍         | 7/186 [01:04<28:12,  9.46s/it]  4%|▍         | 8/186 [01:14<28:29,  9.61s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1233 > 1048). Running this sequence through the model will result in indexing errors
  5%|▍         | 9/186 [01:24<28:50,  9.78s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
  5%|▌         | 10/186 [01:34<28:47,  9.82s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1153 > 1048). Running this sequence through the model will result in indexing errors
  6%|▌         | 11/186 [01:44<28:54,  9.91s/it]  6%|▋         | 12/186 [01:55<28:59, 10.00s/it]  7%|▋         | 13/186 [02:05<28:47,  9.99s/it]  8%|▊         | 14/186 [02:15<28:37,  9.98s/it]  8%|▊         | 15/186 [02:25<28:39, 10.06s/it]  9%|▊         | 16/186 [02:35<28:43, 10.14s/it]  9%|▉         | 17/186 [02:45<28:39, 10.17s/it] 10%|▉         | 18/186 [02:56<28:33, 10.20s/it] 10%|█         | 19/186 [03:06<28:22, 10.19s/it] 11%|█         | 20/186 [03:16<28:29, 10.30s/it] 11%|█▏        | 21/186 [03:27<28:23, 10.32s/it] 12%|█▏        | 22/186 [03:37<28:15, 10.34s/it] 12%|█▏        | 23/186 [03:48<28:10, 10.37s/it] 13%|█▎        | 24/186 [03:58<28:10, 10.44s/it] 13%|█▎        | 25/186 [04:09<28:01, 10.44s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
 14%|█▍        | 26/186 [04:19<27:57, 10.49s/it] 15%|█▍        | 27/186 [04:30<27:58, 10.56s/it] 15%|█▌        | 28/186 [04:40<27:36, 10.48s/it] 16%|█▌        | 29/186 [04:51<27:21, 10.46s/it] 16%|█▌        | 30/186 [05:01<27:19, 10.51s/it] 17%|█▋        | 31/186 [05:12<27:04, 10.48s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1158 > 1048). Running this sequence through the model will result in indexing errors
 17%|█▋        | 32/186 [05:22<27:01, 10.53s/it] 18%|█▊        | 33/186 [05:33<26:39, 10.45s/it] 18%|█▊        | 34/186 [05:43<26:35, 10.50s/it] 19%|█▉        | 35/186 [05:54<26:32, 10.54s/it] 19%|█▉        | 36/186 [06:04<26:18, 10.52s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 20%|█▉        | 37/186 [06:15<26:04, 10.50s/it] 20%|██        | 38/186 [06:26<26:06, 10.59s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1102 > 1048). Running this sequence through the model will result in indexing errors
 21%|██        | 39/186 [06:37<26:10, 10.69s/it] 22%|██▏       | 40/186 [06:47<26:03, 10.71s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1150 > 1048). Running this sequence through the model will result in indexing errors
 22%|██▏       | 41/186 [06:58<25:54, 10.72s/it] 23%|██▎       | 42/186 [07:09<25:36, 10.67s/it] 23%|██▎       | 43/186 [07:19<25:22, 10.65s/it] 24%|██▎       | 44/186 [07:30<25:09, 10.63s/it] 24%|██▍       | 45/186 [07:40<24:59, 10.63s/it] 25%|██▍       | 46/186 [07:51<24:50, 10.65s/it] 25%|██▌       | 47/186 [08:02<24:45, 10.68s/it] 26%|██▌       | 48/186 [08:12<24:29, 10.65s/it] 26%|██▋       | 49/186 [08:23<24:11, 10.59s/it] 27%|██▋       | 50/186 [08:33<24:00, 10.59s/it] 27%|██▋       | 51/186 [08:44<23:51, 10.61s/it] 28%|██▊       | 52/186 [08:55<23:36, 10.57s/it] 28%|██▊       | 53/186 [09:05<23:26, 10.57s/it] 29%|██▉       | 54/186 [09:16<23:12, 10.55s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 30%|██▉       | 55/186 [09:26<23:07, 10.59s/it] 30%|███       | 56/186 [09:37<22:56, 10.59s/it] 31%|███       | 57/186 [09:47<22:41, 10.56s/it] 31%|███       | 58/186 [09:58<22:26, 10.52s/it] 32%|███▏      | 59/186 [10:09<22:28, 10.62s/it] 32%|███▏      | 60/186 [10:19<22:19, 10.63s/it] 33%|███▎      | 61/186 [10:30<22:05, 10.60s/it] 33%|███▎      | 62/186 [10:41<22:03, 10.67s/it] 34%|███▍      | 63/186 [10:51<21:52, 10.67s/it] 34%|███▍      | 64/186 [11:02<21:35, 10.62s/it] 35%|███▍      | 65/186 [11:12<21:20, 10.58s/it] 35%|███▌      | 66/186 [11:23<21:24, 10.70s/it] 36%|███▌      | 67/186 [11:34<21:14, 10.71s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 37%|███▋      | 68/186 [11:45<21:00, 10.69s/it] 37%|███▋      | 69/186 [11:56<21:01, 10.78s/it] 38%|███▊      | 70/186 [12:07<20:53, 10.81s/it] 38%|███▊      | 71/186 [12:17<20:39, 10.78s/it] 39%|███▊      | 72/186 [12:28<20:21, 10.71s/it] 39%|███▉      | 73/186 [12:39<20:08, 10.70s/it] 40%|███▉      | 74/186 [12:49<20:00, 10.72s/it] 40%|████      | 75/186 [13:00<19:41, 10.64s/it] 41%|████      | 76/186 [13:10<19:31, 10.65s/it] 41%|████▏     | 77/186 [13:21<19:23, 10.67s/it] 42%|████▏     | 78/186 [13:32<19:13, 10.68s/it] 42%|████▏     | 79/186 [13:43<19:07, 10.73s/it] 43%|████▎     | 80/186 [13:54<19:04, 10.80s/it] 44%|████▎     | 81/186 [14:04<18:51, 10.78s/it] 44%|████▍     | 82/186 [14:15<18:39, 10.77s/it] 45%|████▍     | 83/186 [14:26<18:29, 10.77s/it] 45%|████▌     | 84/186 [14:37<18:13, 10.72s/it] 46%|████▌     | 85/186 [14:47<18:06, 10.76s/it] 46%|████▌     | 86/186 [14:58<17:50, 10.71s/it] 47%|████▋     | 87/186 [15:09<17:44, 10.75s/it] 47%|████▋     | 88/186 [15:19<17:30, 10.72s/it] 48%|████▊     | 89/186 [15:30<17:16, 10.68s/it] 48%|████▊     | 90/186 [15:41<17:03, 10.66s/it] 49%|████▉     | 91/186 [15:51<16:55, 10.69s/it] 49%|████▉     | 92/186 [16:02<16:39, 10.63s/it] 50%|█████     | 93/186 [16:13<16:37, 10.72s/it] 51%|█████     | 94/186 [16:24<16:24, 10.71s/it] 51%|█████     | 95/186 [16:34<16:18, 10.75s/it] 52%|█████▏    | 96/186 [16:45<16:06, 10.74s/it] 52%|█████▏    | 97/186 [16:56<15:55, 10.73s/it] 53%|█████▎    | 98/186 [17:06<15:40, 10.69s/it] 53%|█████▎    | 99/186 [17:17<15:27, 10.66s/it] 54%|█████▍    | 100/186 [17:28<15:17, 10.67s/it] 54%|█████▍    | 101/186 [17:38<15:01, 10.61s/it] 55%|█████▍    | 102/186 [17:49<15:02, 10.74s/it] 55%|█████▌    | 103/186 [18:01<15:08, 10.95s/it] 56%|█████▌    | 104/186 [18:11<14:54, 10.91s/it] 56%|█████▋    | 105/186 [18:22<14:43, 10.91s/it] 57%|█████▋    | 106/186 [18:33<14:25, 10.82s/it] 58%|█████▊    | 107/186 [18:44<14:11, 10.78s/it] 58%|█████▊    | 108/186 [18:54<13:54, 10.69s/it] 59%|█████▊    | 109/186 [19:05<13:48, 10.76s/it] 59%|█████▉    | 110/186 [19:16<13:36, 10.74s/it] 60%|█████▉    | 111/186 [19:26<13:21, 10.69s/it] 60%|██████    | 112/186 [19:37<13:05, 10.61s/it] 61%|██████    | 113/186 [19:47<12:54, 10.62s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 61%|██████▏   | 114/186 [19:58<12:43, 10.60s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 62%|██████▏   | 115/186 [20:09<12:38, 10.69s/it] 62%|██████▏   | 116/186 [20:20<12:36, 10.80s/it] 63%|██████▎   | 117/186 [20:30<12:20, 10.74s/it] 63%|██████▎   | 118/186 [20:41<12:06, 10.69s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1285 > 1048). Running this sequence through the model will result in indexing errors
 64%|██████▍   | 119/186 [20:52<11:58, 10.73s/it] 65%|██████▍   | 120/186 [21:03<11:50, 10.76s/it] 65%|██████▌   | 121/186 [21:13<11:34, 10.69s/it]WARNING: tokenization mismatch: 1 vs. 48. (ignored)
 66%|██████▌   | 122/186 [21:24<11:22, 10.67s/it] 66%|██████▌   | 123/186 [21:35<11:20, 10.80s/it] 67%|██████▋   | 124/186 [21:46<11:09, 10.79s/it] 67%|██████▋   | 125/186 [21:56<10:54, 10.74s/it] 68%|██████▊   | 126/186 [22:07<10:41, 10.69s/it] 68%|██████▊   | 127/186 [22:18<10:31, 10.71s/it] 69%|██████▉   | 128/186 [22:29<10:24, 10.76s/it] 69%|██████▉   | 129/186 [22:40<10:16, 10.82s/it] 70%|██████▉   | 130/186 [22:50<10:05, 10.82s/it] 70%|███████   | 131/186 [23:01<09:51, 10.76s/it] 71%|███████   | 132/186 [23:12<09:42, 10.78s/it] 72%|███████▏  | 133/186 [23:22<09:28, 10.72s/it] 72%|███████▏  | 134/186 [23:33<09:16, 10.70s/it] 73%|███████▎  | 135/186 [23:44<09:05, 10.70s/it] 73%|███████▎  | 136/186 [23:54<08:55, 10.72s/it] 74%|███████▎  | 137/186 [24:05<08:47, 10.77s/it] 74%|███████▍  | 138/186 [24:16<08:38, 10.81s/it] 75%|███████▍  | 139/186 [24:27<08:30, 10.86s/it] 75%|███████▌  | 140/186 [24:38<08:16, 10.79s/it] 76%|███████▌  | 141/186 [24:49<08:06, 10.81s/it] 76%|███████▋  | 142/186 [24:59<07:54, 10.79s/it] 77%|███████▋  | 143/186 [25:10<07:46, 10.86s/it] 77%|███████▋  | 144/186 [25:21<07:34, 10.81s/it] 78%|███████▊  | 145/186 [25:32<07:22, 10.80s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 78%|███████▊  | 146/186 [25:43<07:13, 10.83s/it] 79%|███████▉  | 147/186 [25:54<07:00, 10.77s/it] 80%|███████▉  | 148/186 [26:04<06:49, 10.77s/it] 80%|████████  | 149/186 [26:15<06:38, 10.77s/it] 81%|████████  | 150/186 [26:26<06:26, 10.74s/it] 81%|████████  | 151/186 [26:37<06:17, 10.79s/it] 82%|████████▏ | 152/186 [26:47<06:06, 10.77s/it] 82%|████████▏ | 153/186 [26:58<05:54, 10.74s/it] 83%|████████▎ | 154/186 [27:09<05:43, 10.74s/it] 83%|████████▎ | 155/186 [27:19<05:32, 10.73s/it] 84%|████████▍ | 156/186 [27:31<05:27, 10.92s/it] 84%|████████▍ | 157/186 [27:42<05:18, 10.99s/it] 85%|████████▍ | 158/186 [27:53<05:05, 10.90s/it] 85%|████████▌ | 159/186 [28:03<04:52, 10.85s/it] 86%|████████▌ | 160/186 [28:14<04:41, 10.83s/it] 87%|████████▋ | 161/186 [28:25<04:29, 10.77s/it] 87%|████████▋ | 162/186 [28:36<04:18, 10.78s/it] 88%|████████▊ | 163/186 [28:46<04:08, 10.79s/it] 88%|████████▊ | 164/186 [28:57<03:57, 10.79s/it] 89%|████████▊ | 165/186 [29:08<03:47, 10.82s/it] 89%|████████▉ | 166/186 [29:19<03:36, 10.82s/it] 90%|████████▉ | 167/186 [29:30<03:25, 10.84s/it] 90%|█████████ | 168/186 [29:41<03:14, 10.79s/it] 91%|█████████ | 169/186 [29:51<03:02, 10.72s/it]WARNING: tokenization mismatch: 1 vs. 48. (ignored)
 91%|█████████▏| 170/186 [30:02<02:51, 10.73s/it] 92%|█████████▏| 171/186 [30:13<02:42, 10.84s/it] 92%|█████████▏| 172/186 [30:24<02:31, 10.83s/it] 93%|█████████▎| 173/186 [30:35<02:22, 10.94s/it] 94%|█████████▎| 174/186 [30:46<02:10, 10.90s/it] 94%|█████████▍| 175/186 [30:56<01:59, 10.84s/it] 95%|█████████▍| 176/186 [31:07<01:48, 10.83s/it] 95%|█████████▌| 177/186 [31:18<01:37, 10.81s/it] 96%|█████████▌| 178/186 [31:29<01:26, 10.83s/it] 96%|█████████▌| 179/186 [31:40<01:15, 10.84s/it] 97%|█████████▋| 180/186 [31:51<01:05, 10.85s/it] 97%|█████████▋| 181/186 [32:01<00:54, 10.82s/it] 98%|█████████▊| 182/186 [32:12<00:43, 10.82s/it] 98%|█████████▊| 183/186 [32:23<00:32, 10.84s/it] 99%|█████████▉| 184/186 [32:34<00:21, 10.93s/it] 99%|█████████▉| 185/186 [32:45<00:10, 11.00s/it]100%|██████████| 186/186 [32:56<00:00, 11.02s/it]                                                 {'train_runtime': 1976.9392, 'train_samples_per_second': 6.796, 'train_steps_per_second': 0.094, 'train_loss': 0.11789133215463289, 'epoch': 1.0}
100%|██████████| 186/186 [32:56<00:00, 11.02s/it]100%|██████████| 186/186 [32:56<00:00, 10.63s/it]
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-0
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33msliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-0[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251017_020233-ks3grx36/logs[0m
[2025-10-17 02:37:17,682] [INFO] [launch.py:347:main] Process 2187261 exits successfully.
[2025-10-17 02:37:17,683] [INFO] [launch.py:347:main] Process 2187258 exits successfully.
[2025-10-17 02:37:17,683] [INFO] [launch.py:347:main] Process 2187262 exits successfully.
[2025-10-17 02:37:17,683] [INFO] [launch.py:347:main] Process 2187260 exits successfully.
[2025-10-17 02:37:17,683] [INFO] [launch.py:347:main] Process 2187259 exits successfully.
[2025-10-17 02:37:22,689] [INFO] [launch.py:347:main] Process 2187257 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 02:37:25,913] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 02:37:26,468] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-17 02:37:26,468] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12423 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1.5e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-0 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-0 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 2 --use_anchor False --use_sample_weight True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 02:37:28,121] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 02:37:28,702] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-17 02:37:28,702] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-17 02:37:28,702] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-17 02:37:28,702] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-17 02:37:28,702] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 02:37:32,480] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 02:37:32,568] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 02:37:32,785] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 02:37:32,878] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 02:37:32,957] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 02:37:33,267] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 02:37:33,837] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-17 02:37:34,048] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 02:37:34,145] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 02:37:34,161] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 02:37:34,515] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 02:37:34,515] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 02:37:34,638] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: creating run
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251017_023735-f0f9raca
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-0
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/f0f9raca
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.10s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.35s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.64s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.70s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.06s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.28s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.48s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.42s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.43s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.54s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.72s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.76s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.45s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.77s/it]
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.10s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.96s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.58s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.75s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.08s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.16s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.28s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.61s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.54s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.65s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.55s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.86s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.49s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.80s/it]
Formatting inputs...Skip in lazy mode
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:4')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:5')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:3')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:0')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:2')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:1')
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.011878728866577148 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.012113809585571289 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.013100862503051758 seconds
Loading extension module fused_adam...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Time to load fused_adam op: 0.10146594047546387 seconds
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.012854814529418945 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.20112228393554688 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/186 [00:13<42:05, 13.65s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1297 > 1048). Running this sequence through the model will result in indexing errors
  1%|          | 2/186 [00:22<33:45, 11.01s/it]  2%|▏         | 3/186 [00:32<31:10, 10.22s/it]  2%|▏         | 4/186 [00:41<29:58,  9.88s/it]  3%|▎         | 5/186 [00:51<29:26,  9.76s/it]  3%|▎         | 6/186 [01:00<29:13,  9.74s/it]  4%|▍         | 7/186 [01:10<29:21,  9.84s/it]  4%|▍         | 8/186 [01:20<29:13,  9.85s/it]  5%|▍         | 9/186 [01:30<29:04,  9.86s/it]  5%|▌         | 10/186 [01:40<29:12,  9.96s/it]  6%|▌         | 11/186 [01:50<29:16, 10.04s/it]  6%|▋         | 12/186 [02:01<29:14, 10.08s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1200 > 1048). Running this sequence through the model will result in indexing errors
  7%|▋         | 13/186 [02:11<29:20, 10.18s/it]  8%|▊         | 14/186 [02:21<29:14, 10.20s/it]  8%|▊         | 15/186 [02:31<28:58, 10.17s/it]  9%|▊         | 16/186 [02:41<28:44, 10.14s/it]  9%|▉         | 17/186 [02:52<28:32, 10.14s/it] 10%|▉         | 18/186 [03:02<28:24, 10.14s/it] 10%|█         | 19/186 [03:12<28:22, 10.19s/it] 11%|█         | 20/186 [03:22<28:13, 10.20s/it] 11%|█▏        | 21/186 [03:32<27:57, 10.16s/it] 12%|█▏        | 22/186 [03:42<27:47, 10.17s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1150 > 1048). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1297 > 1048). Running this sequence through the model will result in indexing errors
 12%|█▏        | 23/186 [03:53<27:52, 10.26s/it] 13%|█▎        | 24/186 [04:04<28:19, 10.49s/it] 13%|█▎        | 25/186 [04:14<27:59, 10.43s/it] 14%|█▍        | 26/186 [04:25<27:53, 10.46s/it] 15%|█▍        | 27/186 [04:35<27:45, 10.47s/it] 15%|█▌        | 28/186 [04:46<27:23, 10.40s/it] 16%|█▌        | 29/186 [04:56<27:16, 10.42s/it] 16%|█▌        | 30/186 [05:07<27:09, 10.44s/it] 17%|█▋        | 31/186 [05:17<26:50, 10.39s/it] 17%|█▋        | 32/186 [05:27<26:43, 10.41s/it] 18%|█▊        | 33/186 [05:38<26:39, 10.45s/it] 18%|█▊        | 34/186 [05:48<26:27, 10.44s/it] 19%|█▉        | 35/186 [05:59<26:22, 10.48s/it] 19%|█▉        | 36/186 [06:09<26:02, 10.42s/it] 20%|█▉        | 37/186 [06:19<25:51, 10.41s/it] 20%|██        | 38/186 [06:30<25:38, 10.39s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 21%|██        | 39/186 [06:40<25:35, 10.44s/it] 22%|██▏       | 40/186 [06:51<25:33, 10.50s/it] 22%|██▏       | 41/186 [07:01<25:15, 10.45s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1240 > 1048). Running this sequence through the model will result in indexing errors
 23%|██▎       | 42/186 [07:12<25:26, 10.60s/it] 23%|██▎       | 43/186 [07:23<25:09, 10.56s/it] 24%|██▎       | 44/186 [07:33<24:57, 10.54s/it] 24%|██▍       | 45/186 [07:44<24:45, 10.53s/it] 25%|██▍       | 46/186 [07:54<24:36, 10.55s/it] 25%|██▌       | 47/186 [08:05<24:38, 10.64s/it] 26%|██▌       | 48/186 [08:16<24:26, 10.63s/it] 26%|██▋       | 49/186 [08:26<24:13, 10.61s/it] 27%|██▋       | 50/186 [08:37<23:53, 10.54s/it] 27%|██▋       | 51/186 [08:47<23:46, 10.57s/it] 28%|██▊       | 52/186 [08:58<23:40, 10.60s/it] 28%|██▊       | 53/186 [09:09<23:32, 10.62s/it] 29%|██▉       | 54/186 [09:19<23:19, 10.60s/it] 30%|██▉       | 55/186 [09:30<23:03, 10.56s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
 30%|███       | 56/186 [09:41<23:08, 10.68s/it] 31%|███       | 57/186 [09:51<22:59, 10.69s/it] 31%|███       | 58/186 [10:02<22:51, 10.72s/it] 32%|███▏      | 59/186 [10:13<22:51, 10.80s/it] 32%|███▏      | 60/186 [10:24<22:38, 10.78s/it] 33%|███▎      | 61/186 [10:35<22:30, 10.80s/it] 33%|███▎      | 62/186 [10:45<22:09, 10.72s/it] 34%|███▍      | 63/186 [10:56<21:54, 10.69s/it] 34%|███▍      | 64/186 [11:06<21:40, 10.66s/it] 35%|███▍      | 65/186 [11:17<21:22, 10.60s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
 35%|███▌      | 66/186 [11:28<21:19, 10.66s/it] 36%|███▌      | 67/186 [11:38<21:05, 10.63s/it] 37%|███▋      | 68/186 [11:49<20:50, 10.60s/it] 37%|███▋      | 69/186 [12:00<20:48, 10.67s/it] 38%|███▊      | 70/186 [12:11<21:01, 10.87s/it] 38%|███▊      | 71/186 [12:22<20:40, 10.79s/it] 39%|███▊      | 72/186 [12:32<20:22, 10.73s/it] 39%|███▉      | 73/186 [12:43<20:02, 10.64s/it] 40%|███▉      | 74/186 [12:53<19:52, 10.65s/it] 40%|████      | 75/186 [13:04<19:45, 10.68s/it] 41%|████      | 76/186 [13:15<19:31, 10.65s/it] 41%|████▏     | 77/186 [13:25<19:16, 10.61s/it] 42%|████▏     | 78/186 [13:36<19:02, 10.58s/it] 42%|████▏     | 79/186 [13:46<18:57, 10.63s/it] 43%|████▎     | 80/186 [13:57<18:46, 10.62s/it] 44%|████▎     | 81/186 [14:08<18:45, 10.71s/it] 44%|████▍     | 82/186 [14:19<18:34, 10.71s/it] 45%|████▍     | 83/186 [14:29<18:23, 10.72s/it] 45%|████▌     | 84/186 [14:40<18:17, 10.76s/it] 46%|████▌     | 85/186 [14:51<18:04, 10.74s/it] 46%|████▌     | 86/186 [15:02<17:52, 10.72s/it] 47%|████▋     | 87/186 [15:12<17:39, 10.70s/it] 47%|████▋     | 88/186 [15:23<17:25, 10.67s/it] 48%|████▊     | 89/186 [15:34<17:20, 10.73s/it] 48%|████▊     | 90/186 [15:44<17:04, 10.67s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1143 > 1048). Running this sequence through the model will result in indexing errors
 49%|████▉     | 91/186 [15:55<17:02, 10.76s/it] 49%|████▉     | 92/186 [16:06<16:44, 10.69s/it] 50%|█████     | 93/186 [16:17<16:37, 10.73s/it] 51%|█████     | 94/186 [16:27<16:23, 10.69s/it] 51%|█████     | 95/186 [16:38<16:08, 10.64s/it] 52%|█████▏    | 96/186 [16:48<15:53, 10.60s/it] 52%|█████▏    | 97/186 [16:59<15:45, 10.62s/it] 53%|█████▎    | 98/186 [17:09<15:34, 10.61s/it] 53%|█████▎    | 99/186 [17:20<15:28, 10.67s/it] 54%|█████▍    | 100/186 [17:31<15:21, 10.72s/it] 54%|█████▍    | 101/186 [17:42<15:10, 10.71s/it] 55%|█████▍    | 102/186 [17:52<14:59, 10.70s/it] 55%|█████▌    | 103/186 [18:03<14:48, 10.70s/it] 56%|█████▌    | 104/186 [18:14<14:33, 10.65s/it] 56%|█████▋    | 105/186 [18:24<14:21, 10.64s/it] 57%|█████▋    | 106/186 [18:35<14:11, 10.64s/it] 58%|█████▊    | 107/186 [18:46<14:00, 10.64s/it] 58%|█████▊    | 108/186 [18:56<13:46, 10.59s/it] 59%|█████▊    | 109/186 [19:07<13:45, 10.72s/it] 59%|█████▉    | 110/186 [19:18<13:34, 10.72s/it] 60%|█████▉    | 111/186 [19:28<13:20, 10.67s/it] 60%|██████    | 112/186 [19:39<13:10, 10.69s/it] 61%|██████    | 113/186 [19:50<12:58, 10.67s/it] 61%|██████▏   | 114/186 [20:00<12:47, 10.66s/it] 62%|██████▏   | 115/186 [20:11<12:35, 10.64s/it] 62%|██████▏   | 116/186 [20:22<12:34, 10.78s/it] 63%|██████▎   | 117/186 [20:33<12:18, 10.70s/it] 63%|██████▎   | 118/186 [20:43<12:04, 10.66s/it] 64%|██████▍   | 119/186 [20:54<11:56, 10.70s/it] 65%|██████▍   | 120/186 [21:04<11:41, 10.63s/it] 65%|██████▌   | 121/186 [21:15<11:33, 10.66s/it] 66%|██████▌   | 122/186 [21:26<11:22, 10.66s/it] 66%|██████▌   | 123/186 [21:37<11:18, 10.77s/it] 67%|██████▋   | 124/186 [21:48<11:06, 10.75s/it] 67%|██████▋   | 125/186 [21:59<11:01, 10.84s/it] 68%|██████▊   | 126/186 [22:09<10:45, 10.76s/it] 68%|██████▊   | 127/186 [22:20<10:35, 10.77s/it] 69%|██████▉   | 128/186 [22:31<10:29, 10.85s/it] 69%|██████▉   | 129/186 [22:42<10:16, 10.82s/it] 70%|██████▉   | 130/186 [22:53<10:05, 10.82s/it] 70%|███████   | 131/186 [23:03<09:53, 10.78s/it] 71%|███████   | 132/186 [23:14<09:45, 10.85s/it] 72%|███████▏  | 133/186 [23:25<09:30, 10.76s/it] 72%|███████▏  | 134/186 [23:35<09:17, 10.71s/it] 73%|███████▎  | 135/186 [23:46<09:08, 10.75s/it] 73%|███████▎  | 136/186 [23:57<08:59, 10.78s/it] 74%|███████▎  | 137/186 [24:08<08:48, 10.78s/it] 74%|███████▍  | 138/186 [24:19<08:38, 10.81s/it] 75%|███████▍  | 139/186 [24:30<08:29, 10.84s/it] 75%|███████▌  | 140/186 [24:40<08:15, 10.78s/it] 76%|███████▌  | 141/186 [24:51<08:02, 10.72s/it] 76%|███████▋  | 142/186 [25:02<07:53, 10.77s/it] 77%|███████▋  | 143/186 [25:12<07:42, 10.75s/it] 77%|███████▋  | 144/186 [25:23<07:30, 10.72s/it] 78%|███████▊  | 145/186 [25:34<07:19, 10.73s/it] 78%|███████▊  | 146/186 [25:45<07:12, 10.81s/it] 79%|███████▉  | 147/186 [25:55<06:59, 10.75s/it] 80%|███████▉  | 148/186 [26:06<06:47, 10.73s/it] 80%|████████  | 149/186 [26:17<06:38, 10.78s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 81%|████████  | 150/186 [26:28<06:26, 10.74s/it] 81%|████████  | 151/186 [26:38<06:14, 10.69s/it] 82%|████████▏ | 152/186 [26:49<06:03, 10.69s/it] 82%|████████▏ | 153/186 [27:00<05:54, 10.75s/it] 83%|████████▎ | 154/186 [27:11<05:45, 10.79s/it] 83%|████████▎ | 155/186 [27:21<05:34, 10.78s/it] 84%|████████▍ | 156/186 [27:33<05:28, 10.94s/it] 84%|████████▍ | 157/186 [27:44<05:18, 10.97s/it] 85%|████████▍ | 158/186 [27:55<05:11, 11.11s/it] 85%|████████▌ | 159/186 [28:06<04:55, 10.96s/it] 86%|████████▌ | 160/186 [28:17<04:43, 10.90s/it] 87%|████████▋ | 161/186 [28:28<04:32, 10.92s/it] 87%|████████▋ | 162/186 [28:39<04:23, 11.00s/it] 88%|████████▊ | 163/186 [28:50<04:11, 10.95s/it] 88%|████████▊ | 164/186 [29:00<03:59, 10.89s/it] 89%|████████▊ | 165/186 [29:11<03:45, 10.75s/it] 89%|████████▉ | 166/186 [29:22<03:34, 10.74s/it] 90%|████████▉ | 167/186 [29:32<03:24, 10.76s/it] 90%|█████████ | 168/186 [29:43<03:13, 10.74s/it] 91%|█████████ | 169/186 [29:54<03:02, 10.76s/it] 91%|█████████▏| 170/186 [30:05<02:52, 10.78s/it] 92%|█████████▏| 171/186 [30:16<02:42, 10.85s/it] 92%|█████████▏| 172/186 [30:26<02:31, 10.84s/it] 93%|█████████▎| 173/186 [30:37<02:21, 10.90s/it] 94%|█████████▎| 174/186 [30:48<02:09, 10.82s/it] 94%|█████████▍| 175/186 [30:59<01:58, 10.82s/it] 95%|█████████▍| 176/186 [31:10<01:48, 10.84s/it] 95%|█████████▌| 177/186 [31:21<01:38, 10.94s/it] 96%|█████████▌| 178/186 [31:32<01:27, 10.92s/it] 96%|█████████▌| 179/186 [31:43<01:16, 10.91s/it] 97%|█████████▋| 180/186 [31:53<01:05, 10.83s/it] 97%|█████████▋| 181/186 [32:04<00:54, 10.85s/it] 98%|█████████▊| 182/186 [32:15<00:43, 10.79s/it] 98%|█████████▊| 183/186 [32:26<00:32, 10.83s/it] 99%|█████████▉| 184/186 [32:37<00:21, 10.80s/it] 99%|█████████▉| 185/186 [32:48<00:10, 10.86s/it]100%|██████████| 186/186 [32:58<00:00, 10.85s/it]                                                 {'train_runtime': 1978.9834, 'train_samples_per_second': 6.789, 'train_steps_per_second': 0.094, 'train_loss': 0.1344609824560022, 'epoch': 1.0}
100%|██████████| 186/186 [32:58<00:00, 10.85s/it]100%|██████████| 186/186 [32:58<00:00, 10.64s/it]
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-0
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33msliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-0[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251017_023735-f0f9raca/logs[0m
[2025-10-17 03:12:14,132] [INFO] [launch.py:347:main] Process 2188996 exits successfully.
[2025-10-17 03:12:14,133] [INFO] [launch.py:347:main] Process 2188993 exits successfully.
[2025-10-17 03:12:14,133] [INFO] [launch.py:347:main] Process 2188997 exits successfully.
[2025-10-17 03:12:14,133] [INFO] [launch.py:347:main] Process 2188995 exits successfully.
[2025-10-17 03:12:14,133] [INFO] [launch.py:347:main] Process 2188994 exits successfully.
[2025-10-17 03:12:18,138] [INFO] [launch.py:347:main] Process 2188992 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 03:12:21,313] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 03:12:21,862] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-17 03:12:21,862] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12423 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-1 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-1 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 0.5 --use_anchor False --use_sample_weight True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 03:12:23,549] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 03:12:24,175] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-17 03:12:24,175] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-17 03:12:24,175] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-17 03:12:24,175] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-17 03:12:24,175] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 03:12:28,717] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 03:12:29,037] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 03:12:29,071] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 03:12:29,272] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 03:12:29,317] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 03:12:29,516] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 03:12:29,516] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 03:12:29,584] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 03:12:29,649] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-10-17 03:12:29,934] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-17 03:12:30,157] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 03:12:30,298] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 03:12:30,725] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251017_031230-7zguq020
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-1
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/7zguq020
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.44s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.61s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.63s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.90s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.61s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.91s/it]
to bfloat16...
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.70s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.69s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.99s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.27s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.79s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.67s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.64s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.63s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.95s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.59s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.90s/it]
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.87s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.66s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.98s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.57s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.89s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.68s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.70s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  4.00s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.63s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.79s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.48s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.80s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.57s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.91s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.62s/it]
Formatting inputs...Skip in lazy mode
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:2')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:4')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:1')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:5')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:0')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:3')
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.010468721389770508 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.011898994445800781 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.009870767593383789 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10207605361938477 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.012425899505615234 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10096549987792969 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/186 [00:09<29:57,  9.71s/it]  1%|          | 2/186 [00:18<27:40,  9.02s/it]  2%|▏         | 3/186 [00:27<27:55,  9.16s/it]  2%|▏         | 4/186 [00:37<28:24,  9.37s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
  3%|▎         | 5/186 [00:47<29:06,  9.65s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1263 > 1048). Running this sequence through the model will result in indexing errors
  3%|▎         | 6/186 [00:57<29:25,  9.81s/it]  4%|▍         | 7/186 [01:07<29:13,  9.80s/it]  4%|▍         | 8/186 [01:17<29:13,  9.85s/it]  5%|▍         | 9/186 [01:27<29:12,  9.90s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1159 > 1048). Running this sequence through the model will result in indexing errors
  5%|▌         | 10/186 [01:37<29:27, 10.05s/it]  6%|▌         | 11/186 [01:47<29:32, 10.13s/it]  6%|▋         | 12/186 [01:58<29:25, 10.15s/it]  7%|▋         | 13/186 [02:08<29:22, 10.19s/it]  8%|▊         | 14/186 [02:18<29:14, 10.20s/it]  8%|▊         | 15/186 [02:29<29:14, 10.26s/it]  9%|▊         | 16/186 [02:39<29:05, 10.27s/it]  9%|▉         | 17/186 [02:49<29:08, 10.35s/it] 10%|▉         | 18/186 [03:00<28:47, 10.28s/it] 10%|█         | 19/186 [03:10<28:41, 10.31s/it] 11%|█         | 20/186 [03:20<28:21, 10.25s/it] 11%|█▏        | 21/186 [03:30<28:21, 10.31s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1263 > 1048). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1233 > 1048). Running this sequence through the model will result in indexing errors
 12%|█▏        | 22/186 [03:41<28:30, 10.43s/it] 12%|█▏        | 23/186 [03:51<28:11, 10.38s/it] 13%|█▎        | 24/186 [04:02<28:08, 10.42s/it] 13%|█▎        | 25/186 [04:12<28:00, 10.44s/it] 14%|█▍        | 26/186 [04:23<27:53, 10.46s/it] 15%|█▍        | 27/186 [04:33<27:40, 10.44s/it] 15%|█▌        | 28/186 [04:44<27:27, 10.43s/it] 16%|█▌        | 29/186 [04:54<27:16, 10.42s/it] 16%|█▌        | 30/186 [05:05<27:09, 10.45s/it] 17%|█▋        | 31/186 [05:15<26:58, 10.44s/it] 17%|█▋        | 32/186 [05:26<26:52, 10.47s/it] 18%|█▊        | 33/186 [05:36<26:45, 10.49s/it] 18%|█▊        | 34/186 [05:47<26:39, 10.52s/it] 19%|█▉        | 35/186 [05:57<26:35, 10.56s/it] 19%|█▉        | 36/186 [06:08<26:24, 10.57s/it] 20%|█▉        | 37/186 [06:19<26:25, 10.64s/it] 20%|██        | 38/186 [06:29<26:08, 10.59s/it] 21%|██        | 39/186 [06:40<26:07, 10.66s/it] 22%|██▏       | 40/186 [06:51<25:52, 10.64s/it] 22%|██▏       | 41/186 [07:01<25:33, 10.58s/it] 23%|██▎       | 42/186 [07:12<25:18, 10.54s/it] 23%|██▎       | 43/186 [07:22<25:08, 10.55s/it] 24%|██▎       | 44/186 [07:33<25:00, 10.56s/it] 24%|██▍       | 45/186 [07:43<24:50, 10.57s/it] 25%|██▍       | 46/186 [07:54<24:40, 10.57s/it] 25%|██▌       | 47/186 [08:04<24:28, 10.57s/it] 26%|██▌       | 48/186 [08:15<24:22, 10.59s/it] 26%|██▋       | 49/186 [08:26<24:17, 10.64s/it] 27%|██▋       | 50/186 [08:37<24:10, 10.67s/it] 27%|██▋       | 51/186 [08:47<24:04, 10.70s/it] 28%|██▊       | 52/186 [08:58<23:53, 10.70s/it] 28%|██▊       | 53/186 [09:09<23:39, 10.68s/it] 29%|██▉       | 54/186 [09:19<23:19, 10.60s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 30%|██▉       | 55/186 [09:30<23:06, 10.58s/it] 30%|███       | 56/186 [09:40<22:59, 10.61s/it] 31%|███       | 57/186 [09:51<22:50, 10.62s/it] 31%|███       | 58/186 [10:02<22:49, 10.70s/it] 32%|███▏      | 59/186 [10:13<22:45, 10.75s/it] 32%|███▏      | 60/186 [10:24<22:34, 10.75s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1150 > 1048). Running this sequence through the model will result in indexing errors
 33%|███▎      | 61/186 [10:34<22:23, 10.75s/it] 33%|███▎      | 62/186 [10:45<22:19, 10.80s/it] 34%|███▍      | 63/186 [10:56<22:02, 10.75s/it] 34%|███▍      | 64/186 [11:06<21:42, 10.68s/it] 35%|███▍      | 65/186 [11:17<21:34, 10.70s/it] 35%|███▌      | 66/186 [11:28<21:43, 10.86s/it] 36%|███▌      | 67/186 [11:39<21:40, 10.93s/it] 37%|███▋      | 68/186 [11:50<21:33, 10.96s/it] 37%|███▋      | 69/186 [12:01<21:16, 10.91s/it] 38%|███▊      | 70/186 [12:12<20:57, 10.84s/it] 38%|███▊      | 71/186 [12:23<20:41, 10.80s/it] 39%|███▊      | 72/186 [12:33<20:22, 10.72s/it] 39%|███▉      | 73/186 [12:44<20:12, 10.73s/it] 40%|███▉      | 74/186 [12:55<19:58, 10.70s/it] 40%|████      | 75/186 [13:05<19:41, 10.65s/it] 41%|████      | 76/186 [13:16<19:31, 10.65s/it] 41%|████▏     | 77/186 [13:26<19:16, 10.61s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
 42%|████▏     | 78/186 [13:37<19:02, 10.58s/it] 42%|████▏     | 79/186 [13:47<18:50, 10.57s/it] 43%|████▎     | 80/186 [13:58<18:42, 10.59s/it] 44%|████▎     | 81/186 [14:08<18:29, 10.57s/it] 44%|████▍     | 82/186 [14:19<18:22, 10.61s/it] 45%|████▍     | 83/186 [14:30<18:16, 10.65s/it] 45%|████▌     | 84/186 [14:41<18:18, 10.77s/it] 46%|████▌     | 85/186 [14:52<18:03, 10.73s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1159 > 1048). Running this sequence through the model will result in indexing errors
 46%|████▌     | 86/186 [15:02<17:51, 10.71s/it] 47%|████▋     | 87/186 [15:13<17:41, 10.72s/it] 47%|████▋     | 88/186 [15:24<17:29, 10.70s/it] 48%|████▊     | 89/186 [15:34<17:22, 10.75s/it] 48%|████▊     | 90/186 [15:45<17:15, 10.79s/it] 49%|████▉     | 91/186 [15:56<17:04, 10.78s/it] 49%|████▉     | 92/186 [16:07<16:51, 10.76s/it] 50%|█████     | 93/186 [16:18<16:48, 10.84s/it] 51%|█████     | 94/186 [16:29<16:33, 10.80s/it] 51%|█████     | 95/186 [16:39<16:20, 10.78s/it] 52%|█████▏    | 96/186 [16:50<16:01, 10.69s/it] 52%|█████▏    | 97/186 [17:01<15:56, 10.75s/it] 53%|█████▎    | 98/186 [17:12<15:50, 10.80s/it] 53%|█████▎    | 99/186 [17:22<15:40, 10.81s/it] 54%|█████▍    | 100/186 [17:33<15:29, 10.81s/it] 54%|█████▍    | 101/186 [17:44<15:14, 10.75s/it] 55%|█████▍    | 102/186 [17:54<14:58, 10.69s/it] 55%|█████▌    | 103/186 [18:06<15:01, 10.86s/it] 56%|█████▌    | 104/186 [18:16<14:47, 10.83s/it] 56%|█████▋    | 105/186 [18:27<14:36, 10.82s/it] 57%|█████▋    | 106/186 [18:38<14:26, 10.83s/it] 58%|█████▊    | 107/186 [18:49<14:18, 10.87s/it] 58%|█████▊    | 108/186 [19:00<14:05, 10.84s/it] 59%|█████▊    | 109/186 [19:10<13:49, 10.77s/it] 59%|█████▉    | 110/186 [19:21<13:34, 10.72s/it] 60%|█████▉    | 111/186 [19:32<13:26, 10.75s/it] 60%|██████    | 112/186 [19:42<13:12, 10.72s/it] 61%|██████    | 113/186 [19:53<13:03, 10.74s/it] 61%|██████▏   | 114/186 [20:04<12:54, 10.76s/it] 62%|██████▏   | 115/186 [20:15<12:46, 10.79s/it] 62%|██████▏   | 116/186 [20:26<12:35, 10.79s/it] 63%|██████▎   | 117/186 [20:36<12:20, 10.73s/it] 63%|██████▎   | 118/186 [20:47<12:10, 10.74s/it] 64%|██████▍   | 119/186 [20:58<12:02, 10.79s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 65%|██████▍   | 120/186 [21:09<11:49, 10.75s/it] 65%|██████▌   | 121/186 [21:19<11:39, 10.77s/it] 66%|██████▌   | 122/186 [21:30<11:31, 10.80s/it] 66%|██████▌   | 123/186 [21:41<11:24, 10.86s/it] 67%|██████▋   | 124/186 [21:52<11:11, 10.84s/it] 67%|██████▋   | 125/186 [22:03<11:01, 10.85s/it] 68%|██████▊   | 126/186 [22:14<10:47, 10.79s/it] 68%|██████▊   | 127/186 [22:24<10:37, 10.80s/it] 69%|██████▉   | 128/186 [22:35<10:23, 10.75s/it] 69%|██████▉   | 129/186 [22:46<10:14, 10.78s/it] 70%|██████▉   | 130/186 [22:57<10:02, 10.76s/it] 70%|███████   | 131/186 [23:08<09:55, 10.83s/it] 71%|███████   | 132/186 [23:19<09:45, 10.85s/it] 72%|███████▏  | 133/186 [23:29<09:33, 10.81s/it] 72%|███████▏  | 134/186 [23:40<09:25, 10.87s/it] 73%|███████▎  | 135/186 [23:51<09:17, 10.93s/it] 73%|███████▎  | 136/186 [24:02<09:04, 10.88s/it] 74%|███████▎  | 137/186 [24:13<08:58, 10.98s/it] 74%|███████▍  | 138/186 [24:24<08:41, 10.87s/it] 75%|███████▍  | 139/186 [24:35<08:34, 10.94s/it] 75%|███████▌  | 140/186 [24:46<08:24, 10.97s/it] 76%|███████▌  | 141/186 [24:57<08:12, 10.95s/it] 76%|███████▋  | 142/186 [25:08<08:02, 10.98s/it] 77%|███████▋  | 143/186 [25:19<07:50, 10.94s/it] 77%|███████▋  | 144/186 [25:30<07:40, 10.96s/it] 78%|███████▊  | 145/186 [25:41<07:27, 10.92s/it] 78%|███████▊  | 146/186 [25:52<07:16, 10.90s/it] 79%|███████▉  | 147/186 [26:03<07:06, 10.93s/it] 80%|███████▉  | 148/186 [26:13<06:52, 10.84s/it] 80%|████████  | 149/186 [26:24<06:42, 10.88s/it] 81%|████████  | 150/186 [26:35<06:30, 10.85s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 81%|████████  | 151/186 [26:46<06:18, 10.80s/it] 82%|████████▏ | 152/186 [26:56<06:05, 10.76s/it] 82%|████████▏ | 153/186 [27:07<05:54, 10.73s/it] 83%|████████▎ | 154/186 [27:18<05:46, 10.82s/it] 83%|████████▎ | 155/186 [27:29<05:34, 10.80s/it] 84%|████████▍ | 156/186 [27:40<05:25, 10.86s/it] 84%|████████▍ | 157/186 [27:51<05:15, 10.88s/it] 85%|████████▍ | 158/186 [28:02<05:05, 10.90s/it] 85%|████████▌ | 159/186 [28:13<04:56, 10.99s/it] 86%|████████▌ | 160/186 [28:24<04:46, 11.04s/it] 87%|████████▋ | 161/186 [28:35<04:35, 11.03s/it] 87%|████████▋ | 162/186 [28:47<04:29, 11.21s/it] 88%|████████▊ | 163/186 [28:58<04:18, 11.23s/it] 88%|████████▊ | 164/186 [29:09<04:07, 11.27s/it] 89%|████████▊ | 165/186 [29:20<03:53, 11.14s/it] 89%|████████▉ | 166/186 [29:31<03:42, 11.15s/it] 90%|████████▉ | 167/186 [29:42<03:30, 11.08s/it] 90%|█████████ | 168/186 [29:53<03:20, 11.12s/it] 91%|█████████ | 169/186 [30:04<03:08, 11.09s/it] 91%|█████████▏| 170/186 [30:16<02:57, 11.11s/it] 92%|█████████▏| 171/186 [30:26<02:44, 10.98s/it] 92%|█████████▏| 172/186 [30:37<02:33, 10.95s/it] 93%|█████████▎| 173/186 [30:48<02:23, 11.07s/it] 94%|█████████▎| 174/186 [30:59<02:11, 11.00s/it] 94%|█████████▍| 175/186 [31:11<02:01, 11.06s/it] 95%|█████████▍| 176/186 [31:21<01:50, 11.03s/it] 95%|█████████▌| 177/186 [31:32<01:39, 11.03s/it] 96%|█████████▌| 178/186 [31:43<01:27, 10.99s/it] 96%|█████████▌| 179/186 [31:54<01:16, 10.98s/it] 97%|█████████▋| 180/186 [32:06<01:06, 11.06s/it] 97%|█████████▋| 181/186 [32:17<00:55, 11.03s/it] 98%|█████████▊| 182/186 [32:28<00:44, 11.04s/it] 98%|█████████▊| 183/186 [32:39<00:33, 11.08s/it] 99%|█████████▉| 184/186 [32:50<00:22, 11.06s/it] 99%|█████████▉| 185/186 [33:01<00:11, 11.09s/it]100%|██████████| 186/186 [33:12<00:00, 11.14s/it]                                                 {'train_runtime': 1992.7292, 'train_samples_per_second': 6.742, 'train_steps_per_second': 0.093, 'train_loss': 0.151047122093939, 'epoch': 1.0}
100%|██████████| 186/186 [33:12<00:00, 11.14s/it]100%|██████████| 186/186 [33:12<00:00, 10.71s/it]
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-1
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33msliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-1[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251017_031230-7zguq020/logs[0m
[2025-10-17 03:47:25,627] [INFO] [launch.py:347:main] Process 2190739 exits successfully.
[2025-10-17 03:47:26,629] [INFO] [launch.py:347:main] Process 2190735 exits successfully.
[2025-10-17 03:47:26,629] [INFO] [launch.py:347:main] Process 2190738 exits successfully.
[2025-10-17 03:47:26,630] [INFO] [launch.py:347:main] Process 2190737 exits successfully.
[2025-10-17 03:47:27,631] [INFO] [launch.py:347:main] Process 2190736 exits successfully.
[2025-10-17 03:47:33,638] [INFO] [launch.py:347:main] Process 2190734 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 03:47:36,937] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 03:47:37,491] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-17 03:47:37,491] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12423 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-1 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-1 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 1 --use_anchor False --use_sample_weight True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 03:47:38,964] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 03:47:39,549] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-17 03:47:39,549] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-17 03:47:39,549] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-17 03:47:39,549] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-17 03:47:39,549] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 03:47:43,918] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 03:47:44,232] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 03:47:44,232] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-10-17 03:47:44,276] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-17 03:47:44,716] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 03:47:44,741] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 03:47:44,757] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 03:47:44,813] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 03:47:45,042] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 03:47:45,057] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 03:47:45,138] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 03:47:45,210] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 03:47:45,385] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: creating run
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251017_034745-3xmsw645
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-1
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/3xmsw645
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.86s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.78s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.10s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.85s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.77s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.84s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.63s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  5.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.59s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.93s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.92s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.94s/it]
to bfloat16...
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.84s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.40s/it]
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.98s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.13s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.93s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.11s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.63s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.95s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.71s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.63s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.66s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.93s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.61s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.91s/it]
Formatting inputs...Skip in lazy mode
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.69s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.99s/it]
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:2')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:4')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:1')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:0')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:5')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:3')
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.011117696762084961 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.013718843460083008 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10144543647766113 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.0111236572265625 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10130190849304199 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.1015017032623291 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Token indices sequence length is longer than the specified maximum sequence length for this model (1075 > 1048). Running this sequence through the model will result in indexing errors
  1%|          | 1/186 [00:10<31:28, 10.21s/it]WARNING: tokenization mismatch: 1 vs. 47. (ignored)
  1%|          | 2/186 [00:19<28:45,  9.38s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1160 > 1048). Running this sequence through the model will result in indexing errors
  2%|▏         | 3/186 [00:28<28:10,  9.24s/it]  2%|▏         | 4/186 [00:37<28:46,  9.49s/it]  3%|▎         | 5/186 [00:47<28:57,  9.60s/it]  3%|▎         | 6/186 [00:57<29:05,  9.70s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1073 > 1048). Running this sequence through the model will result in indexing errors
  4%|▍         | 7/186 [01:07<29:07,  9.76s/it]  4%|▍         | 8/186 [01:17<29:20,  9.89s/it]  5%|▍         | 9/186 [01:27<28:57,  9.82s/it]  5%|▌         | 10/186 [01:37<28:54,  9.86s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1297 > 1048). Running this sequence through the model will result in indexing errors
  6%|▌         | 11/186 [01:47<28:59,  9.94s/it]  6%|▋         | 12/186 [01:57<29:14, 10.08s/it]  7%|▋         | 13/186 [02:07<29:06, 10.10s/it]  8%|▊         | 14/186 [02:18<29:00, 10.12s/it]  8%|▊         | 15/186 [02:28<28:55, 10.15s/it]  9%|▊         | 16/186 [02:38<28:38, 10.11s/it]  9%|▉         | 17/186 [02:48<28:27, 10.11s/it] 10%|▉         | 18/186 [02:58<28:24, 10.14s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1263 > 1048). Running this sequence through the model will result in indexing errors
 10%|█         | 19/186 [03:09<28:33, 10.26s/it] 11%|█         | 20/186 [03:19<28:26, 10.28s/it] 11%|█▏        | 21/186 [03:29<28:10, 10.24s/it] 12%|█▏        | 22/186 [03:40<28:06, 10.29s/it] 12%|█▏        | 23/186 [03:50<28:06, 10.35s/it] 13%|█▎        | 24/186 [04:01<28:14, 10.46s/it] 13%|█▎        | 25/186 [04:11<27:58, 10.42s/it] 14%|█▍        | 26/186 [04:21<27:39, 10.37s/it] 15%|█▍        | 27/186 [04:32<27:46, 10.48s/it] 15%|█▌        | 28/186 [04:43<27:33, 10.46s/it] 16%|█▌        | 29/186 [04:53<27:35, 10.55s/it] 16%|█▌        | 30/186 [05:04<27:23, 10.54s/it] 17%|█▋        | 31/186 [05:14<27:19, 10.58s/it] 17%|█▋        | 32/186 [05:25<27:02, 10.54s/it] 18%|█▊        | 33/186 [05:35<26:47, 10.51s/it] 18%|█▊        | 34/186 [05:46<26:32, 10.48s/it] 19%|█▉        | 35/186 [05:57<26:38, 10.58s/it] 19%|█▉        | 36/186 [06:07<26:26, 10.57s/it] 20%|█▉        | 37/186 [06:18<26:13, 10.56s/it] 20%|██        | 38/186 [06:28<25:58, 10.53s/it] 21%|██        | 39/186 [06:39<25:51, 10.55s/it] 22%|██▏       | 40/186 [06:49<25:36, 10.53s/it] 22%|██▏       | 41/186 [06:59<25:15, 10.45s/it] 23%|██▎       | 42/186 [07:10<25:03, 10.44s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1165 > 1048). Running this sequence through the model will result in indexing errors
 23%|██▎       | 43/186 [07:20<24:55, 10.46s/it] 24%|██▎       | 44/186 [07:31<24:42, 10.44s/it] 24%|██▍       | 45/186 [07:42<24:43, 10.52s/it] 25%|██▍       | 46/186 [07:52<24:49, 10.64s/it] 25%|██▌       | 47/186 [08:04<25:00, 10.80s/it] 26%|██▌       | 48/186 [08:14<24:40, 10.73s/it] 26%|██▋       | 49/186 [08:25<24:33, 10.76s/it] 27%|██▋       | 50/186 [08:36<24:13, 10.69s/it] 27%|██▋       | 51/186 [08:46<23:57, 10.65s/it] 28%|██▊       | 52/186 [08:56<23:37, 10.58s/it] 28%|██▊       | 53/186 [09:07<23:20, 10.53s/it] 29%|██▉       | 54/186 [09:18<23:15, 10.57s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 30%|██▉       | 55/186 [09:28<22:57, 10.51s/it] 30%|███       | 56/186 [09:39<22:48, 10.53s/it] 31%|███       | 57/186 [09:49<22:41, 10.56s/it] 31%|███       | 58/186 [10:00<22:34, 10.58s/it] 32%|███▏      | 59/186 [10:11<22:38, 10.70s/it] 32%|███▏      | 60/186 [10:21<22:17, 10.61s/it] 33%|███▎      | 61/186 [10:32<22:06, 10.61s/it] 33%|███▎      | 62/186 [10:42<21:55, 10.61s/it] 34%|███▍      | 63/186 [10:53<21:46, 10.62s/it] 34%|███▍      | 64/186 [11:04<21:32, 10.59s/it] 35%|███▍      | 65/186 [11:14<21:21, 10.59s/it] 35%|███▌      | 66/186 [11:25<21:07, 10.56s/it] 36%|███▌      | 67/186 [11:35<20:59, 10.59s/it] 37%|███▋      | 68/186 [11:46<20:57, 10.65s/it] 37%|███▋      | 69/186 [11:57<20:53, 10.72s/it] 38%|███▊      | 70/186 [12:08<20:38, 10.68s/it] 38%|███▊      | 71/186 [12:18<20:31, 10.70s/it] 39%|███▊      | 72/186 [12:29<20:23, 10.73s/it] 39%|███▉      | 73/186 [12:40<20:05, 10.67s/it] 40%|███▉      | 74/186 [12:50<19:51, 10.64s/it] 40%|████      | 75/186 [13:01<19:38, 10.62s/it] 41%|████      | 76/186 [13:11<19:22, 10.57s/it] 41%|████▏     | 77/186 [13:22<19:07, 10.53s/it] 42%|████▏     | 78/186 [13:32<19:00, 10.56s/it] 42%|████▏     | 79/186 [13:43<18:52, 10.58s/it] 43%|████▎     | 80/186 [13:54<18:43, 10.60s/it] 44%|████▎     | 81/186 [14:04<18:25, 10.53s/it] 44%|████▍     | 82/186 [14:14<18:15, 10.53s/it] 45%|████▍     | 83/186 [14:25<18:07, 10.56s/it] 45%|████▌     | 84/186 [14:36<18:00, 10.59s/it] 46%|████▌     | 85/186 [14:46<17:45, 10.55s/it] 46%|████▌     | 86/186 [14:57<17:33, 10.54s/it] 47%|████▋     | 87/186 [15:07<17:27, 10.58s/it] 47%|████▋     | 88/186 [15:18<17:12, 10.53s/it] 48%|████▊     | 89/186 [15:28<16:58, 10.50s/it] 48%|████▊     | 90/186 [15:39<16:46, 10.48s/it] 49%|████▉     | 91/186 [15:50<16:46, 10.59s/it] 49%|████▉     | 92/186 [16:00<16:41, 10.65s/it] 50%|█████     | 93/186 [16:11<16:38, 10.73s/it] 51%|█████     | 94/186 [16:22<16:21, 10.67s/it] 51%|█████     | 95/186 [16:32<16:04, 10.60s/it] 52%|█████▏    | 96/186 [16:43<15:54, 10.60s/it] 52%|█████▏    | 97/186 [16:53<15:40, 10.57s/it] 53%|█████▎    | 98/186 [17:04<15:34, 10.62s/it] 53%|█████▎    | 99/186 [17:15<15:21, 10.59s/it] 54%|█████▍    | 100/186 [17:25<15:13, 10.62s/it] 54%|█████▍    | 101/186 [17:36<14:57, 10.56s/it] 55%|█████▍    | 102/186 [17:46<14:42, 10.51s/it] 55%|█████▌    | 103/186 [17:57<14:39, 10.60s/it] 56%|█████▌    | 104/186 [18:08<14:31, 10.63s/it] 56%|█████▋    | 105/186 [18:18<14:16, 10.57s/it] 57%|█████▋    | 106/186 [18:29<14:04, 10.55s/it] 58%|█████▊    | 107/186 [18:39<13:54, 10.57s/it] 58%|█████▊    | 108/186 [18:50<13:46, 10.59s/it] 59%|█████▊    | 109/186 [19:01<13:39, 10.64s/it] 59%|█████▉    | 110/186 [19:11<13:26, 10.61s/it] 60%|█████▉    | 111/186 [19:22<13:19, 10.66s/it] 60%|██████    | 112/186 [19:32<13:03, 10.59s/it] 61%|██████    | 113/186 [19:43<12:48, 10.53s/it] 61%|██████▏   | 114/186 [19:53<12:40, 10.56s/it] 62%|██████▏   | 115/186 [20:04<12:30, 10.57s/it] 62%|██████▏   | 116/186 [20:15<12:26, 10.66s/it] 63%|██████▎   | 117/186 [20:25<12:17, 10.69s/it] 63%|██████▎   | 118/186 [20:36<12:06, 10.68s/it] 64%|██████▍   | 119/186 [20:47<11:50, 10.61s/it] 65%|██████▍   | 120/186 [20:57<11:39, 10.60s/it] 65%|██████▌   | 121/186 [21:08<11:28, 10.59s/it] 66%|██████▌   | 122/186 [21:19<11:23, 10.67s/it] 66%|██████▌   | 123/186 [21:30<11:23, 10.84s/it] 67%|██████▋   | 124/186 [21:41<11:13, 10.87s/it] 67%|██████▋   | 125/186 [21:51<10:57, 10.78s/it] 68%|██████▊   | 126/186 [22:02<10:40, 10.68s/it] 68%|██████▊   | 127/186 [22:13<10:31, 10.70s/it] 69%|██████▉   | 128/186 [22:23<10:17, 10.65s/it] 69%|██████▉   | 129/186 [22:34<10:04, 10.61s/it] 70%|██████▉   | 130/186 [22:44<09:53, 10.59s/it] 70%|███████   | 131/186 [22:55<09:45, 10.65s/it] 71%|███████   | 132/186 [23:06<09:35, 10.65s/it] 72%|███████▏  | 133/186 [23:16<09:24, 10.65s/it] 72%|███████▏  | 134/186 [23:27<09:13, 10.65s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 73%|███████▎  | 135/186 [23:38<09:04, 10.69s/it] 73%|███████▎  | 136/186 [23:48<08:55, 10.72s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 74%|███████▎  | 137/186 [24:00<08:51, 10.85s/it] 74%|███████▍  | 138/186 [24:11<08:41, 10.86s/it] 75%|███████▍  | 139/186 [24:21<08:31, 10.89s/it] 75%|███████▌  | 140/186 [24:32<08:18, 10.84s/it] 76%|███████▌  | 141/186 [24:43<08:04, 10.77s/it] 76%|███████▋  | 142/186 [24:53<07:51, 10.71s/it] 77%|███████▋  | 143/186 [25:04<07:41, 10.73s/it] 77%|███████▋  | 144/186 [25:15<07:30, 10.73s/it] 78%|███████▊  | 145/186 [25:26<07:24, 10.84s/it] 78%|███████▊  | 146/186 [25:37<07:12, 10.81s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 79%|███████▉  | 147/186 [25:47<06:57, 10.71s/it] 80%|███████▉  | 148/186 [25:58<06:45, 10.66s/it] 80%|████████  | 149/186 [26:08<06:33, 10.63s/it] 81%|████████  | 150/186 [26:19<06:23, 10.64s/it] 81%|████████  | 151/186 [26:30<06:14, 10.70s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 82%|████████▏ | 152/186 [26:41<06:05, 10.74s/it] 82%|████████▏ | 153/186 [26:51<05:53, 10.71s/it] 83%|████████▎ | 154/186 [27:02<05:41, 10.66s/it] 83%|████████▎ | 155/186 [27:13<05:31, 10.68s/it] 84%|████████▍ | 156/186 [27:24<05:24, 10.81s/it] 84%|████████▍ | 157/186 [27:34<05:12, 10.77s/it] 85%|████████▍ | 158/186 [27:45<05:00, 10.74s/it] 85%|████████▌ | 159/186 [27:56<04:54, 10.90s/it] 86%|████████▌ | 160/186 [28:07<04:44, 10.93s/it] 87%|████████▋ | 161/186 [28:18<04:32, 10.88s/it] 87%|████████▋ | 162/186 [28:29<04:23, 10.97s/it] 88%|████████▊ | 163/186 [28:40<04:11, 10.95s/it] 88%|████████▊ | 164/186 [28:51<03:57, 10.81s/it] 89%|████████▊ | 165/186 [29:01<03:46, 10.77s/it] 89%|████████▉ | 166/186 [29:12<03:34, 10.72s/it] 90%|████████▉ | 167/186 [29:23<03:23, 10.69s/it] 90%|█████████ | 168/186 [29:33<03:12, 10.69s/it] 91%|█████████ | 169/186 [29:44<03:01, 10.67s/it] 91%|█████████▏| 170/186 [29:55<02:51, 10.71s/it] 92%|█████████▏| 171/186 [30:06<02:42, 10.82s/it] 92%|█████████▏| 172/186 [30:17<02:32, 10.86s/it] 93%|█████████▎| 173/186 [30:28<02:21, 10.88s/it] 94%|█████████▎| 174/186 [30:38<02:10, 10.86s/it] 94%|█████████▍| 175/186 [30:49<01:59, 10.83s/it] 95%|█████████▍| 176/186 [31:00<01:48, 10.81s/it] 95%|█████████▌| 177/186 [31:11<01:37, 10.83s/it] 96%|█████████▌| 178/186 [31:22<01:26, 10.81s/it] 96%|█████████▌| 179/186 [31:32<01:15, 10.82s/it] 97%|█████████▋| 180/186 [31:43<01:05, 10.86s/it] 97%|█████████▋| 181/186 [31:54<00:54, 10.81s/it] 98%|█████████▊| 182/186 [32:05<00:43, 10.86s/it] 98%|█████████▊| 183/186 [32:16<00:32, 10.86s/it] 99%|█████████▉| 184/186 [32:27<00:21, 10.85s/it] 99%|█████████▉| 185/186 [32:38<00:10, 10.87s/it]100%|██████████| 186/186 [32:48<00:00, 10.82s/it]                                                 {'train_runtime': 1968.8449, 'train_samples_per_second': 6.824, 'train_steps_per_second': 0.094, 'train_loss': 0.16619343911447831, 'epoch': 1.0}
100%|██████████| 186/186 [32:48<00:00, 10.82s/it]100%|██████████| 186/186 [32:48<00:00, 10.59s/it]
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-1
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33msliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-1[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251017_034745-3xmsw645/logs[0m
[2025-10-17 04:22:18,999] [INFO] [launch.py:347:main] Process 2192485 exits successfully.
[2025-10-17 04:22:18,999] [INFO] [launch.py:347:main] Process 2192483 exits successfully.
[2025-10-17 04:22:20,001] [INFO] [launch.py:347:main] Process 2192484 exits successfully.
[2025-10-17 04:22:20,001] [INFO] [launch.py:347:main] Process 2192481 exits successfully.
[2025-10-17 04:22:20,001] [INFO] [launch.py:347:main] Process 2192482 exits successfully.
[2025-10-17 04:22:24,005] [INFO] [launch.py:347:main] Process 2192480 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 04:22:27,248] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 04:22:27,803] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-17 04:22:27,803] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12423 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-1 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-1 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 2 --use_anchor False --use_sample_weight True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 04:22:29,525] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 04:22:30,114] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-17 04:22:30,114] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-17 04:22:30,114] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-17 04:22:30,114] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-17 04:22:30,114] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 04:22:34,524] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 04:22:34,660] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 04:22:34,834] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 04:22:34,970] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 04:22:35,481] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 04:22:35,504] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 04:22:35,616] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-17 04:22:35,658] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 04:22:35,815] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-17 04:22:35,829] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 04:22:35,829] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 04:22:35,943] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 04:22:35,973] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: creating run
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251017_042236-q5mt4r27
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-1
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/q5mt4r27
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.52s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.30s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.50s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.44s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.75s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.91s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.27s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.38s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.20s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.31s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.59s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.54s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.72s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.82s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.68s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.99s/it]
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.35s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.93s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.25s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.56s/it]
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.30s/it]
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.41s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.30s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.35s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.96s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.31s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.88s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.12s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.88s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.21s/it]
Formatting inputs...Skip in lazy mode
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:2')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:4')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:5')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:3')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:1')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:0')
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.011258840560913086 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.01039266586303711 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10127639770507812 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10364961624145508 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.011122465133666992 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10097765922546387 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/186 [00:10<31:24, 10.19s/it]  1%|          | 2/186 [00:18<28:41,  9.36s/it]  2%|▏         | 3/186 [00:28<28:17,  9.28s/it]  2%|▏         | 4/186 [00:37<28:29,  9.40s/it]  3%|▎         | 5/186 [00:47<28:36,  9.48s/it]  3%|▎         | 6/186 [00:57<28:54,  9.64s/it]  4%|▍         | 7/186 [01:07<29:08,  9.77s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1233 > 1048). Running this sequence through the model will result in indexing errors
  4%|▍         | 8/186 [01:17<29:32,  9.96s/it]  5%|▍         | 9/186 [01:28<29:54, 10.14s/it]  5%|▌         | 10/186 [01:38<29:58, 10.22s/it]  6%|▌         | 11/186 [01:49<30:00, 10.29s/it]  6%|▋         | 12/186 [01:59<29:51, 10.30s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
  7%|▋         | 13/186 [02:10<29:58, 10.39s/it]  8%|▊         | 14/186 [02:20<29:54, 10.43s/it]  8%|▊         | 15/186 [02:30<29:41, 10.42s/it]  9%|▊         | 16/186 [02:41<29:35, 10.44s/it]  9%|▉         | 17/186 [02:51<29:20, 10.42s/it] 10%|▉         | 18/186 [03:02<29:07, 10.40s/it] 10%|█         | 19/186 [03:12<29:07, 10.46s/it] 11%|█         | 20/186 [03:23<28:57, 10.46s/it] 11%|█▏        | 21/186 [03:33<28:48, 10.47s/it] 12%|█▏        | 22/186 [03:44<28:39, 10.49s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1263 > 1048). Running this sequence through the model will result in indexing errors
 12%|█▏        | 23/186 [03:55<28:52, 10.63s/it] 13%|█▎        | 24/186 [04:06<28:53, 10.70s/it] 13%|█▎        | 25/186 [04:16<28:38, 10.68s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1143 > 1048). Running this sequence through the model will result in indexing errors
 14%|█▍        | 26/186 [04:27<28:29, 10.69s/it] 15%|█▍        | 27/186 [04:37<28:16, 10.67s/it] 15%|█▌        | 28/186 [04:48<28:00, 10.63s/it] 16%|█▌        | 29/186 [04:59<27:44, 10.60s/it] 16%|█▌        | 30/186 [05:09<27:38, 10.63s/it] 17%|█▋        | 31/186 [05:20<27:34, 10.68s/it] 17%|█▋        | 32/186 [05:31<27:18, 10.64s/it] 18%|█▊        | 33/186 [05:41<27:05, 10.62s/it] 18%|█▊        | 34/186 [05:52<27:04, 10.69s/it] 19%|█▉        | 35/186 [06:03<26:58, 10.72s/it] 19%|█▉        | 36/186 [06:14<26:55, 10.77s/it] 20%|█▉        | 37/186 [06:24<26:38, 10.73s/it] 20%|██        | 38/186 [06:35<26:16, 10.65s/it] 21%|██        | 39/186 [06:45<26:03, 10.64s/it] 22%|██▏       | 40/186 [06:56<25:48, 10.61s/it] 22%|██▏       | 41/186 [07:07<25:45, 10.66s/it] 23%|██▎       | 42/186 [07:17<25:34, 10.65s/it] 23%|██▎       | 43/186 [07:28<25:24, 10.66s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 24%|██▎       | 44/186 [07:39<25:09, 10.63s/it] 24%|██▍       | 45/186 [07:49<25:07, 10.69s/it] 25%|██▍       | 46/186 [08:00<24:53, 10.67s/it] 25%|██▌       | 47/186 [08:11<24:56, 10.77s/it] 26%|██▌       | 48/186 [08:22<24:41, 10.74s/it] 26%|██▋       | 49/186 [08:32<24:26, 10.70s/it] 27%|██▋       | 50/186 [08:43<24:14, 10.69s/it] 27%|██▋       | 51/186 [08:54<24:01, 10.68s/it] 28%|██▊       | 52/186 [09:04<23:45, 10.64s/it] 28%|██▊       | 53/186 [09:15<23:27, 10.58s/it] 29%|██▉       | 54/186 [09:26<23:26, 10.66s/it] 30%|██▉       | 55/186 [09:36<23:14, 10.65s/it] 30%|███       | 56/186 [09:47<22:57, 10.60s/it] 31%|███       | 57/186 [09:57<22:45, 10.59s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 31%|███       | 58/186 [10:08<22:39, 10.62s/it] 32%|███▏      | 59/186 [10:19<22:42, 10.73s/it] 32%|███▏      | 60/186 [10:30<22:32, 10.74s/it] 33%|███▎      | 61/186 [10:40<22:17, 10.70s/it] 33%|███▎      | 62/186 [10:51<22:10, 10.73s/it] 34%|███▍      | 63/186 [11:02<21:58, 10.72s/it] 34%|███▍      | 64/186 [11:12<21:45, 10.70s/it] 35%|███▍      | 65/186 [11:23<21:28, 10.65s/it] 35%|███▌      | 66/186 [11:34<21:30, 10.75s/it] 36%|███▌      | 67/186 [11:45<21:25, 10.80s/it] 37%|███▋      | 68/186 [11:55<21:09, 10.76s/it] 37%|███▋      | 69/186 [12:07<21:09, 10.85s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 38%|███▊      | 70/186 [12:17<21:02, 10.89s/it] 38%|███▊      | 71/186 [12:28<20:40, 10.79s/it] 39%|███▊      | 72/186 [12:39<20:28, 10.78s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 39%|███▉      | 73/186 [12:49<20:11, 10.72s/it] 40%|███▉      | 74/186 [13:00<19:50, 10.63s/it] 40%|████      | 75/186 [13:10<19:39, 10.62s/it] 41%|████      | 76/186 [13:21<19:27, 10.62s/it] 41%|████▏     | 77/186 [13:32<19:17, 10.62s/it] 42%|████▏     | 78/186 [13:42<19:03, 10.59s/it] 42%|████▏     | 79/186 [13:53<18:48, 10.55s/it] 43%|████▎     | 80/186 [14:03<18:42, 10.59s/it] 44%|████▎     | 81/186 [14:14<18:35, 10.62s/it]WARNING: tokenization mismatch: 1 vs. 48. (ignored)
 44%|████▍     | 82/186 [14:25<18:21, 10.59s/it] 45%|████▍     | 83/186 [14:35<18:14, 10.63s/it] 45%|████▌     | 84/186 [14:46<18:02, 10.61s/it] 46%|████▌     | 85/186 [14:56<17:50, 10.60s/it] 46%|████▌     | 86/186 [15:07<17:37, 10.57s/it] 47%|████▋     | 87/186 [15:18<17:28, 10.59s/it] 47%|████▋     | 88/186 [15:28<17:22, 10.64s/it] 48%|████▊     | 89/186 [15:39<17:09, 10.61s/it] 48%|████▊     | 90/186 [15:50<17:03, 10.66s/it] 49%|████▉     | 91/186 [16:00<16:49, 10.63s/it] 49%|████▉     | 92/186 [16:11<16:44, 10.69s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1125 > 1048). Running this sequence through the model will result in indexing errors
 50%|█████     | 93/186 [16:22<16:50, 10.87s/it] 51%|█████     | 94/186 [16:33<16:41, 10.89s/it] 51%|█████     | 95/186 [16:44<16:27, 10.85s/it] 52%|█████▏    | 96/186 [16:55<16:14, 10.83s/it] 52%|█████▏    | 97/186 [17:05<16:01, 10.81s/it] 53%|█████▎    | 98/186 [17:16<15:52, 10.82s/it] 53%|█████▎    | 99/186 [17:27<15:41, 10.82s/it] 54%|█████▍    | 100/186 [17:38<15:24, 10.75s/it] 54%|█████▍    | 101/186 [17:48<15:11, 10.72s/it] 55%|█████▍    | 102/186 [17:59<15:01, 10.74s/it] 55%|█████▌    | 103/186 [18:10<14:56, 10.80s/it] 56%|█████▌    | 104/186 [18:21<14:39, 10.72s/it] 56%|█████▋    | 105/186 [18:31<14:26, 10.70s/it] 57%|█████▋    | 106/186 [18:42<14:14, 10.68s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 58%|█████▊    | 107/186 [18:53<14:01, 10.65s/it] 58%|█████▊    | 108/186 [19:03<13:51, 10.67s/it] 59%|█████▊    | 109/186 [19:14<13:43, 10.69s/it] 59%|█████▉    | 110/186 [19:24<13:28, 10.64s/it] 60%|█████▉    | 111/186 [19:35<13:22, 10.70s/it] 60%|██████    | 112/186 [19:46<13:17, 10.78s/it] 61%|██████    | 113/186 [19:57<13:10, 10.83s/it] 61%|██████▏   | 114/186 [20:08<12:59, 10.82s/it] 62%|██████▏   | 115/186 [20:19<12:46, 10.79s/it] 62%|██████▏   | 116/186 [20:30<12:36, 10.80s/it] 63%|██████▎   | 117/186 [20:40<12:25, 10.80s/it] 63%|██████▎   | 118/186 [20:51<12:13, 10.78s/it] 64%|██████▍   | 119/186 [21:02<11:58, 10.72s/it] 65%|██████▍   | 120/186 [21:12<11:43, 10.67s/it] 65%|██████▌   | 121/186 [21:23<11:34, 10.69s/it] 66%|██████▌   | 122/186 [21:34<11:25, 10.70s/it] 66%|██████▌   | 123/186 [21:45<11:19, 10.79s/it] 67%|██████▋   | 124/186 [21:56<11:08, 10.78s/it] 67%|██████▋   | 125/186 [22:06<10:55, 10.75s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 68%|██████▊   | 126/186 [22:17<10:42, 10.70s/it] 68%|██████▊   | 127/186 [22:27<10:31, 10.70s/it] 69%|██████▉   | 128/186 [22:38<10:23, 10.75s/it] 69%|██████▉   | 129/186 [22:49<10:16, 10.82s/it] 70%|██████▉   | 130/186 [23:00<10:03, 10.78s/it] 70%|███████   | 131/186 [23:11<09:51, 10.76s/it] 71%|███████   | 132/186 [23:22<09:42, 10.79s/it] 72%|███████▏  | 133/186 [23:32<09:33, 10.82s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 72%|███████▏  | 134/186 [23:43<09:22, 10.82s/it] 73%|███████▎  | 135/186 [23:54<09:10, 10.80s/it] 73%|███████▎  | 136/186 [24:05<09:00, 10.80s/it] 74%|███████▎  | 137/186 [24:16<08:50, 10.83s/it] 74%|███████▍  | 138/186 [24:27<08:44, 10.93s/it] 75%|███████▍  | 139/186 [24:38<08:35, 10.97s/it] 75%|███████▌  | 140/186 [24:49<08:22, 10.92s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 76%|███████▌  | 141/186 [25:00<08:09, 10.89s/it] 76%|███████▋  | 142/186 [25:10<07:58, 10.86s/it] 77%|███████▋  | 143/186 [25:21<07:47, 10.86s/it] 77%|███████▋  | 144/186 [25:32<07:35, 10.86s/it] 78%|███████▊  | 145/186 [25:43<07:22, 10.78s/it] 78%|███████▊  | 146/186 [25:53<07:09, 10.73s/it] 79%|███████▉  | 147/186 [26:04<06:58, 10.74s/it] 80%|███████▉  | 148/186 [26:15<06:46, 10.70s/it] 80%|████████  | 149/186 [26:26<06:37, 10.73s/it] 81%|████████  | 150/186 [26:36<06:26, 10.72s/it] 81%|████████  | 151/186 [26:47<06:15, 10.74s/it] 82%|████████▏ | 152/186 [26:58<06:03, 10.68s/it] 82%|████████▏ | 153/186 [27:08<05:52, 10.68s/it] 83%|████████▎ | 154/186 [27:19<05:41, 10.66s/it] 83%|████████▎ | 155/186 [27:30<05:31, 10.70s/it] 84%|████████▍ | 156/186 [27:41<05:25, 10.85s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1074 > 1048). Running this sequence through the model will result in indexing errors
 84%|████████▍ | 157/186 [27:51<05:13, 10.80s/it] 85%|████████▍ | 158/186 [28:02<05:03, 10.84s/it] 85%|████████▌ | 159/186 [28:13<04:52, 10.83s/it] 86%|████████▌ | 160/186 [28:24<04:40, 10.79s/it] 87%|████████▋ | 161/186 [28:34<04:28, 10.73s/it] 87%|████████▋ | 162/186 [28:45<04:18, 10.79s/it] 88%|████████▊ | 163/186 [28:56<04:08, 10.82s/it] 88%|████████▊ | 164/186 [29:07<03:57, 10.79s/it] 89%|████████▊ | 165/186 [29:18<03:45, 10.76s/it] 89%|████████▉ | 166/186 [29:28<03:34, 10.73s/it] 90%|████████▉ | 167/186 [29:39<03:23, 10.73s/it] 90%|█████████ | 168/186 [29:50<03:13, 10.73s/it] 91%|█████████ | 169/186 [30:00<03:01, 10.69s/it] 91%|█████████▏| 170/186 [30:11<02:51, 10.72s/it] 92%|█████████▏| 171/186 [30:22<02:42, 10.82s/it] 92%|█████████▏| 172/186 [30:33<02:32, 10.86s/it] 93%|█████████▎| 173/186 [30:44<02:20, 10.82s/it] 94%|█████████▎| 174/186 [30:55<02:09, 10.78s/it] 94%|█████████▍| 175/186 [31:05<01:58, 10.79s/it] 95%|█████████▍| 176/186 [31:16<01:47, 10.80s/it] 95%|█████████▌| 177/186 [31:27<01:38, 10.90s/it] 96%|█████████▌| 178/186 [31:38<01:26, 10.78s/it] 96%|█████████▌| 179/186 [31:49<01:15, 10.86s/it] 97%|█████████▋| 180/186 [32:00<01:05, 10.85s/it] 97%|█████████▋| 181/186 [32:11<00:54, 10.82s/it] 98%|█████████▊| 182/186 [32:22<00:43, 10.87s/it] 98%|█████████▊| 183/186 [32:33<00:32, 10.96s/it] 99%|█████████▉| 184/186 [32:43<00:21, 10.91s/it] 99%|█████████▉| 185/186 [32:54<00:10, 10.92s/it]100%|██████████| 186/186 [33:05<00:00, 10.92s/it]                                                 {'train_runtime': 1985.8361, 'train_samples_per_second': 6.765, 'train_steps_per_second': 0.094, 'train_loss': 0.15638146349178847, 'epoch': 1.0}
100%|██████████| 186/186 [33:05<00:00, 10.92s/it]100%|██████████| 186/186 [33:05<00:00, 10.68s/it]
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-1
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33msliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-1[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251017_042236-q5mt4r27/logs[0m
[2025-10-17 04:57:31,523] [INFO] [launch.py:347:main] Process 2194216 exits successfully.
[2025-10-17 04:57:31,523] [INFO] [launch.py:347:main] Process 2194218 exits successfully.
[2025-10-17 04:57:31,524] [INFO] [launch.py:347:main] Process 2194215 exits successfully.
[2025-10-17 04:57:32,525] [INFO] [launch.py:347:main] Process 2194217 exits successfully.
[2025-10-17 04:57:32,525] [INFO] [launch.py:347:main] Process 2194219 exits successfully.
[2025-10-17 04:57:36,530] [INFO] [launch.py:347:main] Process 2194214 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 04:57:39,817] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 04:57:40,370] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-17 04:57:40,370] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12423 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1.5e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-1 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-1 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 0.5 --use_anchor False --use_sample_weight True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 04:57:42,021] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 04:57:42,602] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-17 04:57:42,602] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-17 04:57:42,602] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-17 04:57:42,602] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-17 04:57:42,602] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 04:57:46,210] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 04:57:46,221] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 04:57:46,222] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 04:57:46,245] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 04:57:46,534] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 04:57:46,543] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 04:57:46,560] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 04:57:46,574] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 04:57:46,690] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 04:57:46,996] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 04:57:48,335] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 04:57:48,656] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 04:57:48,656] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251017_045749-d74yaxtq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-1
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/d74yaxtq
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.24s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.20s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.08s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.08s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.01s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.08s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.13s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.18s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.84s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.18s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.19s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.16s/it]
to bfloat16...
to bfloat16...
to bfloat16...
to bfloat16...
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.58s/it]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.14s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.51s/it]
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.11s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.23s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.51s/it]
Formatting inputs...Skip in lazy mode
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.38s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.26s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.30s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.72s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.10s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.89s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.25s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.30s/it]
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:0')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:1')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:3')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:4')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:5')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:2')
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.010915279388427734 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...

Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.014228105545043945 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.013189315795898438 seconds
Loading extension module fused_adam...Loading extension module fused_adam...

Loading extension module fused_adam...
Time to load fused_adam op: 0.10161876678466797 seconds
Time to load fused_adam op: 0.10194969177246094 seconds
Time to load fused_adam op: 0.10153961181640625 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/186 [00:13<41:38, 13.51s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1075 > 1048). Running this sequence through the model will result in indexing errors
  1%|          | 2/186 [00:22<33:02, 10.77s/it]  2%|▏         | 3/186 [00:31<30:38, 10.05s/it]  2%|▏         | 4/186 [00:41<29:55,  9.86s/it]  3%|▎         | 5/186 [00:50<29:02,  9.63s/it]  3%|▎         | 6/186 [00:59<28:43,  9.57s/it]  4%|▍         | 7/186 [01:09<28:54,  9.69s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
  4%|▍         | 8/186 [01:19<29:00,  9.78s/it]  5%|▍         | 9/186 [01:29<29:03,  9.85s/it]  5%|▌         | 10/186 [01:39<29:08,  9.94s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1245 > 1048). Running this sequence through the model will result in indexing errors
  6%|▌         | 11/186 [01:50<29:11, 10.01s/it]  6%|▋         | 12/186 [02:00<29:18, 10.10s/it]  7%|▋         | 13/186 [02:10<29:19, 10.17s/it]  8%|▊         | 14/186 [02:20<29:09, 10.17s/it]  8%|▊         | 15/186 [02:31<29:08, 10.23s/it]  9%|▊         | 16/186 [02:41<29:13, 10.32s/it]  9%|▉         | 17/186 [02:52<29:10, 10.36s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1080 > 1048). Running this sequence through the model will result in indexing errors
 10%|▉         | 18/186 [03:02<29:08, 10.41s/it] 10%|█         | 19/186 [03:13<28:52, 10.37s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 11%|█         | 20/186 [03:23<28:54, 10.45s/it] 11%|█▏        | 21/186 [03:33<28:35, 10.40s/it] 12%|█▏        | 22/186 [03:44<28:20, 10.37s/it] 12%|█▏        | 23/186 [03:54<28:15, 10.40s/it] 13%|█▎        | 24/186 [04:05<28:14, 10.46s/it] 13%|█▎        | 25/186 [04:16<28:21, 10.57s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1065 > 1048). Running this sequence through the model will result in indexing errors
 14%|█▍        | 26/186 [04:26<28:09, 10.56s/it] 15%|█▍        | 27/186 [04:37<28:15, 10.66s/it] 15%|█▌        | 28/186 [04:48<27:56, 10.61s/it] 16%|█▌        | 29/186 [04:58<27:50, 10.64s/it] 16%|█▌        | 30/186 [05:09<27:47, 10.69s/it] 17%|█▋        | 31/186 [05:20<27:45, 10.75s/it] 17%|█▋        | 32/186 [05:31<27:46, 10.82s/it] 18%|█▊        | 33/186 [05:42<27:27, 10.77s/it] 18%|█▊        | 34/186 [05:52<27:20, 10.80s/it] 19%|█▉        | 35/186 [06:03<27:12, 10.81s/it] 19%|█▉        | 36/186 [06:14<27:07, 10.85s/it] 20%|█▉        | 37/186 [06:25<26:50, 10.81s/it] 20%|██        | 38/186 [06:36<26:39, 10.81s/it] 21%|██        | 39/186 [06:47<26:34, 10.85s/it] 22%|██▏       | 40/186 [06:58<26:23, 10.84s/it] 22%|██▏       | 41/186 [07:08<26:09, 10.82s/it] 23%|██▎       | 42/186 [07:19<25:54, 10.80s/it] 23%|██▎       | 43/186 [07:30<25:47, 10.82s/it] 24%|██▎       | 44/186 [07:41<25:36, 10.82s/it] 24%|██▍       | 45/186 [07:51<25:20, 10.78s/it] 25%|██▍       | 46/186 [08:02<25:02, 10.73s/it] 25%|██▌       | 47/186 [08:13<25:03, 10.82s/it] 26%|██▌       | 48/186 [08:24<24:51, 10.81s/it] 26%|██▋       | 49/186 [08:35<24:47, 10.86s/it] 27%|██▋       | 50/186 [08:45<24:28, 10.80s/it] 27%|██▋       | 51/186 [08:56<24:24, 10.85s/it] 28%|██▊       | 52/186 [09:07<24:11, 10.83s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1125 > 1048). Running this sequence through the model will result in indexing errors
 28%|██▊       | 53/186 [09:18<24:02, 10.85s/it] 29%|██▉       | 54/186 [09:29<23:49, 10.83s/it] 30%|██▉       | 55/186 [09:40<23:40, 10.84s/it] 30%|███       | 56/186 [09:51<23:33, 10.88s/it] 31%|███       | 57/186 [10:02<23:21, 10.87s/it] 31%|███       | 58/186 [10:13<23:15, 10.90s/it] 32%|███▏      | 59/186 [10:24<23:06, 10.92s/it] 32%|███▏      | 60/186 [10:35<22:58, 10.94s/it] 33%|███▎      | 61/186 [10:46<22:53, 10.99s/it] 33%|███▎      | 62/186 [10:57<22:40, 10.98s/it] 34%|███▍      | 63/186 [11:08<22:37, 11.04s/it] 34%|███▍      | 64/186 [11:19<22:18, 10.97s/it] 35%|███▍      | 65/186 [11:29<22:01, 10.92s/it] 35%|███▌      | 66/186 [11:41<22:19, 11.16s/it] 36%|███▌      | 67/186 [11:52<21:52, 11.03s/it] 37%|███▋      | 68/186 [12:03<21:41, 11.03s/it] 37%|███▋      | 69/186 [12:14<21:29, 11.02s/it] 38%|███▊      | 70/186 [12:25<21:14, 10.99s/it] 38%|███▊      | 71/186 [12:36<21:03, 10.98s/it] 39%|███▊      | 72/186 [12:47<20:46, 10.94s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1285 > 1048). Running this sequence through the model will result in indexing errors
 39%|███▉      | 73/186 [12:58<20:39, 10.97s/it] 40%|███▉      | 74/186 [13:08<20:24, 10.93s/it]WARNING: tokenization mismatch: 1 vs. 48. (ignored)
 40%|████      | 75/186 [13:19<20:08, 10.88s/it] 41%|████      | 76/186 [13:30<20:05, 10.95s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 41%|████▏     | 77/186 [13:42<20:02, 11.03s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
 42%|████▏     | 78/186 [13:52<19:41, 10.94s/it] 42%|████▏     | 79/186 [14:03<19:21, 10.85s/it] 43%|████▎     | 80/186 [14:14<19:18, 10.93s/it] 44%|████▎     | 81/186 [14:25<19:16, 11.01s/it] 44%|████▍     | 82/186 [14:36<19:02, 10.98s/it] 45%|████▍     | 83/186 [14:47<18:55, 11.02s/it] 45%|████▌     | 84/186 [14:58<18:35, 10.93s/it] 46%|████▌     | 85/186 [15:09<18:25, 10.95s/it] 46%|████▌     | 86/186 [15:20<18:05, 10.86s/it] 47%|████▋     | 87/186 [15:31<17:59, 10.90s/it] 47%|████▋     | 88/186 [15:41<17:42, 10.84s/it] 48%|████▊     | 89/186 [15:52<17:41, 10.95s/it] 48%|████▊     | 90/186 [16:04<17:34, 10.99s/it] 49%|████▉     | 91/186 [16:15<17:24, 10.99s/it] 49%|████▉     | 92/186 [16:25<17:03, 10.89s/it] 50%|█████     | 93/186 [16:36<16:56, 10.93s/it] 51%|█████     | 94/186 [16:47<16:42, 10.89s/it] 51%|█████     | 95/186 [16:58<16:41, 11.00s/it] 52%|█████▏    | 96/186 [17:09<16:27, 10.97s/it] 52%|█████▏    | 97/186 [17:20<16:13, 10.94s/it] 53%|█████▎    | 98/186 [17:31<16:09, 11.02s/it] 53%|█████▎    | 99/186 [17:42<15:57, 11.01s/it] 54%|█████▍    | 100/186 [17:53<15:47, 11.02s/it] 54%|█████▍    | 101/186 [18:04<15:36, 11.01s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 55%|█████▍    | 102/186 [18:15<15:24, 11.00s/it] 55%|█████▌    | 103/186 [18:27<15:19, 11.07s/it] 56%|█████▌    | 104/186 [18:37<15:00, 10.98s/it] 56%|█████▋    | 105/186 [18:49<14:54, 11.05s/it] 57%|█████▋    | 106/186 [18:59<14:37, 10.97s/it] 58%|█████▊    | 107/186 [19:10<14:25, 10.95s/it] 58%|█████▊    | 108/186 [19:21<14:10, 10.90s/it] 59%|█████▊    | 109/186 [19:32<13:58, 10.89s/it] 59%|█████▉    | 110/186 [19:43<13:56, 11.00s/it] 60%|█████▉    | 111/186 [19:54<13:47, 11.04s/it] 60%|██████    | 112/186 [20:05<13:33, 10.99s/it] 61%|██████    | 113/186 [20:16<13:21, 10.98s/it] 61%|██████▏   | 114/186 [20:27<13:03, 10.88s/it] 62%|██████▏   | 115/186 [20:37<12:49, 10.84s/it] 62%|██████▏   | 116/186 [20:48<12:36, 10.81s/it] 63%|██████▎   | 117/186 [20:59<12:26, 10.82s/it] 63%|██████▎   | 118/186 [21:10<12:25, 10.96s/it] 64%|██████▍   | 119/186 [21:21<12:16, 10.99s/it] 65%|██████▍   | 120/186 [21:32<12:01, 10.93s/it] 65%|██████▌   | 121/186 [21:43<11:45, 10.86s/it] 66%|██████▌   | 122/186 [21:54<11:35, 10.87s/it] 66%|██████▌   | 123/186 [22:05<11:28, 10.93s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 67%|██████▋   | 124/186 [22:16<11:19, 10.96s/it] 67%|██████▋   | 125/186 [22:27<11:05, 10.91s/it] 68%|██████▊   | 126/186 [22:38<10:55, 10.93s/it] 68%|██████▊   | 127/186 [22:49<10:51, 11.04s/it] 69%|██████▉   | 128/186 [23:00<10:40, 11.05s/it] 69%|██████▉   | 129/186 [23:11<10:22, 10.92s/it] 70%|██████▉   | 130/186 [23:22<10:16, 11.02s/it] 70%|███████   | 131/186 [23:33<10:04, 10.98s/it] 71%|███████   | 132/186 [23:44<09:57, 11.06s/it] 72%|███████▏  | 133/186 [23:55<09:42, 10.99s/it] 72%|███████▏  | 134/186 [24:06<09:28, 10.94s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
 73%|███████▎  | 135/186 [24:16<09:13, 10.86s/it] 73%|███████▎  | 136/186 [24:27<09:01, 10.83s/it] 74%|███████▎  | 137/186 [24:38<08:57, 10.97s/it] 74%|███████▍  | 138/186 [24:49<08:44, 10.92s/it] 75%|███████▍  | 139/186 [25:01<08:39, 11.05s/it] 75%|███████▌  | 140/186 [25:12<08:29, 11.08s/it] 76%|███████▌  | 141/186 [25:23<08:15, 11.01s/it] 76%|███████▋  | 142/186 [25:33<08:03, 10.98s/it] 77%|███████▋  | 143/186 [25:44<07:49, 10.91s/it] 77%|███████▋  | 144/186 [25:55<07:35, 10.86s/it] 78%|███████▊  | 145/186 [26:06<07:24, 10.85s/it] 78%|███████▊  | 146/186 [26:17<07:12, 10.82s/it] 79%|███████▉  | 147/186 [26:27<07:01, 10.80s/it] 80%|███████▉  | 148/186 [26:38<06:52, 10.87s/it] 80%|████████  | 149/186 [26:49<06:41, 10.84s/it] 81%|████████  | 150/186 [27:00<06:30, 10.84s/it] 81%|████████  | 151/186 [27:11<06:20, 10.87s/it] 82%|████████▏ | 152/186 [27:22<06:12, 10.95s/it] 82%|████████▏ | 153/186 [27:33<06:03, 11.00s/it] 83%|████████▎ | 154/186 [27:44<05:51, 10.99s/it] 83%|████████▎ | 155/186 [27:55<05:39, 10.94s/it] 84%|████████▍ | 156/186 [28:06<05:30, 11.03s/it] 84%|████████▍ | 157/186 [28:17<05:19, 11.03s/it] 85%|████████▍ | 158/186 [28:28<05:08, 11.04s/it] 85%|████████▌ | 159/186 [28:39<04:55, 10.95s/it] 86%|████████▌ | 160/186 [28:50<04:43, 10.91s/it] 87%|████████▋ | 161/186 [29:01<04:32, 10.89s/it] 87%|████████▋ | 162/186 [29:12<04:22, 10.95s/it] 88%|████████▊ | 163/186 [29:23<04:12, 10.98s/it] 88%|████████▊ | 164/186 [29:34<04:02, 11.02s/it] 89%|████████▊ | 165/186 [29:45<03:50, 10.99s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
 89%|████████▉ | 166/186 [29:56<03:39, 10.99s/it] 90%|████████▉ | 167/186 [30:07<03:28, 10.99s/it] 90%|█████████ | 168/186 [30:18<03:19, 11.10s/it] 91%|█████████ | 169/186 [30:29<03:08, 11.09s/it] 91%|█████████▏| 170/186 [30:40<02:58, 11.14s/it] 92%|█████████▏| 171/186 [30:52<02:48, 11.23s/it] 92%|█████████▏| 172/186 [31:03<02:36, 11.17s/it] 93%|█████████▎| 173/186 [31:14<02:25, 11.21s/it] 94%|█████████▎| 174/186 [31:25<02:13, 11.14s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
 94%|█████████▍| 175/186 [31:36<02:02, 11.11s/it] 95%|█████████▍| 176/186 [31:47<01:51, 11.11s/it] 95%|█████████▌| 177/186 [31:58<01:39, 11.09s/it] 96%|█████████▌| 178/186 [32:09<01:28, 11.08s/it] 96%|█████████▌| 179/186 [32:20<01:17, 11.03s/it] 97%|█████████▋| 180/186 [32:31<01:06, 11.03s/it] 97%|█████████▋| 181/186 [32:43<00:55, 11.06s/it] 98%|█████████▊| 182/186 [32:53<00:44, 11.01s/it] 98%|█████████▊| 183/186 [33:04<00:32, 11.00s/it] 99%|█████████▉| 184/186 [33:15<00:21, 10.94s/it] 99%|█████████▉| 185/186 [33:26<00:11, 11.02s/it]100%|██████████| 186/186 [33:37<00:00, 11.01s/it]                                                 {'train_runtime': 2017.8812, 'train_samples_per_second': 6.658, 'train_steps_per_second': 0.092, 'train_loss': 0.12948787853281984, 'epoch': 1.0}
100%|██████████| 186/186 [33:37<00:00, 11.01s/it]100%|██████████| 186/186 [33:37<00:00, 10.85s/it]
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-1
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33msliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-1[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251017_045749-d74yaxtq/logs[0m
[2025-10-17 05:33:12,116] [INFO] [launch.py:347:main] Process 2195950 exits successfully.
[2025-10-17 05:33:12,117] [INFO] [launch.py:347:main] Process 2195952 exits successfully.
[2025-10-17 05:33:12,117] [INFO] [launch.py:347:main] Process 2195953 exits successfully.
[2025-10-17 05:33:12,117] [INFO] [launch.py:347:main] Process 2195951 exits successfully.
[2025-10-17 05:33:13,119] [INFO] [launch.py:347:main] Process 2195949 exits successfully.
[2025-10-17 05:33:17,123] [INFO] [launch.py:347:main] Process 2195948 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 05:33:20,353] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 05:33:20,909] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-17 05:33:20,910] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12423 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1.5e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-1 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-1 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 1 --use_anchor False --use_sample_weight True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 05:33:22,372] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 05:33:22,938] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-17 05:33:22,938] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-17 05:33:22,938] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-17 05:33:22,938] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-17 05:33:22,938] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 05:33:26,552] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 05:33:26,892] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 05:33:26,893] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-10-17 05:33:27,105] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 05:33:27,144] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 05:33:27,412] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 05:33:27,471] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 05:33:27,501] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-17 05:33:27,747] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 05:33:27,828] [INFO] [comm.py:637:init_distributed] cdb=None
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 05:33:28,232] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-10-17 05:33:28,548] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 05:33:29,016] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: creating run
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251017_053329-2q8l0m7r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-1
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/2q8l0m7r
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.99s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.48s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.59s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.58s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.86s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.81s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.84s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.49s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.84s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.63s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.90s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.68s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.61s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.91s/it]
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.29s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.89s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.37s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.77s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.15s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.19s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.44s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.89s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.26s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.70s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.98s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.85s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.48s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.76s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.08s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.58s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.87s/it]
Formatting inputs...Skip in lazy mode
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:2')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:3')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:5')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:4')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:0')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:1')
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.010596752166748047 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...Building extension module fused_adam...

Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.01403188705444336 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10103368759155273 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.011479377746582031 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10260772705078125 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10151052474975586 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/186 [00:09<29:40,  9.63s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1153 > 1048). Running this sequence through the model will result in indexing errors
  1%|          | 2/186 [00:18<27:42,  9.04s/it]  2%|▏         | 3/186 [00:27<28:12,  9.25s/it]  2%|▏         | 4/186 [00:37<28:29,  9.39s/it]  3%|▎         | 5/186 [00:47<28:56,  9.59s/it]  3%|▎         | 6/186 [00:57<29:12,  9.74s/it]  4%|▍         | 7/186 [01:07<29:35,  9.92s/it]  4%|▍         | 8/186 [01:17<29:38,  9.99s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1240 > 1048). Running this sequence through the model will result in indexing errors
  5%|▍         | 9/186 [01:28<30:00, 10.18s/it]  5%|▌         | 10/186 [01:38<30:11, 10.30s/it]  6%|▌         | 11/186 [01:49<30:07, 10.33s/it]  6%|▋         | 12/186 [01:59<29:53, 10.31s/it]  7%|▋         | 13/186 [02:10<29:51, 10.36s/it]  8%|▊         | 14/186 [02:20<29:41, 10.36s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
Token indices sequence length is longer than the specified maximum sequence length for this model (1165 > 1048). Running this sequence through the model will result in indexing errors
  8%|▊         | 15/186 [02:31<29:50, 10.47s/it]  9%|▊         | 16/186 [02:41<29:42, 10.48s/it]  9%|▉         | 17/186 [02:52<29:34, 10.50s/it] 10%|▉         | 18/186 [03:02<29:17, 10.46s/it] 10%|█         | 19/186 [03:13<29:08, 10.47s/it] 11%|█         | 20/186 [03:23<28:59, 10.48s/it] 11%|█▏        | 21/186 [03:34<29:00, 10.55s/it] 12%|█▏        | 22/186 [03:44<28:52, 10.57s/it] 12%|█▏        | 23/186 [03:55<28:48, 10.61s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1065 > 1048). Running this sequence through the model will result in indexing errors
 13%|█▎        | 24/186 [04:06<28:44, 10.65s/it] 13%|█▎        | 25/186 [04:17<28:52, 10.76s/it] 14%|█▍        | 26/186 [04:27<28:36, 10.73s/it] 15%|█▍        | 27/186 [04:38<28:15, 10.66s/it] 15%|█▌        | 28/186 [04:49<28:06, 10.67s/it] 16%|█▌        | 29/186 [04:59<27:48, 10.63s/it] 16%|█▌        | 30/186 [05:10<27:31, 10.59s/it] 17%|█▋        | 31/186 [05:20<27:20, 10.58s/it] 17%|█▋        | 32/186 [05:31<27:05, 10.56s/it] 18%|█▊        | 33/186 [05:42<27:05, 10.63s/it] 18%|█▊        | 34/186 [05:52<26:54, 10.62s/it] 19%|█▉        | 35/186 [06:03<27:08, 10.79s/it] 19%|█▉        | 36/186 [06:14<26:54, 10.76s/it] 20%|█▉        | 37/186 [06:25<26:37, 10.72s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1159 > 1048). Running this sequence through the model will result in indexing errors
 20%|██        | 38/186 [06:36<26:39, 10.81s/it] 21%|██        | 39/186 [06:46<26:27, 10.80s/it] 22%|██▏       | 40/186 [06:57<26:12, 10.77s/it] 22%|██▏       | 41/186 [07:08<25:57, 10.74s/it] 23%|██▎       | 42/186 [07:18<25:41, 10.71s/it] 23%|██▎       | 43/186 [07:29<25:28, 10.69s/it] 24%|██▎       | 44/186 [07:40<25:25, 10.74s/it] 24%|██▍       | 45/186 [07:51<25:15, 10.75s/it] 25%|██▍       | 46/186 [08:02<25:08, 10.77s/it] 25%|██▌       | 47/186 [08:12<24:57, 10.78s/it] 26%|██▌       | 48/186 [08:23<24:48, 10.79s/it] 26%|██▋       | 49/186 [08:34<24:32, 10.75s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 27%|██▋       | 50/186 [08:45<24:22, 10.76s/it] 27%|██▋       | 51/186 [08:55<24:18, 10.80s/it] 28%|██▊       | 52/186 [09:06<24:08, 10.81s/it] 28%|██▊       | 53/186 [09:17<24:02, 10.85s/it] 29%|██▉       | 54/186 [09:28<23:47, 10.81s/it] 30%|██▉       | 55/186 [09:39<23:31, 10.78s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 30%|███       | 56/186 [09:49<23:16, 10.74s/it] 31%|███       | 57/186 [10:00<23:09, 10.77s/it] 31%|███       | 58/186 [10:11<22:56, 10.75s/it] 32%|███▏      | 59/186 [10:22<22:53, 10.82s/it] 32%|███▏      | 60/186 [10:33<22:37, 10.77s/it] 33%|███▎      | 61/186 [10:43<22:25, 10.76s/it] 33%|███▎      | 62/186 [10:54<22:14, 10.77s/it] 34%|███▍      | 63/186 [11:05<22:03, 10.76s/it] 34%|███▍      | 64/186 [11:16<21:52, 10.76s/it] 35%|███▍      | 65/186 [11:26<21:38, 10.73s/it] 35%|███▌      | 66/186 [11:38<22:01, 11.01s/it] 36%|███▌      | 67/186 [11:49<21:38, 10.91s/it] 37%|███▋      | 68/186 [11:59<21:18, 10.83s/it] 37%|███▋      | 69/186 [12:10<21:13, 10.89s/it] 38%|███▊      | 70/186 [12:21<21:09, 10.94s/it] 38%|███▊      | 71/186 [12:32<20:51, 10.88s/it] 39%|███▊      | 72/186 [12:43<20:34, 10.83s/it] 39%|███▉      | 73/186 [12:53<20:17, 10.78s/it] 40%|███▉      | 74/186 [13:04<20:04, 10.76s/it] 40%|████      | 75/186 [13:15<19:55, 10.77s/it] 41%|████      | 76/186 [13:26<19:42, 10.75s/it] 41%|████▏     | 77/186 [13:36<19:27, 10.71s/it] 42%|████▏     | 78/186 [13:47<19:31, 10.85s/it] 42%|████▏     | 79/186 [13:58<19:13, 10.78s/it] 43%|████▎     | 80/186 [14:09<19:00, 10.76s/it] 44%|████▎     | 81/186 [14:19<18:43, 10.70s/it] 44%|████▍     | 82/186 [14:30<18:28, 10.66s/it] 45%|████▍     | 83/186 [14:41<18:27, 10.75s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 45%|████▌     | 84/186 [14:52<18:14, 10.73s/it] 46%|████▌     | 85/186 [15:02<18:00, 10.70s/it] 46%|████▌     | 86/186 [15:13<17:52, 10.72s/it] 47%|████▋     | 87/186 [15:24<17:41, 10.72s/it] 47%|████▋     | 88/186 [15:34<17:32, 10.74s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 48%|████▊     | 89/186 [15:45<17:25, 10.77s/it] 48%|████▊     | 90/186 [15:56<17:13, 10.77s/it] 49%|████▉     | 91/186 [16:07<17:16, 10.91s/it] 49%|████▉     | 92/186 [16:18<16:59, 10.85s/it] 50%|█████     | 93/186 [16:29<16:56, 10.93s/it] 51%|█████     | 94/186 [16:40<16:43, 10.91s/it] 51%|█████     | 95/186 [16:51<16:26, 10.84s/it] 52%|█████▏    | 96/186 [17:02<16:22, 10.91s/it] 52%|█████▏    | 97/186 [17:13<16:10, 10.90s/it] 53%|█████▎    | 98/186 [17:23<15:54, 10.85s/it] 53%|█████▎    | 99/186 [17:34<15:39, 10.80s/it] 54%|█████▍    | 100/186 [17:45<15:32, 10.84s/it] 54%|█████▍    | 101/186 [17:56<15:22, 10.86s/it] 55%|█████▍    | 102/186 [18:07<15:08, 10.81s/it] 55%|█████▌    | 103/186 [18:18<15:02, 10.87s/it] 56%|█████▌    | 104/186 [18:28<14:47, 10.82s/it] 56%|█████▋    | 105/186 [18:39<14:34, 10.80s/it] 57%|█████▋    | 106/186 [18:50<14:25, 10.82s/it] 58%|█████▊    | 107/186 [19:01<14:16, 10.84s/it] 58%|█████▊    | 108/186 [19:11<14:02, 10.80s/it] 59%|█████▊    | 109/186 [19:22<13:49, 10.77s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1143 > 1048). Running this sequence through the model will result in indexing errors
 59%|█████▉    | 110/186 [19:33<13:41, 10.81s/it] 60%|█████▉    | 111/186 [19:44<13:29, 10.79s/it] 60%|██████    | 112/186 [19:55<13:19, 10.80s/it] 61%|██████    | 113/186 [20:06<13:10, 10.83s/it] 61%|██████▏   | 114/186 [20:16<12:59, 10.83s/it] 62%|██████▏   | 115/186 [20:27<12:48, 10.83s/it] 62%|██████▏   | 116/186 [20:38<12:45, 10.93s/it] 63%|██████▎   | 117/186 [20:49<12:30, 10.88s/it] 63%|██████▎   | 118/186 [21:00<12:20, 10.89s/it] 64%|██████▍   | 119/186 [21:11<12:06, 10.84s/it] 65%|██████▍   | 120/186 [21:21<11:52, 10.79s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 65%|██████▌   | 121/186 [21:32<11:38, 10.75s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 66%|██████▌   | 122/186 [21:43<11:27, 10.74s/it] 66%|██████▌   | 123/186 [21:54<11:18, 10.76s/it] 67%|██████▋   | 124/186 [22:05<11:10, 10.82s/it] 67%|██████▋   | 125/186 [22:16<11:02, 10.85s/it] 68%|██████▊   | 126/186 [22:26<10:52, 10.87s/it] 68%|██████▊   | 127/186 [22:37<10:42, 10.89s/it] 69%|██████▉   | 128/186 [22:48<10:32, 10.91s/it] 69%|██████▉   | 129/186 [22:59<10:21, 10.90s/it] 70%|██████▉   | 130/186 [23:10<10:10, 10.90s/it] 70%|███████   | 131/186 [23:21<10:01, 10.94s/it] 71%|███████   | 132/186 [23:32<09:55, 11.02s/it] 72%|███████▏  | 133/186 [23:43<09:39, 10.94s/it] 72%|███████▏  | 134/186 [23:54<09:28, 10.93s/it] 73%|███████▎  | 135/186 [24:05<09:19, 10.97s/it] 73%|███████▎  | 136/186 [24:16<09:06, 10.94s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 74%|███████▎  | 137/186 [24:27<08:59, 11.02s/it] 74%|███████▍  | 138/186 [24:38<08:45, 10.96s/it] 75%|███████▍  | 139/186 [24:49<08:38, 11.02s/it] 75%|███████▌  | 140/186 [25:00<08:25, 10.99s/it] 76%|███████▌  | 141/186 [25:11<08:11, 10.91s/it] 76%|███████▋  | 142/186 [25:22<08:03, 10.99s/it] 77%|███████▋  | 143/186 [25:33<07:53, 11.00s/it] 77%|███████▋  | 144/186 [25:44<07:40, 10.97s/it] 78%|███████▊  | 145/186 [25:55<07:27, 10.92s/it] 78%|███████▊  | 146/186 [26:05<07:14, 10.85s/it] 79%|███████▉  | 147/186 [26:16<07:03, 10.85s/it] 80%|███████▉  | 148/186 [26:27<06:57, 10.97s/it] 80%|████████  | 149/186 [26:39<06:47, 11.02s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 81%|████████  | 150/186 [26:50<06:35, 11.00s/it] 81%|████████  | 151/186 [27:00<06:23, 10.97s/it] 82%|████████▏ | 152/186 [27:11<06:11, 10.94s/it] 82%|████████▏ | 153/186 [27:23<06:03, 11.02s/it] 83%|████████▎ | 154/186 [27:33<05:51, 10.99s/it] 83%|████████▎ | 155/186 [27:45<05:41, 11.02s/it] 84%|████████▍ | 156/186 [27:56<05:32, 11.09s/it] 84%|████████▍ | 157/186 [28:07<05:23, 11.16s/it] 85%|████████▍ | 158/186 [28:18<05:10, 11.09s/it] 85%|████████▌ | 159/186 [28:29<04:57, 11.01s/it] 86%|████████▌ | 160/186 [28:40<04:44, 10.95s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 87%|████████▋ | 161/186 [28:51<04:33, 10.95s/it] 87%|████████▋ | 162/186 [29:02<04:22, 10.95s/it] 88%|████████▊ | 163/186 [29:13<04:11, 10.95s/it] 88%|████████▊ | 164/186 [29:23<04:00, 10.92s/it] 89%|████████▊ | 165/186 [29:34<03:47, 10.85s/it] 89%|████████▉ | 166/186 [29:45<03:37, 10.86s/it] 90%|████████▉ | 167/186 [29:56<03:26, 10.84s/it] 90%|█████████ | 168/186 [30:07<03:17, 10.95s/it]WARNING: tokenization mismatch: 1 vs. 48. (ignored)
 91%|█████████ | 169/186 [30:18<03:06, 10.99s/it] 91%|█████████▏| 170/186 [30:29<02:55, 10.95s/it] 92%|█████████▏| 171/186 [30:40<02:46, 11.09s/it] 92%|█████████▏| 172/186 [30:51<02:34, 11.01s/it] 93%|█████████▎| 173/186 [31:02<02:22, 10.94s/it] 94%|█████████▎| 174/186 [31:13<02:11, 10.97s/it] 94%|█████████▍| 175/186 [31:24<02:01, 11.03s/it] 95%|█████████▍| 176/186 [31:35<01:49, 10.96s/it] 95%|█████████▌| 177/186 [31:46<01:38, 10.99s/it] 96%|█████████▌| 178/186 [31:57<01:28, 11.03s/it] 96%|█████████▌| 179/186 [32:08<01:17, 11.09s/it] 97%|█████████▋| 180/186 [32:19<01:06, 11.08s/it] 97%|█████████▋| 181/186 [32:31<00:55, 11.11s/it] 98%|█████████▊| 182/186 [32:41<00:44, 11.02s/it] 98%|█████████▊| 183/186 [32:53<00:33, 11.10s/it] 99%|█████████▉| 184/186 [33:04<00:22, 11.04s/it] 99%|█████████▉| 185/186 [33:14<00:11, 11.00s/it]100%|██████████| 186/186 [33:25<00:00, 10.96s/it]                                                 {'train_runtime': 2005.8039, 'train_samples_per_second': 6.698, 'train_steps_per_second': 0.093, 'train_loss': 0.12869893350908834, 'epoch': 1.0}
100%|██████████| 186/186 [33:25<00:00, 10.96s/it]100%|██████████| 186/186 [33:25<00:00, 10.78s/it]
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-1
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33msliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-1[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251017_053329-2q8l0m7r/logs[0m
[2025-10-17 06:08:39,329] [INFO] [launch.py:347:main] Process 2197682 exits successfully.
[2025-10-17 06:08:40,330] [INFO] [launch.py:347:main] Process 2197686 exits successfully.
[2025-10-17 06:08:40,331] [INFO] [launch.py:347:main] Process 2197683 exits successfully.
[2025-10-17 06:08:40,331] [INFO] [launch.py:347:main] Process 2197685 exits successfully.
[2025-10-17 06:08:40,331] [INFO] [launch.py:347:main] Process 2197684 exits successfully.
[2025-10-17 06:08:44,336] [INFO] [launch.py:347:main] Process 2197681 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 06:08:47,713] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 06:08:48,264] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-17 06:08:48,264] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12423 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1.5e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-1 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-1 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 2 --use_anchor False --use_sample_weight True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 06:08:49,705] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 06:08:50,272] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-17 06:08:50,272] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-17 06:08:50,272] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-17 06:08:50,272] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-17 06:08:50,272] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 06:08:53,805] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 06:08:53,840] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 06:08:54,151] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 06:08:54,153] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 06:08:54,193] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 06:08:54,503] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 06:08:54,625] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 06:08:54,638] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 06:08:54,934] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 06:08:54,934] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-10-17 06:08:54,942] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-10-17 06:08:55,418] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 06:08:55,923] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: creating run
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251017_060856-sps26dgt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-1
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/sps26dgt
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.03s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.75s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.05s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.10s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  6.00s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.75s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.54s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.92s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.74s/it]to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.66s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.03s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.64s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.99s/it]
to bfloat16...
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.44s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.78s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.47s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.81s/it]
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.91s/it]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.66s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  4.00s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.08s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.36s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.11s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.13s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.64s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.00s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.77s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.16s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.67s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.04s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.67s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.04s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  4.00s/it]
Formatting inputs...Skip in lazy mode
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:4')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:2')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:5')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:1')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:3')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:0')
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.011243820190429688 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.011042594909667969 seconds
Loading extension module fused_adam...
Loading extension module fused_adam...
Time to load fused_adam op: 0.10145211219787598 seconds
Time to load fused_adam op: 0.10104608535766602 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10288190841674805 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10098528861999512 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/186 [00:09<30:15,  9.81s/it]  1%|          | 2/186 [00:18<28:41,  9.36s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
  2%|▏         | 3/186 [00:28<29:17,  9.60s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1158 > 1048). Running this sequence through the model will result in indexing errors
  2%|▏         | 4/186 [00:38<29:33,  9.74s/it]  3%|▎         | 5/186 [00:48<29:27,  9.77s/it]  3%|▎         | 6/186 [00:58<29:32,  9.84s/it]  4%|▍         | 7/186 [01:08<29:23,  9.85s/it]  4%|▍         | 8/186 [01:18<29:22,  9.90s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1160 > 1048). Running this sequence through the model will result in indexing errors
  5%|▍         | 9/186 [01:28<29:24,  9.97s/it]  5%|▌         | 10/186 [01:38<29:35, 10.09s/it]  6%|▌         | 11/186 [01:49<29:42, 10.19s/it]  6%|▋         | 12/186 [01:59<29:33, 10.19s/it]  7%|▋         | 13/186 [02:09<29:28, 10.22s/it]  8%|▊         | 14/186 [02:19<29:14, 10.20s/it]  8%|▊         | 15/186 [02:30<29:27, 10.34s/it]  9%|▊         | 16/186 [02:40<29:14, 10.32s/it]  9%|▉         | 17/186 [02:51<29:08, 10.35s/it] 10%|▉         | 18/186 [03:01<28:57, 10.34s/it] 10%|█         | 19/186 [03:12<28:54, 10.39s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1153 > 1048). Running this sequence through the model will result in indexing errors
 11%|█         | 20/186 [03:22<28:59, 10.48s/it] 11%|█▏        | 21/186 [03:33<28:58, 10.54s/it] 12%|█▏        | 22/186 [03:43<28:44, 10.51s/it] 12%|█▏        | 23/186 [03:54<28:44, 10.58s/it] 13%|█▎        | 24/186 [04:05<28:47, 10.67s/it] 13%|█▎        | 25/186 [04:16<28:35, 10.66s/it] 14%|█▍        | 26/186 [04:26<28:23, 10.65s/it] 15%|█▍        | 27/186 [04:37<28:27, 10.74s/it] 15%|█▌        | 28/186 [04:48<28:18, 10.75s/it] 16%|█▌        | 29/186 [04:59<28:00, 10.70s/it] 16%|█▌        | 30/186 [05:09<27:37, 10.63s/it] 17%|█▋        | 31/186 [05:20<27:27, 10.63s/it] 17%|█▋        | 32/186 [05:30<27:16, 10.63s/it] 18%|█▊        | 33/186 [05:41<27:07, 10.63s/it] 18%|█▊        | 34/186 [05:52<26:57, 10.64s/it] 19%|█▉        | 35/186 [06:03<27:08, 10.78s/it] 19%|█▉        | 36/186 [06:14<26:58, 10.79s/it] 20%|█▉        | 37/186 [06:24<26:36, 10.72s/it] 20%|██        | 38/186 [06:35<26:18, 10.66s/it] 21%|██        | 39/186 [06:45<26:03, 10.64s/it] 22%|██▏       | 40/186 [06:56<25:54, 10.64s/it] 22%|██▏       | 41/186 [07:06<25:40, 10.62s/it] 23%|██▎       | 42/186 [07:17<25:39, 10.69s/it] 23%|██▎       | 43/186 [07:28<25:36, 10.75s/it] 24%|██▎       | 44/186 [07:39<25:29, 10.77s/it] 24%|██▍       | 45/186 [07:50<25:09, 10.70s/it] 25%|██▍       | 46/186 [08:00<25:04, 10.75s/it] 25%|██▌       | 47/186 [08:11<25:00, 10.80s/it] 26%|██▌       | 48/186 [08:22<24:44, 10.76s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 26%|██▋       | 49/186 [08:33<24:32, 10.75s/it] 27%|██▋       | 50/186 [08:43<24:23, 10.76s/it] 27%|██▋       | 51/186 [08:54<24:11, 10.75s/it] 28%|██▊       | 52/186 [09:05<24:10, 10.82s/it] 28%|██▊       | 53/186 [09:16<23:59, 10.83s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1073 > 1048). Running this sequence through the model will result in indexing errors
 29%|██▉       | 54/186 [09:27<23:55, 10.88s/it] 30%|██▉       | 55/186 [09:38<23:40, 10.84s/it] 30%|███       | 56/186 [09:49<23:28, 10.84s/it] 31%|███       | 57/186 [09:59<23:11, 10.79s/it] 31%|███       | 58/186 [10:10<22:55, 10.75s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1075 > 1048). Running this sequence through the model will result in indexing errors
 32%|███▏      | 59/186 [10:21<23:04, 10.90s/it] 32%|███▏      | 60/186 [10:32<22:47, 10.85s/it] 33%|███▎      | 61/186 [10:43<22:42, 10.90s/it] 33%|███▎      | 62/186 [10:54<22:30, 10.89s/it] 34%|███▍      | 63/186 [11:05<22:19, 10.89s/it] 34%|███▍      | 64/186 [11:15<22:02, 10.84s/it] 35%|███▍      | 65/186 [11:26<21:46, 10.79s/it] 35%|███▌      | 66/186 [11:37<21:44, 10.87s/it] 36%|███▌      | 67/186 [11:48<21:33, 10.87s/it] 37%|███▋      | 68/186 [11:59<21:13, 10.79s/it] 37%|███▋      | 69/186 [12:10<21:17, 10.92s/it] 38%|███▊      | 70/186 [12:21<21:00, 10.87s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 38%|███▊      | 71/186 [12:31<20:44, 10.82s/it] 39%|███▊      | 72/186 [12:42<20:36, 10.84s/it] 39%|███▉      | 73/186 [12:53<20:30, 10.89s/it] 40%|███▉      | 74/186 [13:04<20:19, 10.89s/it] 40%|████      | 75/186 [13:15<20:14, 10.94s/it] 41%|████      | 76/186 [13:26<20:10, 11.01s/it] 41%|████▏     | 77/186 [13:37<19:54, 10.96s/it] 42%|████▏     | 78/186 [13:48<19:42, 10.95s/it] 42%|████▏     | 79/186 [13:59<19:26, 10.90s/it] 43%|████▎     | 80/186 [14:09<19:02, 10.78s/it] 44%|████▎     | 81/186 [14:20<18:50, 10.77s/it] 44%|████▍     | 82/186 [14:31<18:41, 10.79s/it] 45%|████▍     | 83/186 [14:42<18:28, 10.76s/it] 45%|████▌     | 84/186 [14:52<18:17, 10.76s/it] 46%|████▌     | 85/186 [15:03<18:13, 10.83s/it] 46%|████▌     | 86/186 [15:14<17:56, 10.77s/it] 47%|████▋     | 87/186 [15:25<17:47, 10.78s/it] 47%|████▋     | 88/186 [15:36<17:44, 10.87s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 48%|████▊     | 89/186 [15:47<17:50, 11.04s/it] 48%|████▊     | 90/186 [15:58<17:34, 10.98s/it] 49%|████▉     | 91/186 [16:09<17:22, 10.97s/it] 49%|████▉     | 92/186 [16:20<17:18, 11.05s/it] 50%|█████     | 93/186 [16:31<17:02, 11.00s/it] 51%|█████     | 94/186 [16:42<16:42, 10.90s/it] 51%|█████     | 95/186 [16:52<16:23, 10.81s/it] 52%|█████▏    | 96/186 [17:03<16:17, 10.87s/it] 52%|█████▏    | 97/186 [17:14<16:09, 10.89s/it] 53%|█████▎    | 98/186 [17:26<16:03, 10.95s/it] 53%|█████▎    | 99/186 [17:36<15:47, 10.89s/it] 54%|█████▍    | 100/186 [17:47<15:31, 10.83s/it] 54%|█████▍    | 101/186 [17:58<15:23, 10.86s/it] 55%|█████▍    | 102/186 [18:08<15:03, 10.76s/it] 55%|█████▌    | 103/186 [18:20<15:01, 10.86s/it] 56%|█████▌    | 104/186 [18:30<14:48, 10.84s/it] 56%|█████▋    | 105/186 [18:41<14:34, 10.80s/it] 57%|█████▋    | 106/186 [18:52<14:28, 10.85s/it] 58%|█████▊    | 107/186 [19:03<14:12, 10.79s/it] 58%|█████▊    | 108/186 [19:14<14:04, 10.83s/it] 59%|█████▊    | 109/186 [19:24<13:50, 10.78s/it] 59%|█████▉    | 110/186 [19:35<13:41, 10.80s/it] 60%|█████▉    | 111/186 [19:46<13:27, 10.77s/it] 60%|██████    | 112/186 [19:57<13:19, 10.80s/it] 61%|██████    | 113/186 [20:07<13:09, 10.82s/it] 61%|██████▏   | 114/186 [20:18<13:02, 10.87s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 62%|██████▏   | 115/186 [20:29<12:44, 10.77s/it] 62%|██████▏   | 116/186 [20:40<12:46, 10.95s/it] 63%|██████▎   | 117/186 [20:51<12:34, 10.94s/it] 63%|██████▎   | 118/186 [21:02<12:21, 10.90s/it] 64%|██████▍   | 119/186 [21:13<12:08, 10.87s/it] 65%|██████▍   | 120/186 [21:24<11:53, 10.81s/it] 65%|██████▌   | 121/186 [21:34<11:41, 10.79s/it] 66%|██████▌   | 122/186 [21:45<11:33, 10.83s/it] 66%|██████▌   | 123/186 [21:56<11:26, 10.90s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1233 > 1048). Running this sequence through the model will result in indexing errors
 67%|██████▋   | 124/186 [22:07<11:17, 10.92s/it] 67%|██████▋   | 125/186 [22:18<11:05, 10.91s/it] 68%|██████▊   | 126/186 [22:29<10:58, 10.97s/it] 68%|██████▊   | 127/186 [22:40<10:49, 11.01s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
WARNING: tokenization mismatch: 1 vs. 48. (ignored)
 69%|██████▉   | 128/186 [22:51<10:34, 10.94s/it] 69%|██████▉   | 129/186 [23:02<10:22, 10.91s/it] 70%|██████▉   | 130/186 [23:13<10:12, 10.95s/it] 70%|███████   | 131/186 [23:24<09:57, 10.87s/it] 71%|███████   | 132/186 [23:35<09:46, 10.87s/it] 72%|███████▏  | 133/186 [23:45<09:35, 10.86s/it] 72%|███████▏  | 134/186 [23:56<09:27, 10.92s/it] 73%|███████▎  | 135/186 [24:07<09:15, 10.89s/it] 73%|███████▎  | 136/186 [24:18<09:05, 10.91s/it] 74%|███████▎  | 137/186 [24:30<09:03, 11.10s/it] 74%|███████▍  | 138/186 [24:41<08:50, 11.04s/it] 75%|███████▍  | 139/186 [24:52<08:42, 11.12s/it] 75%|███████▌  | 140/186 [25:03<08:28, 11.06s/it] 76%|███████▌  | 141/186 [25:14<08:18, 11.07s/it] 76%|███████▋  | 142/186 [25:25<08:06, 11.05s/it] 77%|███████▋  | 143/186 [25:36<07:52, 10.99s/it] 77%|███████▋  | 144/186 [25:47<07:40, 10.97s/it] 78%|███████▊  | 145/186 [25:58<07:29, 10.97s/it] 78%|███████▊  | 146/186 [26:09<07:17, 10.95s/it] 79%|███████▉  | 147/186 [26:20<07:06, 10.95s/it] 80%|███████▉  | 148/186 [26:31<06:57, 10.98s/it] 80%|████████  | 149/186 [26:41<06:44, 10.93s/it] 81%|████████  | 150/186 [26:53<06:34, 10.96s/it] 81%|████████  | 151/186 [27:04<06:24, 10.98s/it] 82%|████████▏ | 152/186 [27:14<06:13, 10.97s/it] 82%|████████▏ | 153/186 [27:25<06:01, 10.96s/it] 83%|████████▎ | 154/186 [27:37<05:53, 11.03s/it] 83%|████████▎ | 155/186 [27:48<05:41, 11.02s/it] 84%|████████▍ | 156/186 [27:59<05:32, 11.10s/it] 84%|████████▍ | 157/186 [28:10<05:21, 11.09s/it] 85%|████████▍ | 158/186 [28:21<05:11, 11.12s/it] 85%|████████▌ | 159/186 [28:32<04:58, 11.06s/it] 86%|████████▌ | 160/186 [28:43<04:46, 11.01s/it] 87%|████████▋ | 161/186 [28:54<04:34, 10.97s/it] 87%|████████▋ | 162/186 [29:05<04:25, 11.08s/it] 88%|████████▊ | 163/186 [29:17<04:16, 11.16s/it] 88%|████████▊ | 164/186 [29:28<04:05, 11.16s/it] 89%|████████▊ | 165/186 [29:38<03:51, 11.04s/it] 89%|████████▉ | 166/186 [29:49<03:40, 11.04s/it] 90%|████████▉ | 167/186 [30:01<03:29, 11.05s/it] 90%|█████████ | 168/186 [30:12<03:20, 11.13s/it] 91%|█████████ | 169/186 [30:23<03:09, 11.13s/it] 91%|█████████▏| 170/186 [30:34<02:57, 11.11s/it] 92%|█████████▏| 171/186 [30:46<02:48, 11.21s/it] 92%|█████████▏| 172/186 [30:56<02:35, 11.10s/it] 93%|█████████▎| 173/186 [31:07<02:23, 11.02s/it] 94%|█████████▎| 174/186 [31:18<02:13, 11.11s/it] 94%|█████████▍| 175/186 [31:29<02:01, 11.03s/it] 95%|█████████▍| 176/186 [31:40<01:50, 11.05s/it] 95%|█████████▌| 177/186 [31:51<01:39, 11.03s/it] 96%|█████████▌| 178/186 [32:02<01:27, 10.99s/it]WARNING: tokenization mismatch: 1 vs. 47. (ignored)
 96%|█████████▌| 179/186 [32:14<01:17, 11.07s/it] 97%|█████████▋| 180/186 [32:25<01:06, 11.06s/it] 97%|█████████▋| 181/186 [32:36<00:55, 11.05s/it] 98%|█████████▊| 182/186 [32:47<00:44, 11.17s/it] 98%|█████████▊| 183/186 [32:58<00:33, 11.11s/it] 99%|█████████▉| 184/186 [33:09<00:22, 11.14s/it] 99%|█████████▉| 185/186 [33:20<00:11, 11.10s/it]100%|██████████| 186/186 [33:31<00:00, 11.04s/it]                                                 {'train_runtime': 2011.6768, 'train_samples_per_second': 6.679, 'train_steps_per_second': 0.092, 'train_loss': 0.1348769792946436, 'epoch': 1.0}
100%|██████████| 186/186 [33:31<00:00, 11.04s/it]100%|██████████| 186/186 [33:31<00:00, 10.82s/it]
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-1
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33msliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-1[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251017_060856-sps26dgt/logs[0m
[2025-10-17 06:44:15,710] [INFO] [launch.py:347:main] Process 2199428 exits successfully.
[2025-10-17 06:44:15,710] [INFO] [launch.py:347:main] Process 2199425 exits successfully.
[2025-10-17 06:44:15,711] [INFO] [launch.py:347:main] Process 2199429 exits successfully.
[2025-10-17 06:44:15,711] [INFO] [launch.py:347:main] Process 2199427 exits successfully.
[2025-10-17 06:44:15,711] [INFO] [launch.py:347:main] Process 2199426 exits successfully.
[2025-10-17 06:44:20,717] [INFO] [launch.py:347:main] Process 2199424 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 06:44:23,993] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 06:44:24,546] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-17 06:44:24,546] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12423 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-2 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-2 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 0.5 --use_anchor False --use_sample_weight True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 06:44:26,017] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 06:44:26,591] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-17 06:44:26,591] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-17 06:44:26,591] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-17 06:44:26,591] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-17 06:44:26,591] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 06:44:30,430] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 06:44:30,753] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 06:44:30,775] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 06:44:30,885] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 06:44:31,087] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 06:44:31,144] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 06:44:31,145] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 06:44:31,198] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 06:44:31,455] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 06:44:31,460] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 06:44:31,460] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
[2025-10-17 06:44:32,169] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
[2025-10-17 06:44:32,517] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251017_064432-kxfx2aff
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-2
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/kxfx2aff
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.43s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.14s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.34s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.06s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.08s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.85s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.97s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.53s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.57s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.94s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.55s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.93s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.58s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.92s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.71s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.05s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.51s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.81s/it]
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.48s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.97s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.34s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.84s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.69s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.59s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.93s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.79s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.58s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.75s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.82s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.86s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.92s/it]
Formatting inputs...Skip in lazy mode
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:2')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:1')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:4')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:5')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:0')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:3')
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.012751340866088867 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.011722803115844727 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.013033628463745117 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.013627767562866211 seconds
Loading extension module fused_adam...
Loading extension module fused_adam...
Time to load fused_adam op: 0.20156431198120117 seconds
Time to load fused_adam op: 0.20245885848999023 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/186 [00:09<30:26,  9.87s/it]  1%|          | 2/186 [00:18<28:06,  9.17s/it]  2%|▏         | 3/186 [00:27<28:03,  9.20s/it]  2%|▏         | 4/186 [00:37<27:59,  9.23s/it]  3%|▎         | 5/186 [00:47<28:41,  9.51s/it]  3%|▎         | 6/186 [00:57<29:01,  9.67s/it]  4%|▍         | 7/186 [01:07<29:07,  9.76s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
  4%|▍         | 8/186 [01:16<28:56,  9.76s/it]  5%|▍         | 9/186 [01:26<28:52,  9.79s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
  5%|▌         | 10/186 [01:36<28:54,  9.86s/it]  6%|▌         | 11/186 [01:46<29:02,  9.96s/it]  6%|▋         | 12/186 [01:56<29:02, 10.01s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
  7%|▋         | 13/186 [02:07<29:01, 10.07s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1165 > 1048). Running this sequence through the model will result in indexing errors
  8%|▊         | 14/186 [02:17<29:09, 10.17s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1102 > 1048). Running this sequence through the model will result in indexing errors
  8%|▊         | 15/186 [02:27<29:11, 10.24s/it]  9%|▊         | 16/186 [02:38<29:02, 10.25s/it]  9%|▉         | 17/186 [02:48<28:51, 10.25s/it] 10%|▉         | 18/186 [02:58<28:47, 10.28s/it] 10%|█         | 19/186 [03:09<28:37, 10.29s/it] 11%|█         | 20/186 [03:19<28:20, 10.24s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1065 > 1048). Running this sequence through the model will result in indexing errors
 11%|█▏        | 21/186 [03:29<28:28, 10.35s/it] 12%|█▏        | 22/186 [03:40<28:22, 10.38s/it] 12%|█▏        | 23/186 [03:50<28:24, 10.46s/it] 13%|█▎        | 24/186 [04:01<28:23, 10.51s/it] 13%|█▎        | 25/186 [04:11<28:02, 10.45s/it] 14%|█▍        | 26/186 [04:22<27:50, 10.44s/it] 15%|█▍        | 27/186 [04:32<27:51, 10.51s/it] 15%|█▌        | 28/186 [04:43<27:34, 10.47s/it] 16%|█▌        | 29/186 [04:53<27:20, 10.45s/it] 16%|█▌        | 30/186 [05:04<27:03, 10.40s/it] 17%|█▋        | 31/186 [05:14<26:58, 10.44s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 17%|█▋        | 32/186 [05:25<26:54, 10.48s/it] 18%|█▊        | 33/186 [05:35<26:38, 10.45s/it] 18%|█▊        | 34/186 [05:46<26:34, 10.49s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1200 > 1048). Running this sequence through the model will result in indexing errors
 19%|█▉        | 35/186 [05:56<26:39, 10.60s/it] 19%|█▉        | 36/186 [06:07<26:21, 10.54s/it] 20%|█▉        | 37/186 [06:18<26:17, 10.59s/it] 20%|██        | 38/186 [06:28<26:08, 10.60s/it] 21%|██        | 39/186 [06:39<26:02, 10.63s/it] 22%|██▏       | 40/186 [06:50<25:52, 10.63s/it] 22%|██▏       | 41/186 [07:00<25:40, 10.62s/it] 23%|██▎       | 42/186 [07:11<25:40, 10.69s/it] 23%|██▎       | 43/186 [07:22<25:31, 10.71s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
Token indices sequence length is longer than the specified maximum sequence length for this model (1200 > 1048). Running this sequence through the model will result in indexing errors
 24%|██▎       | 44/186 [07:33<25:22, 10.72s/it] 24%|██▍       | 45/186 [07:43<25:00, 10.64s/it] 25%|██▍       | 46/186 [07:54<24:50, 10.65s/it] 25%|██▌       | 47/186 [08:05<24:56, 10.77s/it] 26%|██▌       | 48/186 [08:16<24:50, 10.80s/it] 26%|██▋       | 49/186 [08:26<24:37, 10.78s/it] 27%|██▋       | 50/186 [08:37<24:20, 10.74s/it] 27%|██▋       | 51/186 [08:48<24:19, 10.81s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 28%|██▊       | 52/186 [08:59<24:10, 10.82s/it] 28%|██▊       | 53/186 [09:10<24:02, 10.85s/it] 29%|██▉       | 54/186 [09:20<23:48, 10.82s/it] 30%|██▉       | 55/186 [09:31<23:29, 10.76s/it] 30%|███       | 56/186 [09:42<23:14, 10.72s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 31%|███       | 57/186 [09:52<23:01, 10.71s/it] 31%|███       | 58/186 [10:03<22:54, 10.74s/it] 32%|███▏      | 59/186 [10:14<22:45, 10.75s/it] 32%|███▏      | 60/186 [10:25<22:37, 10.77s/it] 33%|███▎      | 61/186 [10:36<22:25, 10.77s/it] 33%|███▎      | 62/186 [10:46<22:19, 10.80s/it] 34%|███▍      | 63/186 [10:57<21:58, 10.72s/it] 34%|███▍      | 64/186 [11:08<21:42, 10.68s/it] 35%|███▍      | 65/186 [11:18<21:33, 10.69s/it] 35%|███▌      | 66/186 [11:29<21:32, 10.77s/it] 36%|███▌      | 67/186 [11:40<21:25, 10.81s/it] 37%|███▋      | 68/186 [11:51<21:06, 10.74s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1153 > 1048). Running this sequence through the model will result in indexing errors
 37%|███▋      | 69/186 [12:02<21:03, 10.80s/it] 38%|███▊      | 70/186 [12:13<20:57, 10.84s/it] 38%|███▊      | 71/186 [12:23<20:40, 10.79s/it] 39%|███▊      | 72/186 [12:34<20:21, 10.71s/it] 39%|███▉      | 73/186 [12:44<20:05, 10.67s/it] 40%|███▉      | 74/186 [12:55<19:51, 10.64s/it] 40%|████      | 75/186 [13:06<19:45, 10.68s/it] 41%|████      | 76/186 [13:16<19:31, 10.65s/it] 41%|████▏     | 77/186 [13:27<19:22, 10.67s/it] 42%|████▏     | 78/186 [13:38<19:08, 10.64s/it] 42%|████▏     | 79/186 [13:48<18:59, 10.65s/it] 43%|████▎     | 80/186 [13:59<18:52, 10.69s/it] 44%|████▎     | 81/186 [14:10<18:41, 10.68s/it] 44%|████▍     | 82/186 [14:20<18:28, 10.66s/it] 45%|████▍     | 83/186 [14:31<18:19, 10.67s/it] 45%|████▌     | 84/186 [14:42<18:14, 10.73s/it] 46%|████▌     | 85/186 [14:53<18:05, 10.75s/it] 46%|████▌     | 86/186 [15:03<17:50, 10.70s/it] 47%|████▋     | 87/186 [15:14<17:33, 10.64s/it] 47%|████▋     | 88/186 [15:24<17:19, 10.61s/it] 48%|████▊     | 89/186 [15:35<17:24, 10.76s/it] 48%|████▊     | 90/186 [15:46<17:11, 10.75s/it] 49%|████▉     | 91/186 [15:57<17:08, 10.83s/it] 49%|████▉     | 92/186 [16:08<16:57, 10.83s/it] 50%|█████     | 93/186 [16:19<16:47, 10.84s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 51%|█████     | 94/186 [16:30<16:43, 10.90s/it] 51%|█████     | 95/186 [16:41<16:28, 10.87s/it] 52%|█████▏    | 96/186 [16:51<16:09, 10.77s/it] 52%|█████▏    | 97/186 [17:02<15:50, 10.67s/it] 53%|█████▎    | 98/186 [17:12<15:40, 10.69s/it] 53%|█████▎    | 99/186 [17:23<15:24, 10.63s/it] 54%|█████▍    | 100/186 [17:34<15:21, 10.71s/it] 54%|█████▍    | 101/186 [17:44<15:10, 10.71s/it] 55%|█████▍    | 102/186 [17:55<14:59, 10.71s/it] 55%|█████▌    | 103/186 [18:06<15:03, 10.88s/it] 56%|█████▌    | 104/186 [18:17<14:47, 10.83s/it] 56%|█████▋    | 105/186 [18:28<14:36, 10.82s/it] 57%|█████▋    | 106/186 [18:38<14:17, 10.72s/it] 58%|█████▊    | 107/186 [18:49<14:02, 10.67s/it] 58%|█████▊    | 108/186 [19:00<13:50, 10.65s/it] 59%|█████▊    | 109/186 [19:10<13:40, 10.65s/it] 59%|█████▉    | 110/186 [19:21<13:28, 10.64s/it] 60%|█████▉    | 111/186 [19:31<13:18, 10.64s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 60%|██████    | 112/186 [19:42<13:02, 10.58s/it] 61%|██████    | 113/186 [19:52<12:50, 10.56s/it] 61%|██████▏   | 114/186 [20:03<12:45, 10.63s/it] 62%|██████▏   | 115/186 [20:14<12:34, 10.63s/it] 62%|██████▏   | 116/186 [20:25<12:30, 10.72s/it] 63%|██████▎   | 117/186 [20:36<12:20, 10.73s/it] 63%|██████▎   | 118/186 [20:46<12:10, 10.74s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 64%|██████▍   | 119/186 [20:57<11:58, 10.72s/it] 65%|██████▍   | 120/186 [21:08<11:50, 10.76s/it] 65%|██████▌   | 121/186 [21:19<11:38, 10.74s/it] 66%|██████▌   | 122/186 [21:29<11:27, 10.74s/it] 66%|██████▌   | 123/186 [21:40<11:21, 10.82s/it] 67%|██████▋   | 124/186 [21:51<11:08, 10.78s/it] 67%|██████▋   | 125/186 [22:02<10:56, 10.77s/it] 68%|██████▊   | 126/186 [22:12<10:41, 10.68s/it] 68%|██████▊   | 127/186 [22:23<10:29, 10.66s/it] 69%|██████▉   | 128/186 [22:34<10:20, 10.70s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 69%|██████▉   | 129/186 [22:44<10:07, 10.67s/it] 70%|██████▉   | 130/186 [22:55<09:56, 10.65s/it] 70%|███████   | 131/186 [23:06<09:56, 10.85s/it] 71%|███████   | 132/186 [23:17<09:46, 10.87s/it] 72%|███████▏  | 133/186 [23:28<09:33, 10.83s/it] 72%|███████▏  | 134/186 [23:38<09:20, 10.77s/it] 73%|███████▎  | 135/186 [23:49<09:07, 10.73s/it] 73%|███████▎  | 136/186 [24:00<08:57, 10.76s/it] 74%|███████▎  | 137/186 [24:11<08:54, 10.90s/it] 74%|███████▍  | 138/186 [24:22<08:39, 10.83s/it] 75%|███████▍  | 139/186 [24:33<08:32, 10.90s/it] 75%|███████▌  | 140/186 [24:44<08:18, 10.84s/it] 76%|███████▌  | 141/186 [24:54<08:04, 10.76s/it] 76%|███████▋  | 142/186 [25:05<07:54, 10.79s/it] 77%|███████▋  | 143/186 [25:16<07:41, 10.74s/it] 77%|███████▋  | 144/186 [25:26<07:30, 10.73s/it] 78%|███████▊  | 145/186 [25:37<07:18, 10.70s/it] 78%|███████▊  | 146/186 [25:48<07:09, 10.74s/it] 79%|███████▉  | 147/186 [25:59<07:02, 10.82s/it] 80%|███████▉  | 148/186 [26:09<06:49, 10.77s/it] 80%|████████  | 149/186 [26:20<06:37, 10.75s/it] 81%|████████  | 150/186 [26:31<06:24, 10.69s/it] 81%|████████  | 151/186 [26:41<06:14, 10.69s/it] 82%|████████▏ | 152/186 [26:52<06:02, 10.65s/it] 82%|████████▏ | 153/186 [27:03<05:53, 10.70s/it] 83%|████████▎ | 154/186 [27:13<05:41, 10.66s/it] 83%|████████▎ | 155/186 [27:24<05:29, 10.64s/it] 84%|████████▍ | 156/186 [27:35<05:24, 10.80s/it] 84%|████████▍ | 157/186 [27:46<05:12, 10.77s/it] 85%|████████▍ | 158/186 [27:56<05:01, 10.77s/it] 85%|████████▌ | 159/186 [28:07<04:50, 10.76s/it] 86%|████████▌ | 160/186 [28:18<04:41, 10.82s/it] 87%|████████▋ | 161/186 [28:29<04:29, 10.79s/it] 87%|████████▋ | 162/186 [28:40<04:19, 10.80s/it] 88%|████████▊ | 163/186 [28:50<04:07, 10.76s/it] 88%|████████▊ | 164/186 [29:01<03:56, 10.75s/it] 89%|████████▊ | 165/186 [29:12<03:45, 10.75s/it] 89%|████████▉ | 166/186 [29:23<03:35, 10.79s/it] 90%|████████▉ | 167/186 [29:34<03:25, 10.79s/it] 90%|█████████ | 168/186 [29:44<03:14, 10.81s/it] 91%|█████████ | 169/186 [29:55<03:02, 10.74s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 91%|█████████▏| 170/186 [30:06<02:51, 10.75s/it] 92%|█████████▏| 171/186 [30:17<02:42, 10.85s/it] 92%|█████████▏| 172/186 [30:27<02:30, 10.74s/it] 93%|█████████▎| 173/186 [30:38<02:19, 10.76s/it] 94%|█████████▎| 174/186 [30:49<02:08, 10.70s/it] 94%|█████████▍| 175/186 [30:59<01:57, 10.65s/it] 95%|█████████▍| 176/186 [31:10<01:47, 10.72s/it] 95%|█████████▌| 177/186 [31:21<01:36, 10.70s/it] 96%|█████████▌| 178/186 [31:31<01:24, 10.61s/it] 96%|█████████▌| 179/186 [31:42<01:14, 10.63s/it] 97%|█████████▋| 180/186 [31:52<01:03, 10.62s/it] 97%|█████████▋| 181/186 [32:03<00:53, 10.67s/it] 98%|█████████▊| 182/186 [32:14<00:42, 10.69s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 98%|█████████▊| 183/186 [32:25<00:32, 10.74s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 99%|█████████▉| 184/186 [32:36<00:21, 10.77s/it] 99%|█████████▉| 185/186 [32:47<00:10, 10.84s/it]100%|██████████| 186/186 [32:57<00:00, 10.76s/it]                                                 {'train_runtime': 1977.7268, 'train_samples_per_second': 6.793, 'train_steps_per_second': 0.094, 'train_loss': 0.1433672648604198, 'epoch': 1.0}
100%|██████████| 186/186 [32:57<00:00, 10.76s/it]100%|██████████| 186/186 [32:57<00:00, 10.63s/it]
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-2
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33msliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-2[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251017_064432-kxfx2aff/logs[0m
[2025-10-17 07:19:14,039] [INFO] [launch.py:347:main] Process 2201185 exits successfully.
[2025-10-17 07:19:15,041] [INFO] [launch.py:347:main] Process 2201184 exits successfully.
[2025-10-17 07:19:15,041] [INFO] [launch.py:347:main] Process 2201186 exits successfully.
[2025-10-17 07:19:15,041] [INFO] [launch.py:347:main] Process 2201183 exits successfully.
[2025-10-17 07:19:15,041] [INFO] [launch.py:347:main] Process 2201187 exits successfully.
[2025-10-17 07:19:19,046] [INFO] [launch.py:347:main] Process 2201182 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 07:19:22,268] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 07:19:22,823] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-17 07:19:22,823] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12423 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-2 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-2 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 1 --use_anchor False --use_sample_weight True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 07:19:24,289] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 07:19:24,867] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-17 07:19:24,867] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-17 07:19:24,867] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-17 07:19:24,867] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-17 07:19:24,867] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 07:19:28,455] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-17 07:19:28,868] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 07:19:28,887] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 07:19:28,919] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 07:19:28,931] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 07:19:29,186] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 07:19:29,239] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 07:19:29,254] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 07:19:29,254] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-10-17 07:19:29,799] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-17 07:19:30,100] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-10-17 07:19:30,156] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
[2025-10-17 07:19:30,532] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251017_071930-q79v86yy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-2
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/q79v86yy
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.82s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.22s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.04s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.05s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.06s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.68s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.25s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.64s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.00s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.74s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.09s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.59s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.90s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.75s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.10s/it]
to bfloat16...
to bfloat16...
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.61s/it]
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.18s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.20s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.53s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.17s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.21s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.58s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.87s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.06s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.16s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.42s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.26s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.68s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.78s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.14s/it]
Formatting inputs...Skip in lazy mode
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:1')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:4')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:2')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:3')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:0')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:5')
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.012527704238891602 seconds
Loading extension module fused_adam...
Loading extension module fused_adam...
Time to load fused_adam op: 0.10112881660461426 seconds
Time to load fused_adam op: 0.10114216804504395 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.010629653930664062 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.0130157470703125 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10152649879455566 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/186 [00:12<38:26, 12.47s/it]  1%|          | 2/186 [00:21<32:08, 10.48s/it]  2%|▏         | 3/186 [00:30<30:12,  9.91s/it]  2%|▏         | 4/186 [00:40<29:30,  9.73s/it]  3%|▎         | 5/186 [00:49<29:17,  9.71s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
  3%|▎         | 6/186 [00:59<29:09,  9.72s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1089 > 1048). Running this sequence through the model will result in indexing errors
  4%|▍         | 7/186 [01:09<29:34,  9.92s/it]  4%|▍         | 8/186 [01:20<29:43, 10.02s/it]  5%|▍         | 9/186 [01:30<29:45, 10.09s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1233 > 1048). Running this sequence through the model will result in indexing errors
WARNING: tokenization mismatch: 1 vs. 50. (ignored)
  5%|▌         | 10/186 [01:40<29:55, 10.20s/it]  6%|▌         | 11/186 [01:51<29:48, 10.22s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1150 > 1048). Running this sequence through the model will result in indexing errors
  6%|▋         | 12/186 [02:01<29:45, 10.26s/it]  7%|▋         | 13/186 [02:12<29:49, 10.35s/it]  8%|▊         | 14/186 [02:22<29:36, 10.33s/it]  8%|▊         | 15/186 [02:32<29:28, 10.34s/it]  9%|▊         | 16/186 [02:43<29:22, 10.37s/it]  9%|▉         | 17/186 [02:53<29:06, 10.34s/it] 10%|▉         | 18/186 [03:03<28:54, 10.32s/it] 10%|█         | 19/186 [03:14<28:45, 10.33s/it] 11%|█         | 20/186 [03:24<28:50, 10.42s/it] 11%|█▏        | 21/186 [03:35<28:41, 10.43s/it] 12%|█▏        | 22/186 [03:45<28:49, 10.54s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1285 > 1048). Running this sequence through the model will result in indexing errors
 12%|█▏        | 23/186 [03:56<28:48, 10.60s/it] 13%|█▎        | 24/186 [04:07<29:03, 10.76s/it] 13%|█▎        | 25/186 [04:18<28:41, 10.69s/it] 14%|█▍        | 26/186 [04:28<28:26, 10.67s/it] 15%|█▍        | 27/186 [04:39<28:27, 10.74s/it] 15%|█▌        | 28/186 [04:50<28:01, 10.64s/it] 16%|█▌        | 29/186 [05:00<27:47, 10.62s/it] 16%|█▌        | 30/186 [05:11<27:44, 10.67s/it] 17%|█▋        | 31/186 [05:22<27:32, 10.66s/it] 17%|█▋        | 32/186 [05:32<27:22, 10.67s/it] 18%|█▊        | 33/186 [05:43<27:13, 10.68s/it] 18%|█▊        | 34/186 [05:54<26:55, 10.63s/it] 19%|█▉        | 35/186 [06:05<26:56, 10.70s/it] 19%|█▉        | 36/186 [06:15<26:41, 10.68s/it] 20%|█▉        | 37/186 [06:26<26:32, 10.69s/it] 20%|██        | 38/186 [06:37<26:21, 10.68s/it] 21%|██        | 39/186 [06:47<26:10, 10.68s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1263 > 1048). Running this sequence through the model will result in indexing errors
 22%|██▏       | 40/186 [06:58<26:13, 10.78s/it] 22%|██▏       | 41/186 [07:09<25:57, 10.74s/it] 23%|██▎       | 42/186 [07:19<25:37, 10.68s/it] 23%|██▎       | 43/186 [07:30<25:26, 10.68s/it] 24%|██▎       | 44/186 [07:41<25:12, 10.65s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1153 > 1048). Running this sequence through the model will result in indexing errors
 24%|██▍       | 45/186 [07:51<25:06, 10.68s/it] 25%|██▍       | 46/186 [08:03<25:18, 10.84s/it] 25%|██▌       | 47/186 [08:14<25:26, 10.98s/it] 26%|██▌       | 48/186 [08:25<25:10, 10.95s/it] 26%|██▋       | 49/186 [08:36<24:48, 10.87s/it] 27%|██▋       | 50/186 [08:47<24:49, 10.95s/it] 27%|██▋       | 51/186 [08:57<24:24, 10.85s/it] 28%|██▊       | 52/186 [09:08<24:04, 10.78s/it] 28%|██▊       | 53/186 [09:19<24:02, 10.85s/it] 29%|██▉       | 54/186 [09:30<23:46, 10.81s/it] 30%|██▉       | 55/186 [09:40<23:28, 10.75s/it] 30%|███       | 56/186 [09:51<23:15, 10.74s/it] 31%|███       | 57/186 [10:02<23:07, 10.76s/it] 31%|███       | 58/186 [10:13<22:57, 10.76s/it] 32%|███▏      | 59/186 [10:24<22:59, 10.86s/it] 32%|███▏      | 60/186 [10:35<22:49, 10.87s/it] 33%|███▎      | 61/186 [10:45<22:32, 10.82s/it] 33%|███▎      | 62/186 [10:56<22:23, 10.83s/it] 34%|███▍      | 63/186 [11:07<22:07, 10.80s/it] 34%|███▍      | 64/186 [11:18<21:59, 10.82s/it] 35%|███▍      | 65/186 [11:29<21:53, 10.85s/it] 35%|███▌      | 66/186 [11:40<21:49, 10.91s/it] 36%|███▌      | 67/186 [11:50<21:35, 10.89s/it] 37%|███▋      | 68/186 [12:01<21:21, 10.86s/it] 37%|███▋      | 69/186 [12:12<21:23, 10.97s/it] 38%|███▊      | 70/186 [12:24<21:19, 11.03s/it] 38%|███▊      | 71/186 [12:34<20:55, 10.92s/it] 39%|███▊      | 72/186 [12:45<20:41, 10.89s/it] 39%|███▉      | 73/186 [12:56<20:28, 10.87s/it] 40%|███▉      | 74/186 [13:07<20:16, 10.86s/it] 40%|████      | 75/186 [13:18<20:04, 10.85s/it] 41%|████      | 76/186 [13:28<19:53, 10.85s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 41%|████▏     | 77/186 [13:40<19:53, 10.95s/it] 42%|████▏     | 78/186 [13:51<19:42, 10.95s/it] 42%|████▏     | 79/186 [14:01<19:24, 10.89s/it] 43%|████▎     | 80/186 [14:12<19:10, 10.86s/it] 44%|████▎     | 81/186 [14:23<18:55, 10.81s/it] 44%|████▍     | 82/186 [14:34<18:46, 10.84s/it] 45%|████▍     | 83/186 [14:45<18:34, 10.82s/it] 45%|████▌     | 84/186 [14:55<18:21, 10.80s/it] 46%|████▌     | 85/186 [15:06<18:13, 10.82s/it] 46%|████▌     | 86/186 [15:17<18:09, 10.90s/it] 47%|████▋     | 87/186 [15:28<17:56, 10.88s/it] 47%|████▋     | 88/186 [15:39<17:48, 10.91s/it] 48%|████▊     | 89/186 [15:50<17:37, 10.90s/it] 48%|████▊     | 90/186 [16:01<17:24, 10.88s/it] 49%|████▉     | 91/186 [16:12<17:17, 10.92s/it] 49%|████▉     | 92/186 [16:23<17:16, 11.03s/it] 50%|█████     | 93/186 [16:34<17:16, 11.15s/it] 51%|█████     | 94/186 [16:45<17:00, 11.09s/it] 51%|█████     | 95/186 [16:56<16:46, 11.06s/it] 52%|█████▏    | 96/186 [17:07<16:33, 11.04s/it] 52%|█████▏    | 97/186 [17:19<16:25, 11.08s/it] 53%|█████▎    | 98/186 [17:29<16:07, 10.99s/it] 53%|█████▎    | 99/186 [17:40<15:51, 10.93s/it] 54%|█████▍    | 100/186 [17:51<15:39, 10.93s/it] 54%|█████▍    | 101/186 [18:02<15:27, 10.91s/it] 55%|█████▍    | 102/186 [18:13<15:16, 10.91s/it] 55%|█████▌    | 103/186 [18:24<15:12, 10.99s/it] 56%|█████▌    | 104/186 [18:35<14:57, 10.94s/it] 56%|█████▋    | 105/186 [18:46<14:48, 10.97s/it] 57%|█████▋    | 106/186 [18:57<14:39, 10.99s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 58%|█████▊    | 107/186 [19:08<14:28, 10.99s/it] 58%|█████▊    | 108/186 [19:19<14:17, 10.99s/it] 59%|█████▊    | 109/186 [19:30<14:11, 11.05s/it] 59%|█████▉    | 110/186 [19:41<13:57, 11.02s/it] 60%|█████▉    | 111/186 [19:52<13:48, 11.05s/it] 60%|██████    | 112/186 [20:03<13:34, 11.01s/it] 61%|██████    | 113/186 [20:14<13:26, 11.04s/it] 61%|██████▏   | 114/186 [20:25<13:15, 11.05s/it] 62%|██████▏   | 115/186 [20:36<12:59, 10.98s/it] 62%|██████▏   | 116/186 [20:47<12:56, 11.10s/it] 63%|██████▎   | 117/186 [20:58<12:43, 11.06s/it] 63%|██████▎   | 118/186 [21:09<12:26, 10.98s/it] 64%|██████▍   | 119/186 [21:20<12:10, 10.90s/it] 65%|██████▍   | 120/186 [21:31<11:59, 10.90s/it] 65%|██████▌   | 121/186 [21:42<11:51, 10.95s/it] 66%|██████▌   | 122/186 [21:53<11:45, 11.03s/it] 66%|██████▌   | 123/186 [22:05<11:44, 11.18s/it] 67%|██████▋   | 124/186 [22:16<11:29, 11.12s/it] 67%|██████▋   | 125/186 [22:27<11:16, 11.10s/it] 68%|██████▊   | 126/186 [22:38<11:05, 11.09s/it] 68%|██████▊   | 127/186 [22:49<10:53, 11.07s/it] 69%|██████▉   | 128/186 [23:00<10:36, 10.97s/it] 69%|██████▉   | 129/186 [23:11<10:26, 10.98s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
 70%|██████▉   | 130/186 [23:22<10:15, 10.99s/it] 70%|███████   | 131/186 [23:32<10:01, 10.93s/it] 71%|███████   | 132/186 [23:43<09:50, 10.93s/it] 72%|███████▏  | 133/186 [23:54<09:38, 10.91s/it] 72%|███████▏  | 134/186 [24:05<09:32, 11.01s/it] 73%|███████▎  | 135/186 [24:17<09:24, 11.07s/it] 73%|███████▎  | 136/186 [24:28<09:11, 11.03s/it] 74%|███████▎  | 137/186 [24:39<09:02, 11.08s/it] 74%|███████▍  | 138/186 [24:50<08:50, 11.06s/it] 75%|███████▍  | 139/186 [25:01<08:40, 11.08s/it] 75%|███████▌  | 140/186 [25:12<08:28, 11.05s/it] 76%|███████▌  | 141/186 [25:23<08:17, 11.05s/it] 76%|███████▋  | 142/186 [25:34<08:04, 11.02s/it] 77%|███████▋  | 143/186 [25:45<07:53, 11.02s/it] 77%|███████▋  | 144/186 [25:56<07:38, 10.93s/it] 78%|███████▊  | 145/186 [26:07<07:28, 10.94s/it] 78%|███████▊  | 146/186 [26:18<07:18, 10.96s/it] 79%|███████▉  | 147/186 [26:28<07:06, 10.94s/it] 80%|███████▉  | 148/186 [26:39<06:54, 10.91s/it] 80%|████████  | 149/186 [26:50<06:43, 10.90s/it] 81%|████████  | 150/186 [27:01<06:30, 10.86s/it] 81%|████████  | 151/186 [27:12<06:20, 10.88s/it] 82%|████████▏ | 152/186 [27:23<06:09, 10.87s/it] 82%|████████▏ | 153/186 [27:34<05:58, 10.85s/it] 83%|████████▎ | 154/186 [27:45<05:49, 10.91s/it] 83%|████████▎ | 155/186 [27:56<05:40, 10.98s/it] 84%|████████▍ | 156/186 [28:07<05:29, 10.99s/it] 84%|████████▍ | 157/186 [28:18<05:19, 11.00s/it] 85%|████████▍ | 158/186 [28:29<05:07, 10.98s/it] 85%|████████▌ | 159/186 [28:40<04:55, 10.95s/it] 86%|████████▌ | 160/186 [28:51<04:46, 11.02s/it] 87%|████████▋ | 161/186 [29:01<04:33, 10.93s/it] 87%|████████▋ | 162/186 [29:13<04:23, 10.99s/it] 88%|████████▊ | 163/186 [29:23<04:12, 10.96s/it] 88%|████████▊ | 164/186 [29:34<04:01, 10.97s/it] 89%|████████▊ | 165/186 [29:46<03:51, 11.04s/it] 89%|████████▉ | 166/186 [29:57<03:40, 11.05s/it] 90%|████████▉ | 167/186 [30:07<03:28, 10.95s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 90%|█████████ | 168/186 [30:18<03:17, 10.95s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 91%|█████████ | 169/186 [30:29<03:06, 10.95s/it] 91%|█████████▏| 170/186 [30:40<02:55, 10.98s/it] 92%|█████████▏| 171/186 [30:52<02:46, 11.08s/it] 92%|█████████▏| 172/186 [31:03<02:34, 11.01s/it] 93%|█████████▎| 173/186 [31:13<02:22, 10.96s/it] 94%|█████████▎| 174/186 [31:24<02:11, 10.92s/it] 94%|█████████▍| 175/186 [31:35<02:00, 10.95s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 95%|█████████▍| 176/186 [31:46<01:49, 10.94s/it] 95%|█████████▌| 177/186 [31:57<01:38, 10.95s/it] 96%|█████████▌| 178/186 [32:08<01:27, 10.91s/it] 96%|█████████▌| 179/186 [32:19<01:16, 10.90s/it] 97%|█████████▋| 180/186 [32:30<01:05, 10.90s/it]WARNING: tokenization mismatch: 1 vs. 48. (ignored)
 97%|█████████▋| 181/186 [32:41<00:54, 10.89s/it] 98%|█████████▊| 182/186 [32:52<00:44, 11.15s/it] 98%|█████████▊| 183/186 [33:03<00:33, 11.12s/it] 99%|█████████▉| 184/186 [33:14<00:22, 11.11s/it] 99%|█████████▉| 185/186 [33:26<00:11, 11.27s/it]100%|██████████| 186/186 [33:37<00:00, 11.20s/it]                                                 {'train_runtime': 2017.6778, 'train_samples_per_second': 6.659, 'train_steps_per_second': 0.092, 'train_loss': 0.14956906021282237, 'epoch': 1.0}
100%|██████████| 186/186 [33:37<00:00, 11.20s/it]100%|██████████| 186/186 [33:37<00:00, 10.85s/it]
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-2
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33msliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-2[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251017_071930-q79v86yy/logs[0m
[2025-10-17 07:54:54,367] [INFO] [launch.py:347:main] Process 2202995 exits successfully.
[2025-10-17 07:54:54,367] [INFO] [launch.py:347:main] Process 2202992 exits successfully.
[2025-10-17 07:54:54,367] [INFO] [launch.py:347:main] Process 2202996 exits successfully.
[2025-10-17 07:54:54,368] [INFO] [launch.py:347:main] Process 2202994 exits successfully.
[2025-10-17 07:54:54,368] [INFO] [launch.py:347:main] Process 2202993 exits successfully.
[2025-10-17 07:54:59,373] [INFO] [launch.py:347:main] Process 2202991 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 07:55:02,589] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 07:55:03,143] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-17 07:55:03,143] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12423 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-2 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-2 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 2 --use_anchor False --use_sample_weight True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 07:55:04,620] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 07:55:05,191] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-17 07:55:05,191] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-17 07:55:05,192] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-17 07:55:05,192] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-17 07:55:05,192] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 07:55:08,712] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-17 07:55:08,897] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 07:55:09,040] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 07:55:09,211] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 07:55:09,898] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-17 07:55:10,119] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 07:55:10,119] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 07:55:10,178] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 07:55:10,230] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 07:55:10,596] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 07:55:10,596] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 07:55:10,643] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 07:55:10,643] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251017_075511-nsdn9qrn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-2
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/nsdn9qrn
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.11s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.31s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.77s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.12s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.28s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.29s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.32s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.35s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.64s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.69s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.74s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.47s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.79s/it]
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.12s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.05s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.20s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.69s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.05s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.12s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.87s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.22s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.52s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.84s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.44s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.75s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.54s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.89s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.71s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.03s/it]
Formatting inputs...Skip in lazy mode
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:3')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:1')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:2')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:5')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:4')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:0')
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.014033317565917969 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.010633468627929688 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10285687446594238 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.012917280197143555 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.1011502742767334 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10217022895812988 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Token indices sequence length is longer than the specified maximum sequence length for this model (1297 > 1048). Running this sequence through the model will result in indexing errors
  1%|          | 1/186 [00:14<45:34, 14.78s/it]  1%|          | 2/186 [00:23<34:20, 11.20s/it]  2%|▏         | 3/186 [00:32<31:00, 10.17s/it]  2%|▏         | 4/186 [00:42<30:19, 10.00s/it]  3%|▎         | 5/186 [00:51<29:58,  9.94s/it]  3%|▎         | 6/186 [01:01<29:48,  9.94s/it]  4%|▍         | 7/186 [01:11<29:35,  9.92s/it]  4%|▍         | 8/186 [01:21<29:33,  9.96s/it]  5%|▍         | 9/186 [01:31<29:28,  9.99s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1072 > 1048). Running this sequence through the model will result in indexing errors
  5%|▌         | 10/186 [01:42<29:44, 10.14s/it]  6%|▌         | 11/186 [01:52<29:43, 10.19s/it]  6%|▋         | 12/186 [02:02<29:25, 10.15s/it]  7%|▋         | 13/186 [02:12<29:17, 10.16s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1074 > 1048). Running this sequence through the model will result in indexing errors
  8%|▊         | 14/186 [02:23<29:05, 10.15s/it]  8%|▊         | 15/186 [02:33<29:07, 10.22s/it]  9%|▊         | 16/186 [02:43<29:13, 10.32s/it]  9%|▉         | 17/186 [02:54<29:13, 10.38s/it] 10%|▉         | 18/186 [03:04<29:01, 10.36s/it] 10%|█         | 19/186 [03:15<28:57, 10.40s/it] 11%|█         | 20/186 [03:25<28:46, 10.40s/it]WARNING: tokenization mismatch: 1 vs. 48. (ignored)
Token indices sequence length is longer than the specified maximum sequence length for this model (1069 > 1048). Running this sequence through the model will result in indexing errors
 11%|█▏        | 21/186 [03:36<28:52, 10.50s/it] 12%|█▏        | 22/186 [03:46<28:36, 10.47s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1263 > 1048). Running this sequence through the model will result in indexing errors
 12%|█▏        | 23/186 [03:57<28:26, 10.47s/it] 13%|█▎        | 24/186 [04:08<28:41, 10.62s/it] 13%|█▎        | 25/186 [04:18<28:25, 10.60s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1073 > 1048). Running this sequence through the model will result in indexing errors
 14%|█▍        | 26/186 [04:29<28:24, 10.65s/it] 15%|█▍        | 27/186 [04:40<28:11, 10.64s/it] 15%|█▌        | 28/186 [04:50<27:49, 10.56s/it]WARNING: tokenization mismatch: 1 vs. 47. (ignored)
 16%|█▌        | 29/186 [05:00<27:29, 10.51s/it] 16%|█▌        | 30/186 [05:11<27:20, 10.51s/it] 17%|█▋        | 31/186 [05:22<27:12, 10.53s/it] 17%|█▋        | 32/186 [05:32<27:16, 10.63s/it] 18%|█▊        | 33/186 [05:43<27:01, 10.60s/it] 18%|█▊        | 34/186 [05:54<27:05, 10.70s/it] 19%|█▉        | 35/186 [06:05<26:55, 10.70s/it] 19%|█▉        | 36/186 [06:15<26:32, 10.62s/it] 20%|█▉        | 37/186 [06:26<26:24, 10.64s/it] 20%|██        | 38/186 [06:36<26:08, 10.60s/it] 21%|██        | 39/186 [06:47<26:01, 10.62s/it] 22%|██▏       | 40/186 [06:58<25:54, 10.65s/it] 22%|██▏       | 41/186 [07:08<25:39, 10.62s/it] 23%|██▎       | 42/186 [07:19<25:32, 10.64s/it] 23%|██▎       | 43/186 [07:30<25:25, 10.67s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 24%|██▎       | 44/186 [07:40<25:04, 10.60s/it] 24%|██▍       | 45/186 [07:50<24:43, 10.52s/it] 25%|██▍       | 46/186 [08:01<24:38, 10.56s/it] 25%|██▌       | 47/186 [08:12<24:34, 10.61s/it] 26%|██▌       | 48/186 [08:22<24:19, 10.58s/it] 26%|██▋       | 49/186 [08:33<24:07, 10.56s/it] 27%|██▋       | 50/186 [08:44<24:07, 10.65s/it] 27%|██▋       | 51/186 [08:54<23:50, 10.60s/it] 28%|██▊       | 52/186 [09:05<23:33, 10.55s/it] 28%|██▊       | 53/186 [09:15<23:23, 10.55s/it] 29%|██▉       | 54/186 [09:26<23:09, 10.52s/it] 30%|██▉       | 55/186 [09:36<23:03, 10.56s/it] 30%|███       | 56/186 [09:47<22:51, 10.55s/it] 31%|███       | 57/186 [09:57<22:41, 10.56s/it] 31%|███       | 58/186 [10:08<22:37, 10.61s/it] 32%|███▏      | 59/186 [10:19<22:49, 10.79s/it] 32%|███▏      | 60/186 [10:30<22:37, 10.77s/it] 33%|███▎      | 61/186 [10:41<22:24, 10.76s/it] 33%|███▎      | 62/186 [10:51<22:06, 10.70s/it] 34%|███▍      | 63/186 [11:02<21:56, 10.70s/it] 34%|███▍      | 64/186 [11:13<21:51, 10.75s/it] 35%|███▍      | 65/186 [11:23<21:36, 10.71s/it] 35%|███▌      | 66/186 [11:34<21:20, 10.67s/it] 36%|███▌      | 67/186 [11:45<21:10, 10.67s/it] 37%|███▋      | 68/186 [11:55<21:01, 10.69s/it] 37%|███▋      | 69/186 [12:07<21:07, 10.83s/it] 38%|███▊      | 70/186 [12:18<21:05, 10.91s/it] 38%|███▊      | 71/186 [12:28<20:41, 10.80s/it] 39%|███▊      | 72/186 [12:39<20:30, 10.79s/it] 39%|███▉      | 73/186 [12:50<20:18, 10.78s/it] 40%|███▉      | 74/186 [13:01<20:14, 10.84s/it] 40%|████      | 75/186 [13:11<19:58, 10.80s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 41%|████      | 76/186 [13:22<19:41, 10.75s/it] 41%|████▏     | 77/186 [13:33<19:37, 10.80s/it] 42%|████▏     | 78/186 [13:44<19:26, 10.80s/it] 42%|████▏     | 79/186 [13:54<19:07, 10.72s/it] 43%|████▎     | 80/186 [14:05<18:59, 10.75s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 44%|████▎     | 81/186 [14:16<18:44, 10.71s/it] 44%|████▍     | 82/186 [14:26<18:23, 10.61s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 45%|████▍     | 83/186 [14:37<18:18, 10.66s/it] 45%|████▌     | 84/186 [14:48<18:07, 10.66s/it] 46%|████▌     | 85/186 [14:58<17:56, 10.66s/it] 46%|████▌     | 86/186 [15:09<17:43, 10.64s/it] 47%|████▋     | 87/186 [15:19<17:32, 10.63s/it] 47%|████▋     | 88/186 [15:30<17:23, 10.65s/it] 48%|████▊     | 89/186 [15:41<17:12, 10.64s/it] 48%|████▊     | 90/186 [15:51<17:01, 10.64s/it] 49%|████▉     | 91/186 [16:02<16:57, 10.71s/it] 49%|████▉     | 92/186 [16:13<16:49, 10.74s/it] 50%|█████     | 93/186 [16:24<16:41, 10.76s/it] 51%|█████     | 94/186 [16:34<16:24, 10.70s/it] 51%|█████     | 95/186 [16:45<16:07, 10.63s/it] 52%|█████▏    | 96/186 [16:56<15:57, 10.64s/it] 52%|█████▏    | 97/186 [17:06<15:47, 10.64s/it] 53%|█████▎    | 98/186 [17:17<15:37, 10.65s/it] 53%|█████▎    | 99/186 [17:27<15:25, 10.64s/it] 54%|█████▍    | 100/186 [17:39<15:24, 10.75s/it] 54%|█████▍    | 101/186 [17:49<15:10, 10.71s/it] 55%|█████▍    | 102/186 [18:00<14:52, 10.63s/it] 55%|█████▌    | 103/186 [18:10<14:43, 10.64s/it] 56%|█████▌    | 104/186 [18:21<14:37, 10.71s/it] 56%|█████▋    | 105/186 [18:32<14:24, 10.68s/it] 57%|█████▋    | 106/186 [18:42<14:10, 10.63s/it] 58%|█████▊    | 107/186 [18:53<14:04, 10.69s/it] 58%|█████▊    | 108/186 [19:04<13:53, 10.68s/it] 59%|█████▊    | 109/186 [19:14<13:44, 10.71s/it] 59%|█████▉    | 110/186 [19:25<13:38, 10.77s/it] 60%|█████▉    | 111/186 [19:36<13:29, 10.79s/it] 60%|██████    | 112/186 [19:47<13:18, 10.78s/it] 61%|██████    | 113/186 [19:58<13:09, 10.81s/it] 61%|██████▏   | 114/186 [20:09<13:04, 10.89s/it] 62%|██████▏   | 115/186 [20:20<12:48, 10.82s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 62%|██████▏   | 116/186 [20:30<12:38, 10.83s/it] 63%|██████▎   | 117/186 [20:41<12:22, 10.75s/it] 63%|██████▎   | 118/186 [20:52<12:11, 10.76s/it] 64%|██████▍   | 119/186 [21:03<12:00, 10.75s/it] 65%|██████▍   | 120/186 [21:13<11:49, 10.75s/it] 65%|██████▌   | 121/186 [21:24<11:39, 10.76s/it] 66%|██████▌   | 122/186 [21:35<11:26, 10.72s/it] 66%|██████▌   | 123/186 [21:46<11:17, 10.75s/it] 67%|██████▋   | 124/186 [21:56<11:05, 10.73s/it] 67%|██████▋   | 125/186 [22:07<10:55, 10.74s/it] 68%|██████▊   | 126/186 [22:18<10:42, 10.72s/it] 68%|██████▊   | 127/186 [22:28<10:32, 10.72s/it] 69%|██████▉   | 128/186 [22:39<10:21, 10.72s/it] 69%|██████▉   | 129/186 [22:50<10:09, 10.69s/it] 70%|██████▉   | 130/186 [23:00<09:57, 10.68s/it] 70%|███████   | 131/186 [23:11<09:50, 10.73s/it] 71%|███████   | 132/186 [23:22<09:39, 10.73s/it] 72%|███████▏  | 133/186 [23:33<09:25, 10.68s/it] 72%|███████▏  | 134/186 [23:43<09:18, 10.74s/it] 73%|███████▎  | 135/186 [23:54<09:09, 10.77s/it] 73%|███████▎  | 136/186 [24:05<08:59, 10.78s/it] 74%|███████▎  | 137/186 [24:16<08:54, 10.91s/it] 74%|███████▍  | 138/186 [24:27<08:42, 10.89s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 75%|███████▍  | 139/186 [24:38<08:30, 10.87s/it] 75%|███████▌  | 140/186 [24:49<08:18, 10.84s/it] 76%|███████▌  | 141/186 [24:59<08:06, 10.82s/it] 76%|███████▋  | 142/186 [25:10<07:55, 10.80s/it] 77%|███████▋  | 143/186 [25:21<07:42, 10.76s/it] 77%|███████▋  | 144/186 [25:31<07:30, 10.72s/it] 78%|███████▊  | 145/186 [25:43<07:23, 10.81s/it] 78%|███████▊  | 146/186 [25:53<07:10, 10.76s/it] 79%|███████▉  | 147/186 [26:04<07:00, 10.78s/it] 80%|███████▉  | 148/186 [26:15<06:52, 10.86s/it] 80%|████████  | 149/186 [26:26<06:41, 10.86s/it] 81%|████████  | 150/186 [26:37<06:31, 10.87s/it] 81%|████████  | 151/186 [26:48<06:19, 10.85s/it] 82%|████████▏ | 152/186 [26:58<06:06, 10.77s/it] 82%|████████▏ | 153/186 [27:09<05:56, 10.80s/it] 83%|████████▎ | 154/186 [27:20<05:46, 10.82s/it] 83%|████████▎ | 155/186 [27:31<05:35, 10.82s/it] 84%|████████▍ | 156/186 [27:42<05:32, 11.10s/it] 84%|████████▍ | 157/186 [27:53<05:18, 11.00s/it] 85%|████████▍ | 158/186 [28:04<05:06, 10.94s/it] 85%|████████▌ | 159/186 [28:15<04:53, 10.89s/it] 86%|████████▌ | 160/186 [28:26<04:43, 10.91s/it] 87%|████████▋ | 161/186 [28:37<04:32, 10.89s/it] 87%|████████▋ | 162/186 [28:48<04:23, 10.97s/it] 88%|████████▊ | 163/186 [28:59<04:11, 10.92s/it] 88%|████████▊ | 164/186 [29:09<03:59, 10.89s/it] 89%|████████▊ | 165/186 [29:20<03:47, 10.83s/it] 89%|████████▉ | 166/186 [29:31<03:37, 10.89s/it] 90%|████████▉ | 167/186 [29:42<03:25, 10.83s/it] 90%|█████████ | 168/186 [29:52<03:14, 10.78s/it] 91%|█████████ | 169/186 [30:03<03:02, 10.71s/it] 91%|█████████▏| 170/186 [30:14<02:51, 10.74s/it] 92%|█████████▏| 171/186 [30:25<02:42, 10.83s/it] 92%|█████████▏| 172/186 [30:36<02:32, 10.88s/it] 93%|█████████▎| 173/186 [30:47<02:21, 10.85s/it] 94%|█████████▎| 174/186 [30:57<02:09, 10.81s/it] 94%|█████████▍| 175/186 [31:08<01:59, 10.83s/it] 95%|█████████▍| 176/186 [31:19<01:48, 10.81s/it] 95%|█████████▌| 177/186 [31:30<01:37, 10.81s/it] 96%|█████████▌| 178/186 [31:41<01:26, 10.79s/it] 96%|█████████▌| 179/186 [31:51<01:15, 10.79s/it] 97%|█████████▋| 180/186 [32:02<01:04, 10.76s/it] 97%|█████████▋| 181/186 [32:13<00:53, 10.76s/it] 98%|█████████▊| 182/186 [32:24<00:43, 10.84s/it] 98%|█████████▊| 183/186 [32:35<00:32, 10.79s/it] 99%|█████████▉| 184/186 [32:45<00:21, 10.79s/it] 99%|█████████▉| 185/186 [32:57<00:10, 10.92s/it]100%|██████████| 186/186 [33:07<00:00, 10.92s/it]                                                 {'train_runtime': 1987.9674, 'train_samples_per_second': 6.758, 'train_steps_per_second': 0.094, 'train_loss': 0.1684548777918662, 'epoch': 1.0}
100%|██████████| 186/186 [33:07<00:00, 10.92s/it]100%|██████████| 186/186 [33:07<00:00, 10.69s/it]
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-2
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33msliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-2[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251017_075511-nsdn9qrn/logs[0m
[2025-10-17 08:30:02,565] [INFO] [launch.py:347:main] Process 2204726 exits successfully.
[2025-10-17 08:30:02,566] [INFO] [launch.py:347:main] Process 2204727 exits successfully.
[2025-10-17 08:30:02,566] [INFO] [launch.py:347:main] Process 2204724 exits successfully.
[2025-10-17 08:30:03,568] [INFO] [launch.py:347:main] Process 2204723 exits successfully.
[2025-10-17 08:30:03,568] [INFO] [launch.py:347:main] Process 2204725 exits successfully.
[2025-10-17 08:30:07,573] [INFO] [launch.py:347:main] Process 2204722 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 08:30:10,896] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 08:30:11,451] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-17 08:30:11,451] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12423 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1.5e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-2 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-2 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 0.5 --use_anchor False --use_sample_weight True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 08:30:13,340] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 08:30:13,930] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-17 08:30:13,931] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-17 08:30:13,931] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-17 08:30:13,931] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-17 08:30:13,931] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 08:30:17,576] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 08:30:17,701] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 08:30:17,894] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 08:30:17,998] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 08:30:17,998] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-10-17 08:30:18,151] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 08:30:18,470] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
[2025-10-17 08:30:18,939] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][2025-10-17 08:30:19,133] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
[2025-10-17 08:30:19,186] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 08:30:19,335] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 08:30:19,540] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 08:30:19,675] [INFO] [comm.py:637:init_distributed] cdb=None
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: creating run
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251017_083019-2cc1928w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-2
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/2cc1928w
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.91s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.67s/it]
to bfloat16...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.84s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.71s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.03s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.84s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.09s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.92s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.98s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.77s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.11s/it]
to bfloat16...
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.68s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.01s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.67s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.01s/it]
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.14s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.87s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.87s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.21s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.61s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.95s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.14s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.13s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.06s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.12s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.59s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.67s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.04s/it]
Formatting inputs...Skip in lazy mode
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.73s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.08s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.73s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.09s/it]
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:5')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:3')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:2')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:0')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:1')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:4')
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.010876178741455078 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.010030746459960938 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10146784782409668 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.01505136489868164 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10164308547973633 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10148239135742188 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/186 [00:09<30:05,  9.76s/it]  1%|          | 2/186 [00:18<27:42,  9.03s/it]  2%|▏         | 3/186 [00:26<27:06,  8.89s/it]  2%|▏         | 4/186 [00:36<27:08,  8.95s/it]  3%|▎         | 5/186 [00:45<27:38,  9.17s/it]  3%|▎         | 6/186 [00:55<27:48,  9.27s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
  4%|▍         | 7/186 [01:04<27:57,  9.37s/it]WARNING: tokenization mismatch: 1 vs. 47. (ignored)
  4%|▍         | 8/186 [01:14<27:58,  9.43s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1150 > 1048). Running this sequence through the model will result in indexing errors
  5%|▍         | 9/186 [01:24<28:21,  9.61s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
  5%|▌         | 10/186 [01:34<28:33,  9.73s/it]  6%|▌         | 11/186 [01:44<28:52,  9.90s/it]  6%|▋         | 12/186 [01:54<28:59, 10.00s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1074 > 1048). Running this sequence through the model will result in indexing errors
  7%|▋         | 13/186 [02:04<28:56, 10.04s/it]  8%|▊         | 14/186 [02:15<29:01, 10.12s/it]  8%|▊         | 15/186 [02:25<28:50, 10.12s/it]  9%|▊         | 16/186 [02:35<28:48, 10.17s/it]  9%|▉         | 17/186 [02:45<28:32, 10.13s/it] 10%|▉         | 18/186 [02:55<28:25, 10.15s/it] 10%|█         | 19/186 [03:05<28:16, 10.16s/it] 11%|█         | 20/186 [03:16<28:15, 10.22s/it] 11%|█▏        | 21/186 [03:26<28:16, 10.28s/it] 12%|█▏        | 22/186 [03:37<28:09, 10.30s/it] 12%|█▏        | 23/186 [03:47<28:05, 10.34s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 13%|█▎        | 24/186 [03:58<28:28, 10.55s/it] 13%|█▎        | 25/186 [04:08<28:09, 10.49s/it] 14%|█▍        | 26/186 [04:19<27:59, 10.49s/it] 15%|█▍        | 27/186 [04:29<27:51, 10.51s/it] 15%|█▌        | 28/186 [04:40<27:29, 10.44s/it] 16%|█▌        | 29/186 [04:50<27:21, 10.45s/it] 16%|█▌        | 30/186 [05:01<27:05, 10.42s/it] 17%|█▋        | 31/186 [05:11<26:54, 10.41s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 17%|█▋        | 32/186 [05:21<26:46, 10.43s/it] 18%|█▊        | 33/186 [05:32<26:43, 10.48s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1150 > 1048). Running this sequence through the model will result in indexing errors
 18%|█▊        | 34/186 [05:43<26:41, 10.54s/it] 19%|█▉        | 35/186 [05:54<26:44, 10.62s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1072 > 1048). Running this sequence through the model will result in indexing errors
 19%|█▉        | 36/186 [06:04<26:33, 10.63s/it] 20%|█▉        | 37/186 [06:15<26:19, 10.60s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1297 > 1048). Running this sequence through the model will result in indexing errors
 20%|██        | 38/186 [06:26<26:17, 10.66s/it] 21%|██        | 39/186 [06:37<26:24, 10.78s/it] 22%|██▏       | 40/186 [06:47<26:08, 10.74s/it] 22%|██▏       | 41/186 [06:58<25:50, 10.69s/it] 23%|██▎       | 42/186 [07:08<25:38, 10.68s/it] 23%|██▎       | 43/186 [07:19<25:23, 10.66s/it] 24%|██▎       | 44/186 [07:30<25:27, 10.76s/it] 24%|██▍       | 45/186 [07:41<25:26, 10.83s/it] 25%|██▍       | 46/186 [07:52<25:30, 10.93s/it] 25%|██▌       | 47/186 [08:03<25:08, 10.85s/it] 26%|██▌       | 48/186 [08:13<24:45, 10.77s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1159 > 1048). Running this sequence through the model will result in indexing errors
 26%|██▋       | 49/186 [08:24<24:37, 10.78s/it] 27%|██▋       | 50/186 [08:35<24:18, 10.72s/it] 27%|██▋       | 51/186 [08:45<23:58, 10.66s/it] 28%|██▊       | 52/186 [08:56<23:43, 10.62s/it] 28%|██▊       | 53/186 [09:07<23:32, 10.62s/it] 29%|██▉       | 54/186 [09:17<23:21, 10.62s/it] 30%|██▉       | 55/186 [09:28<23:15, 10.65s/it] 30%|███       | 56/186 [09:38<23:01, 10.63s/it] 31%|███       | 57/186 [09:49<22:45, 10.58s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 31%|███       | 58/186 [09:59<22:32, 10.56s/it] 32%|███▏      | 59/186 [10:10<22:27, 10.61s/it] 32%|███▏      | 60/186 [10:21<22:15, 10.60s/it] 33%|███▎      | 61/186 [10:32<22:12, 10.66s/it] 33%|███▎      | 62/186 [10:42<22:08, 10.71s/it] 34%|███▍      | 63/186 [10:53<21:58, 10.72s/it] 34%|███▍      | 64/186 [11:04<21:48, 10.73s/it] 35%|███▍      | 65/186 [11:15<21:39, 10.74s/it] 35%|███▌      | 66/186 [11:25<21:21, 10.68s/it] 36%|███▌      | 67/186 [11:36<21:07, 10.66s/it] 37%|███▋      | 68/186 [11:46<20:51, 10.61s/it] 37%|███▋      | 69/186 [11:58<21:04, 10.81s/it] 38%|███▊      | 70/186 [12:09<21:00, 10.86s/it] 38%|███▊      | 71/186 [12:19<20:45, 10.83s/it] 39%|███▊      | 72/186 [12:30<20:29, 10.79s/it] 39%|███▉      | 73/186 [12:41<20:17, 10.77s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 40%|███▉      | 74/186 [12:51<20:03, 10.75s/it] 40%|████      | 75/186 [13:02<19:50, 10.73s/it] 41%|████      | 76/186 [13:13<19:36, 10.70s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 41%|████▏     | 77/186 [13:23<19:28, 10.72s/it] 42%|████▏     | 78/186 [13:34<19:18, 10.73s/it] 42%|████▏     | 79/186 [13:45<19:09, 10.75s/it] 43%|████▎     | 80/186 [13:56<18:59, 10.75s/it] 44%|████▎     | 81/186 [14:06<18:41, 10.68s/it] 44%|████▍     | 82/186 [14:17<18:36, 10.74s/it] 45%|████▍     | 83/186 [14:28<18:29, 10.77s/it] 45%|████▌     | 84/186 [14:39<18:18, 10.77s/it] 46%|████▌     | 85/186 [14:49<18:05, 10.75s/it] 46%|████▌     | 86/186 [15:00<17:54, 10.75s/it] 47%|████▋     | 87/186 [15:11<17:46, 10.77s/it] 47%|████▋     | 88/186 [15:22<17:33, 10.75s/it] 48%|████▊     | 89/186 [15:33<17:28, 10.81s/it] 48%|████▊     | 90/186 [15:43<17:15, 10.78s/it] 49%|████▉     | 91/186 [15:55<17:15, 10.90s/it] 49%|████▉     | 92/186 [16:06<17:15, 11.01s/it] 50%|█████     | 93/186 [16:17<16:59, 10.96s/it] 51%|█████     | 94/186 [16:27<16:39, 10.87s/it] 51%|█████     | 95/186 [16:38<16:25, 10.83s/it] 52%|█████▏    | 96/186 [16:49<16:15, 10.84s/it] 52%|█████▏    | 97/186 [17:00<16:03, 10.83s/it] 53%|█████▎    | 98/186 [17:11<15:54, 10.85s/it] 53%|█████▎    | 99/186 [17:21<15:41, 10.82s/it] 54%|█████▍    | 100/186 [17:32<15:35, 10.88s/it] 54%|█████▍    | 101/186 [17:43<15:27, 10.92s/it] 55%|█████▍    | 102/186 [17:55<15:27, 11.04s/it] 55%|█████▌    | 103/186 [18:06<15:15, 11.03s/it] 56%|█████▌    | 104/186 [18:17<15:02, 11.01s/it] 56%|█████▋    | 105/186 [18:28<14:47, 10.96s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 57%|█████▋    | 106/186 [18:38<14:31, 10.90s/it] 58%|█████▊    | 107/186 [18:49<14:24, 10.94s/it] 58%|█████▊    | 108/186 [19:00<14:08, 10.88s/it] 59%|█████▊    | 109/186 [19:11<13:57, 10.88s/it] 59%|█████▉    | 110/186 [19:22<13:51, 10.94s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 60%|█████▉    | 111/186 [19:33<13:43, 10.97s/it] 60%|██████    | 112/186 [19:44<13:31, 10.97s/it] 61%|██████    | 113/186 [19:55<13:22, 11.00s/it] 61%|██████▏   | 114/186 [20:06<13:14, 11.04s/it] 62%|██████▏   | 115/186 [20:17<13:00, 10.99s/it] 62%|██████▏   | 116/186 [20:28<12:56, 11.09s/it]WARNING: tokenization mismatch: 1 vs. 48. (ignored)
 63%|██████▎   | 117/186 [20:40<12:44, 11.08s/it] 63%|██████▎   | 118/186 [20:50<12:29, 11.02s/it] 64%|██████▍   | 119/186 [21:01<12:14, 10.96s/it] 65%|██████▍   | 120/186 [21:12<11:58, 10.89s/it] 65%|██████▌   | 121/186 [21:23<11:49, 10.91s/it] 66%|██████▌   | 122/186 [21:34<11:41, 10.97s/it] 66%|██████▌   | 123/186 [21:46<11:45, 11.19s/it] 67%|██████▋   | 124/186 [21:57<11:35, 11.21s/it] 67%|██████▋   | 125/186 [22:08<11:15, 11.07s/it] 68%|██████▊   | 126/186 [22:19<10:59, 11.00s/it] 68%|██████▊   | 127/186 [22:30<10:49, 11.02s/it] 69%|██████▉   | 128/186 [22:40<10:36, 10.97s/it] 69%|██████▉   | 129/186 [22:51<10:24, 10.95s/it] 70%|██████▉   | 130/186 [23:02<10:13, 10.95s/it] 70%|███████   | 131/186 [23:13<10:02, 10.96s/it] 71%|███████   | 132/186 [23:24<09:50, 10.93s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 72%|███████▏  | 133/186 [23:35<09:40, 10.95s/it] 72%|███████▏  | 134/186 [23:46<09:32, 11.00s/it] 73%|███████▎  | 135/186 [23:57<09:23, 11.04s/it] 73%|███████▎  | 136/186 [24:08<09:11, 11.03s/it] 74%|███████▎  | 137/186 [24:20<09:06, 11.15s/it] 74%|███████▍  | 138/186 [24:31<08:53, 11.12s/it] 75%|███████▍  | 139/186 [24:42<08:45, 11.17s/it] 75%|███████▌  | 140/186 [24:53<08:31, 11.11s/it] 76%|███████▌  | 141/186 [25:04<08:14, 10.98s/it] 76%|███████▋  | 142/186 [25:15<08:02, 10.98s/it] 77%|███████▋  | 143/186 [25:26<07:50, 10.93s/it] 77%|███████▋  | 144/186 [25:37<07:40, 10.97s/it] 78%|███████▊  | 145/186 [25:47<07:25, 10.85s/it] 78%|███████▊  | 146/186 [25:58<07:14, 10.86s/it] 79%|███████▉  | 147/186 [26:09<07:03, 10.86s/it] 80%|███████▉  | 148/186 [26:20<06:52, 10.87s/it] 80%|████████  | 149/186 [26:31<06:44, 10.94s/it] 81%|████████  | 150/186 [26:42<06:36, 11.02s/it] 81%|████████  | 151/186 [26:53<06:24, 10.98s/it] 82%|████████▏ | 152/186 [27:04<06:12, 10.94s/it] 82%|████████▏ | 153/186 [27:15<06:01, 10.97s/it] 83%|████████▎ | 154/186 [27:26<05:51, 10.99s/it] 83%|████████▎ | 155/186 [27:37<05:39, 10.95s/it] 84%|████████▍ | 156/186 [27:48<05:29, 10.97s/it] 84%|████████▍ | 157/186 [27:59<05:17, 10.94s/it] 85%|████████▍ | 158/186 [28:10<05:05, 10.91s/it] 85%|████████▌ | 159/186 [28:21<04:54, 10.93s/it] 86%|████████▌ | 160/186 [28:32<04:44, 10.93s/it] 87%|████████▋ | 161/186 [28:42<04:32, 10.89s/it] 87%|████████▋ | 162/186 [28:53<04:23, 10.97s/it] 88%|████████▊ | 163/186 [29:04<04:11, 10.94s/it] 88%|████████▊ | 164/186 [29:15<03:59, 10.91s/it] 89%|████████▊ | 165/186 [29:26<03:49, 10.93s/it] 89%|████████▉ | 166/186 [29:37<03:38, 10.91s/it] 90%|████████▉ | 167/186 [29:48<03:27, 10.92s/it] 90%|█████████ | 168/186 [29:59<03:15, 10.88s/it] 91%|█████████ | 169/186 [30:10<03:04, 10.87s/it] 91%|█████████▏| 170/186 [30:20<02:53, 10.86s/it] 92%|█████████▏| 171/186 [30:32<02:44, 10.98s/it] 92%|█████████▏| 172/186 [30:43<02:33, 10.96s/it] 93%|█████████▎| 173/186 [30:53<02:22, 10.93s/it] 94%|█████████▎| 174/186 [31:04<02:11, 10.94s/it] 94%|█████████▍| 175/186 [31:15<01:59, 10.91s/it] 95%|█████████▍| 176/186 [31:26<01:49, 10.91s/it] 95%|█████████▌| 177/186 [31:37<01:38, 10.92s/it] 96%|█████████▌| 178/186 [31:48<01:27, 10.94s/it] 96%|█████████▌| 179/186 [31:59<01:16, 10.92s/it] 97%|█████████▋| 180/186 [32:10<01:05, 10.96s/it] 97%|█████████▋| 181/186 [32:21<00:55, 11.03s/it] 98%|█████████▊| 182/186 [32:32<00:44, 11.05s/it] 98%|█████████▊| 183/186 [32:44<00:33, 11.12s/it] 99%|█████████▉| 184/186 [32:55<00:22, 11.07s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 99%|█████████▉| 185/186 [33:06<00:11, 11.11s/it]100%|██████████| 186/186 [33:17<00:00, 11.06s/it]                                                 {'train_runtime': 1997.2486, 'train_samples_per_second': 6.727, 'train_steps_per_second': 0.093, 'train_loss': 0.11459068585467595, 'epoch': 1.0}
100%|██████████| 186/186 [33:17<00:00, 11.06s/it]100%|██████████| 186/186 [33:17<00:00, 10.74s/it]
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-2
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33msliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-2[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251017_083019-2cc1928w/logs[0m
[2025-10-17 09:05:23,477] [INFO] [launch.py:347:main] Process 2206458 exits successfully.
[2025-10-17 09:05:23,477] [INFO] [launch.py:347:main] Process 2206455 exits successfully.
[2025-10-17 09:05:23,477] [INFO] [launch.py:347:main] Process 2206457 exits successfully.
[2025-10-17 09:05:24,479] [INFO] [launch.py:347:main] Process 2206459 exits successfully.
[2025-10-17 09:05:24,479] [INFO] [launch.py:347:main] Process 2206456 exits successfully.
[2025-10-17 09:05:28,483] [INFO] [launch.py:347:main] Process 2206454 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 09:05:31,797] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 09:05:32,349] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-17 09:05:32,349] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12423 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1.5e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-2 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-2 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 1 --use_anchor False --use_sample_weight True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 09:05:33,820] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 09:05:34,387] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-17 09:05:34,387] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-17 09:05:34,387] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-17 09:05:34,387] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-17 09:05:34,387] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 09:05:37,966] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 09:05:38,302] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 09:05:38,437] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-17 09:05:38,604] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 09:05:38,923] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 09:05:38,925] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 09:05:38,925] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 09:05:38,976] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-17 09:05:39,310] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 09:05:39,397] [INFO] [comm.py:637:init_distributed] cdb=None
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
[2025-10-17 09:05:39,514] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 09:05:39,906] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 09:05:39,998] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251017_090540-iz2598ja
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-2
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/iz2598ja
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.20s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.07s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.24s/it]
to bfloat16...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.22s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.89s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.24s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.01s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.43s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.69s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.88s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.00s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.64s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.47s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.80s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.63s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.97s/it]
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.49s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.41s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.87s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.88s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.18s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.06s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.86s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.55s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.97s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.01s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.91s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.73s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.61s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.97s/it]
Formatting inputs...Skip in lazy mode
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:5')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:3')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:4')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:2')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:1')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:0')
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.014149188995361328 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.011311531066894531 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.1031043529510498 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.013868570327758789 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.1009833812713623 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.1026620864868164 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
WARNING: tokenization mismatch: 1 vs. 48. (ignored)
  1%|          | 1/186 [00:14<44:30, 14.44s/it]  1%|          | 2/186 [00:23<33:55, 11.06s/it]  2%|▏         | 3/186 [00:32<31:27, 10.31s/it]  2%|▏         | 4/186 [00:42<30:28, 10.05s/it]  3%|▎         | 5/186 [00:52<30:02,  9.96s/it]  3%|▎         | 6/186 [01:01<29:47,  9.93s/it]  4%|▍         | 7/186 [01:11<29:40,  9.94s/it]  4%|▍         | 8/186 [01:21<29:34,  9.97s/it]  5%|▍         | 9/186 [01:31<29:32, 10.01s/it]  5%|▌         | 10/186 [01:42<29:38, 10.10s/it]  6%|▌         | 11/186 [01:52<29:29, 10.11s/it]  6%|▋         | 12/186 [02:02<29:28, 10.17s/it]  7%|▋         | 13/186 [02:12<29:23, 10.19s/it]  8%|▊         | 14/186 [02:23<29:15, 10.20s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1075 > 1048). Running this sequence through the model will result in indexing errors
  8%|▊         | 15/186 [02:33<29:24, 10.32s/it]  9%|▊         | 16/186 [02:44<29:12, 10.31s/it]  9%|▉         | 17/186 [02:54<29:02, 10.31s/it] 10%|▉         | 18/186 [03:04<29:04, 10.38s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1263 > 1048). Running this sequence through the model will result in indexing errors
 10%|█         | 19/186 [03:15<29:06, 10.46s/it] 11%|█         | 20/186 [03:25<28:52, 10.44s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1073 > 1048). Running this sequence through the model will result in indexing errors
 11%|█▏        | 21/186 [03:36<28:58, 10.53s/it] 12%|█▏        | 22/186 [03:47<28:43, 10.51s/it] 12%|█▏        | 23/186 [03:57<28:21, 10.44s/it] 13%|█▎        | 24/186 [04:08<28:36, 10.60s/it] 13%|█▎        | 25/186 [04:18<28:15, 10.53s/it] 14%|█▍        | 26/186 [04:29<27:55, 10.47s/it] 15%|█▍        | 27/186 [04:39<27:41, 10.45s/it] 15%|█▌        | 28/186 [04:50<27:41, 10.52s/it] 16%|█▌        | 29/186 [05:00<27:26, 10.48s/it] 16%|█▌        | 30/186 [05:10<27:08, 10.44s/it] 17%|█▋        | 31/186 [05:21<27:04, 10.48s/it] 17%|█▋        | 32/186 [05:32<26:56, 10.50s/it]WARNING: tokenization mismatch: 1 vs. 47. (ignored)
 18%|█▊        | 33/186 [05:42<26:37, 10.44s/it] 18%|█▊        | 34/186 [05:52<26:29, 10.46s/it] 19%|█▉        | 35/186 [06:03<26:17, 10.45s/it] 19%|█▉        | 36/186 [06:13<26:00, 10.40s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1159 > 1048). Running this sequence through the model will result in indexing errors
 20%|█▉        | 37/186 [06:24<25:57, 10.46s/it] 20%|██        | 38/186 [06:34<25:47, 10.45s/it] 21%|██        | 39/186 [06:45<25:51, 10.56s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1200 > 1048). Running this sequence through the model will result in indexing errors
 22%|██▏       | 40/186 [06:56<25:44, 10.58s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 22%|██▏       | 41/186 [07:06<25:28, 10.54s/it] 23%|██▎       | 42/186 [07:17<25:20, 10.56s/it] 23%|██▎       | 43/186 [07:27<25:16, 10.61s/it] 24%|██▎       | 44/186 [07:38<24:59, 10.56s/it] 24%|██▍       | 45/186 [07:48<24:55, 10.61s/it] 25%|██▍       | 46/186 [07:59<24:39, 10.57s/it] 25%|██▌       | 47/186 [08:10<24:35, 10.62s/it] 26%|██▌       | 48/186 [08:20<24:27, 10.63s/it] 26%|██▋       | 49/186 [08:31<24:10, 10.59s/it] 27%|██▋       | 50/186 [08:41<24:02, 10.61s/it] 27%|██▋       | 51/186 [08:52<23:52, 10.61s/it] 28%|██▊       | 52/186 [09:03<23:40, 10.60s/it] 28%|██▊       | 53/186 [09:13<23:30, 10.60s/it] 29%|██▉       | 54/186 [09:24<23:09, 10.52s/it] 30%|██▉       | 55/186 [09:34<23:01, 10.55s/it] 30%|███       | 56/186 [09:45<22:53, 10.57s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 31%|███       | 57/186 [09:55<22:37, 10.53s/it] 31%|███       | 58/186 [10:06<22:33, 10.57s/it] 32%|███▏      | 59/186 [10:17<22:29, 10.63s/it] 32%|███▏      | 60/186 [10:27<22:16, 10.61s/it] 33%|███▎      | 61/186 [10:38<22:10, 10.64s/it] 33%|███▎      | 62/186 [10:48<21:52, 10.58s/it] 34%|███▍      | 63/186 [10:59<21:38, 10.56s/it] 34%|███▍      | 64/186 [11:10<21:33, 10.60s/it] 35%|███▍      | 65/186 [11:20<21:16, 10.55s/it] 35%|███▌      | 66/186 [11:31<21:09, 10.58s/it] 36%|███▌      | 67/186 [11:41<20:55, 10.55s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 37%|███▋      | 68/186 [11:52<20:50, 10.59s/it] 37%|███▋      | 69/186 [12:03<20:59, 10.76s/it] 38%|███▊      | 70/186 [12:14<20:53, 10.81s/it] 38%|███▊      | 71/186 [12:25<20:34, 10.74s/it] 39%|███▊      | 72/186 [12:35<20:19, 10.70s/it] 39%|███▉      | 73/186 [12:46<20:06, 10.68s/it] 40%|███▉      | 74/186 [12:56<19:50, 10.63s/it] 40%|████      | 75/186 [13:07<19:36, 10.60s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 41%|████      | 76/186 [13:17<19:23, 10.58s/it] 41%|████▏     | 77/186 [13:28<19:14, 10.59s/it] 42%|████▏     | 78/186 [13:39<19:06, 10.62s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1221 > 1048). Running this sequence through the model will result in indexing errors
 42%|████▏     | 79/186 [13:49<18:59, 10.65s/it] 43%|████▎     | 80/186 [14:00<18:48, 10.65s/it] 44%|████▎     | 81/186 [14:11<18:44, 10.71s/it] 44%|████▍     | 82/186 [14:22<18:33, 10.71s/it] 45%|████▍     | 83/186 [14:32<18:26, 10.74s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
 45%|████▌     | 84/186 [14:43<18:16, 10.75s/it] 46%|████▌     | 85/186 [14:54<18:04, 10.74s/it] 46%|████▌     | 86/186 [15:04<17:49, 10.70s/it] 47%|████▋     | 87/186 [15:15<17:36, 10.67s/it] 47%|████▋     | 88/186 [15:26<17:25, 10.66s/it] 48%|████▊     | 89/186 [15:36<17:16, 10.69s/it] 48%|████▊     | 90/186 [15:47<17:03, 10.66s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 49%|████▉     | 91/186 [15:58<17:02, 10.76s/it] 49%|████▉     | 92/186 [16:09<16:58, 10.83s/it] 50%|█████     | 93/186 [16:20<16:51, 10.88s/it]WARNING: tokenization mismatch: 1 vs. 48. (ignored)
 51%|█████     | 94/186 [16:31<16:35, 10.82s/it] 51%|█████     | 95/186 [16:42<16:29, 10.87s/it] 52%|█████▏    | 96/186 [16:52<16:10, 10.79s/it] 52%|█████▏    | 97/186 [17:03<15:59, 10.78s/it] 53%|█████▎    | 98/186 [17:14<15:44, 10.74s/it] 53%|█████▎    | 99/186 [17:24<15:29, 10.69s/it] 54%|█████▍    | 100/186 [17:35<15:23, 10.74s/it] 54%|█████▍    | 101/186 [17:46<15:16, 10.78s/it] 55%|█████▍    | 102/186 [17:57<15:04, 10.76s/it] 55%|█████▌    | 103/186 [18:07<14:50, 10.73s/it] 56%|█████▌    | 104/186 [18:18<14:41, 10.75s/it] 56%|█████▋    | 105/186 [18:29<14:26, 10.70s/it] 57%|█████▋    | 106/186 [18:39<14:15, 10.69s/it] 58%|█████▊    | 107/186 [18:50<14:08, 10.74s/it] 58%|█████▊    | 108/186 [19:01<13:59, 10.76s/it] 59%|█████▊    | 109/186 [19:12<13:47, 10.75s/it] 59%|█████▉    | 110/186 [19:23<13:44, 10.84s/it] 60%|█████▉    | 111/186 [19:34<13:39, 10.93s/it] 60%|██████    | 112/186 [19:45<13:26, 10.89s/it] 61%|██████    | 113/186 [19:56<13:13, 10.86s/it] 61%|██████▏   | 114/186 [20:07<13:07, 10.94s/it] 62%|██████▏   | 115/186 [20:18<12:53, 10.90s/it] 62%|██████▏   | 116/186 [20:29<12:49, 10.99s/it] 63%|██████▎   | 117/186 [20:40<12:42, 11.05s/it] 63%|██████▎   | 118/186 [20:51<12:30, 11.04s/it] 64%|██████▍   | 119/186 [21:02<12:16, 11.00s/it] 65%|██████▍   | 120/186 [21:13<12:01, 10.94s/it] 65%|██████▌   | 121/186 [21:23<11:47, 10.89s/it] 66%|██████▌   | 122/186 [21:34<11:35, 10.87s/it] 66%|██████▌   | 123/186 [21:46<11:31, 10.98s/it] 67%|██████▋   | 124/186 [21:57<11:22, 11.00s/it] 67%|██████▋   | 125/186 [22:07<11:04, 10.89s/it] 68%|██████▊   | 126/186 [22:18<10:51, 10.86s/it] 68%|██████▊   | 127/186 [22:29<10:47, 10.97s/it] 69%|██████▉   | 128/186 [22:40<10:31, 10.88s/it] 69%|██████▉   | 129/186 [22:51<10:20, 10.89s/it] 70%|██████▉   | 130/186 [23:02<10:08, 10.86s/it] 70%|███████   | 131/186 [23:13<09:59, 10.91s/it] 71%|███████   | 132/186 [23:23<09:46, 10.87s/it] 72%|███████▏  | 133/186 [23:34<09:37, 10.90s/it] 72%|███████▏  | 134/186 [23:45<09:27, 10.91s/it] 73%|███████▎  | 135/186 [23:56<09:15, 10.89s/it] 73%|███████▎  | 136/186 [24:07<09:01, 10.83s/it] 74%|███████▎  | 137/186 [24:18<08:59, 11.02s/it] 74%|███████▍  | 138/186 [24:29<08:50, 11.04s/it] 75%|███████▍  | 139/186 [24:41<08:41, 11.10s/it] 75%|███████▌  | 140/186 [24:52<08:30, 11.11s/it] 76%|███████▌  | 141/186 [25:03<08:15, 11.01s/it] 76%|███████▋  | 142/186 [25:13<08:02, 10.96s/it] 77%|███████▋  | 143/186 [25:24<07:49, 10.93s/it] 77%|███████▋  | 144/186 [25:35<07:42, 11.00s/it] 78%|███████▊  | 145/186 [25:46<07:28, 10.95s/it] 78%|███████▊  | 146/186 [25:57<07:18, 10.95s/it] 79%|███████▉  | 147/186 [26:08<07:04, 10.89s/it] 80%|███████▉  | 148/186 [26:19<06:54, 10.90s/it] 80%|████████  | 149/186 [26:30<06:44, 10.93s/it] 81%|████████  | 150/186 [26:41<06:33, 10.92s/it] 81%|████████  | 151/186 [26:52<06:25, 11.00s/it] 82%|████████▏ | 152/186 [27:03<06:16, 11.06s/it] 82%|████████▏ | 153/186 [27:14<06:01, 10.96s/it] 83%|████████▎ | 154/186 [27:25<05:50, 10.96s/it] 83%|████████▎ | 155/186 [27:36<05:38, 10.92s/it] 84%|████████▍ | 156/186 [27:47<05:31, 11.06s/it] 84%|████████▍ | 157/186 [27:58<05:19, 11.00s/it] 85%|████████▍ | 158/186 [28:09<05:07, 11.00s/it] 85%|████████▌ | 159/186 [28:20<04:58, 11.05s/it] 86%|████████▌ | 160/186 [28:31<04:49, 11.12s/it] 87%|████████▋ | 161/186 [28:43<04:39, 11.18s/it] 87%|████████▋ | 162/186 [28:54<04:28, 11.20s/it] 88%|████████▊ | 163/186 [29:05<04:14, 11.08s/it] 88%|████████▊ | 164/186 [29:16<04:03, 11.09s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 89%|████████▊ | 165/186 [29:27<03:53, 11.10s/it] 89%|████████▉ | 166/186 [29:38<03:40, 11.02s/it] 90%|████████▉ | 167/186 [29:49<03:29, 11.00s/it] 90%|█████████ | 168/186 [30:00<03:16, 10.93s/it] 91%|█████████ | 169/186 [30:10<03:05, 10.92s/it] 91%|█████████▏| 170/186 [30:21<02:53, 10.87s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 92%|█████████▏| 171/186 [30:32<02:43, 10.91s/it] 92%|█████████▏| 172/186 [30:43<02:32, 10.88s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 93%|█████████▎| 173/186 [30:54<02:22, 10.97s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 94%|█████████▎| 174/186 [31:05<02:11, 10.95s/it] 94%|█████████▍| 175/186 [31:16<02:00, 10.96s/it] 95%|█████████▍| 176/186 [31:27<01:50, 11.03s/it] 95%|█████████▌| 177/186 [31:38<01:39, 11.02s/it] 96%|█████████▌| 178/186 [31:49<01:28, 11.07s/it] 96%|█████████▌| 179/186 [32:00<01:17, 11.03s/it] 97%|█████████▋| 180/186 [32:11<01:06, 11.05s/it] 97%|█████████▋| 181/186 [32:22<00:55, 11.03s/it] 98%|█████████▊| 182/186 [32:34<00:44, 11.08s/it] 98%|█████████▊| 183/186 [32:45<00:33, 11.10s/it] 99%|█████████▉| 184/186 [32:56<00:22, 11.00s/it] 99%|█████████▉| 185/186 [33:07<00:11, 11.09s/it]100%|██████████| 186/186 [33:18<00:00, 11.04s/it]                                                 {'train_runtime': 1998.2755, 'train_samples_per_second': 6.723, 'train_steps_per_second': 0.093, 'train_loss': 0.11900371633550172, 'epoch': 1.0}
100%|██████████| 186/186 [33:18<00:00, 11.04s/it]100%|██████████| 186/186 [33:18<00:00, 10.74s/it]
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-2
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33msliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-2[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251017_090540-iz2598ja/logs[0m
[2025-10-17 09:40:39,865] [INFO] [launch.py:347:main] Process 2208229 exits successfully.
[2025-10-17 09:40:39,866] [INFO] [launch.py:347:main] Process 2208228 exits successfully.
[2025-10-17 09:40:39,866] [INFO] [launch.py:347:main] Process 2208230 exits successfully.
[2025-10-17 09:40:39,866] [INFO] [launch.py:347:main] Process 2208227 exits successfully.
[2025-10-17 09:40:39,866] [INFO] [launch.py:347:main] Process 2208231 exits successfully.
[2025-10-17 09:40:44,872] [INFO] [launch.py:347:main] Process 2208226 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 09:40:48,092] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 09:40:48,650] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-17 09:40:48,650] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12423 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1.5e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-2 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-2 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 2 --use_anchor False --use_sample_weight True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 09:40:50,308] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 09:40:50,890] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-17 09:40:50,890] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-17 09:40:50,890] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-17 09:40:50,890] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-17 09:40:50,890] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 09:40:55,317] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 09:40:55,325] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 09:40:55,333] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 09:40:55,647] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 09:40:55,665] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 09:40:55,665] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-10-17 09:40:55,670] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 09:40:55,732] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 09:40:56,064] [INFO] [comm.py:637:init_distributed] cdb=None
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-10-17 09:40:56,315] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-17 09:40:56,715] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 09:40:56,886] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 09:40:57,306] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251017_094057-4l1dopcs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-2
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/4l1dopcs
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.15s/it]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.16s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.16s/it]
to bfloat16...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.12s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.15s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.05s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.82s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.81s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.64s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.77s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.11s/it]
to bfloat16...
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.65s/it]
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.47s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.37s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.42s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.49s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.78s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.94s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.05s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.70s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.04s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.64s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.93s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.82s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.66s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.00s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.68s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.01s/it]
Formatting inputs...Skip in lazy mode
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:3')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:2')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:4')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:1')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:0')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:5')
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.012004852294921875 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.011365413665771484 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.011919975280761719 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10278034210205078 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.011117935180664062 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.1016080379486084 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/186 [00:09<30:09,  9.78s/it]  1%|          | 2/186 [00:19<29:03,  9.48s/it]  2%|▏         | 3/186 [00:28<29:19,  9.61s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1074 > 1048). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1069 > 1048). Running this sequence through the model will result in indexing errors
  2%|▏         | 4/186 [00:39<30:08,  9.94s/it]  3%|▎         | 5/186 [00:49<30:01,  9.95s/it]  3%|▎         | 6/186 [00:59<30:00, 10.00s/it]  4%|▍         | 7/186 [01:09<29:50, 10.00s/it]  4%|▍         | 8/186 [01:19<29:36,  9.98s/it]  5%|▍         | 9/186 [01:29<29:28,  9.99s/it]  5%|▌         | 10/186 [01:39<29:26, 10.04s/it]  6%|▌         | 11/186 [01:49<29:34, 10.14s/it]  6%|▋         | 12/186 [02:00<29:29, 10.17s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
  7%|▋         | 13/186 [02:10<29:11, 10.12s/it]  8%|▊         | 14/186 [02:20<29:07, 10.16s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1073 > 1048). Running this sequence through the model will result in indexing errors
  8%|▊         | 15/186 [02:30<29:07, 10.22s/it]  9%|▊         | 16/186 [02:40<28:55, 10.21s/it]  9%|▉         | 17/186 [02:51<28:57, 10.28s/it] 10%|▉         | 18/186 [03:01<28:43, 10.26s/it] 10%|█         | 19/186 [03:11<28:27, 10.22s/it] 11%|█         | 20/186 [03:22<28:30, 10.30s/it] 11%|█▏        | 21/186 [03:32<28:21, 10.31s/it] 12%|█▏        | 22/186 [03:42<28:08, 10.30s/it] 12%|█▏        | 23/186 [03:53<27:58, 10.30s/it] 13%|█▎        | 24/186 [04:04<28:24, 10.52s/it] 13%|█▎        | 25/186 [04:14<28:11, 10.51s/it] 14%|█▍        | 26/186 [04:25<28:06, 10.54s/it] 15%|█▍        | 27/186 [04:35<28:00, 10.57s/it] 15%|█▌        | 28/186 [04:46<27:51, 10.58s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1263 > 1048). Running this sequence through the model will result in indexing errors
 16%|█▌        | 29/186 [04:57<27:49, 10.63s/it] 16%|█▌        | 30/186 [05:07<27:34, 10.60s/it] 17%|█▋        | 31/186 [05:18<27:10, 10.52s/it] 17%|█▋        | 32/186 [05:28<27:07, 10.57s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1240 > 1048). Running this sequence through the model will result in indexing errors
 18%|█▊        | 33/186 [05:39<27:05, 10.62s/it] 18%|█▊        | 34/186 [05:50<26:52, 10.61s/it] 19%|█▉        | 35/186 [06:00<26:54, 10.69s/it] 19%|█▉        | 36/186 [06:11<26:45, 10.70s/it] 20%|█▉        | 37/186 [06:22<26:35, 10.71s/it] 20%|██        | 38/186 [06:32<26:17, 10.66s/it] 21%|██        | 39/186 [06:43<26:05, 10.65s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 22%|██▏       | 40/186 [06:54<26:08, 10.74s/it]WARNING: tokenization mismatch: 1 vs. 47. (ignored)
 22%|██▏       | 41/186 [07:05<25:52, 10.70s/it] 23%|██▎       | 42/186 [07:15<25:40, 10.70s/it] 23%|██▎       | 43/186 [07:26<25:28, 10.69s/it] 24%|██▎       | 44/186 [07:37<25:12, 10.65s/it] 24%|██▍       | 45/186 [07:47<25:04, 10.67s/it] 25%|██▍       | 46/186 [07:58<24:53, 10.67s/it] 25%|██▌       | 47/186 [08:09<24:59, 10.79s/it] 26%|██▌       | 48/186 [08:20<24:42, 10.75s/it] 26%|██▋       | 49/186 [08:30<24:21, 10.67s/it] 27%|██▋       | 50/186 [08:41<24:05, 10.63s/it] 27%|██▋       | 51/186 [08:51<23:48, 10.58s/it] 28%|██▊       | 52/186 [09:02<23:45, 10.64s/it] 28%|██▊       | 53/186 [09:13<23:47, 10.74s/it] 29%|██▉       | 54/186 [09:24<23:42, 10.77s/it] 30%|██▉       | 55/186 [09:34<23:24, 10.72s/it] 30%|███       | 56/186 [09:45<23:10, 10.70s/it] 31%|███       | 57/186 [09:56<22:55, 10.66s/it] 31%|███       | 58/186 [10:06<22:40, 10.63s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 32%|███▏      | 59/186 [10:17<22:41, 10.72s/it] 32%|███▏      | 60/186 [10:28<22:22, 10.65s/it] 33%|███▎      | 61/186 [10:38<22:21, 10.73s/it] 33%|███▎      | 62/186 [10:49<22:05, 10.69s/it] 34%|███▍      | 63/186 [11:00<21:52, 10.67s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1200 > 1048). Running this sequence through the model will result in indexing errors
 34%|███▍      | 64/186 [11:11<21:49, 10.73s/it] 35%|███▍      | 65/186 [11:21<21:35, 10.71s/it] 35%|███▌      | 66/186 [11:32<21:36, 10.80s/it] 36%|███▌      | 67/186 [11:43<21:14, 10.71s/it] 37%|███▋      | 68/186 [11:53<21:00, 10.68s/it] 37%|███▋      | 69/186 [12:04<21:04, 10.80s/it] 38%|███▊      | 70/186 [12:15<20:49, 10.77s/it] 38%|███▊      | 71/186 [12:26<20:26, 10.66s/it] 39%|███▊      | 72/186 [12:36<20:09, 10.61s/it] 39%|███▉      | 73/186 [12:47<19:56, 10.59s/it] 40%|███▉      | 74/186 [12:57<19:47, 10.60s/it] 40%|████      | 75/186 [13:08<19:37, 10.61s/it] 41%|████      | 76/186 [13:18<19:27, 10.61s/it] 41%|████▏     | 77/186 [13:29<19:17, 10.62s/it] 42%|████▏     | 78/186 [13:40<19:07, 10.63s/it] 42%|████▏     | 79/186 [13:51<19:15, 10.80s/it] 43%|████▎     | 80/186 [14:01<18:52, 10.68s/it] 44%|████▎     | 81/186 [14:12<18:46, 10.73s/it] 44%|████▍     | 82/186 [14:23<18:29, 10.67s/it] 45%|████▍     | 83/186 [14:33<18:15, 10.64s/it] 45%|████▌     | 84/186 [14:44<18:03, 10.63s/it] 46%|████▌     | 85/186 [14:55<17:55, 10.65s/it] 46%|████▌     | 86/186 [15:05<17:45, 10.66s/it] 47%|████▋     | 87/186 [15:16<17:34, 10.65s/it] 47%|████▋     | 88/186 [15:27<17:29, 10.71s/it] 48%|████▊     | 89/186 [15:37<17:16, 10.69s/it] 48%|████▊     | 90/186 [15:48<16:57, 10.60s/it] 49%|████▉     | 91/186 [15:58<16:48, 10.61s/it] 49%|████▉     | 92/186 [16:09<16:34, 10.58s/it] 50%|█████     | 93/186 [16:20<16:35, 10.70s/it] 51%|█████     | 94/186 [16:31<16:23, 10.69s/it] 51%|█████     | 95/186 [16:41<16:11, 10.68s/it] 52%|█████▏    | 96/186 [16:52<16:09, 10.77s/it] 52%|█████▏    | 97/186 [17:03<15:54, 10.72s/it] 53%|█████▎    | 98/186 [17:13<15:43, 10.72s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 53%|█████▎    | 99/186 [17:24<15:29, 10.68s/it] 54%|█████▍    | 100/186 [17:35<15:13, 10.62s/it] 54%|█████▍    | 101/186 [17:46<15:13, 10.74s/it] 55%|█████▍    | 102/186 [17:56<14:59, 10.71s/it] 55%|█████▌    | 103/186 [18:07<14:51, 10.74s/it] 56%|█████▌    | 104/186 [18:18<14:35, 10.68s/it] 56%|█████▋    | 105/186 [18:28<14:23, 10.66s/it] 57%|█████▋    | 106/186 [18:39<14:15, 10.70s/it] 58%|█████▊    | 107/186 [18:49<13:58, 10.62s/it] 58%|█████▊    | 108/186 [19:00<13:47, 10.61s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 59%|█████▊    | 109/186 [19:11<13:39, 10.65s/it] 59%|█████▉    | 110/186 [19:21<13:30, 10.66s/it] 60%|█████▉    | 111/186 [19:32<13:23, 10.71s/it] 60%|██████    | 112/186 [19:43<13:11, 10.70s/it] 61%|██████    | 113/186 [19:54<13:02, 10.71s/it] 61%|██████▏   | 114/186 [20:04<12:52, 10.73s/it] 62%|██████▏   | 115/186 [20:15<12:36, 10.66s/it] 62%|██████▏   | 116/186 [20:26<12:36, 10.80s/it] 63%|██████▎   | 117/186 [20:37<12:23, 10.77s/it] 63%|██████▎   | 118/186 [20:47<12:08, 10.71s/it] 64%|██████▍   | 119/186 [20:58<11:53, 10.65s/it] 65%|██████▍   | 120/186 [21:09<11:45, 10.69s/it] 65%|██████▌   | 121/186 [21:19<11:31, 10.64s/it] 66%|██████▌   | 122/186 [21:30<11:22, 10.67s/it] 66%|██████▌   | 123/186 [21:41<11:21, 10.82s/it] 67%|██████▋   | 124/186 [21:52<11:09, 10.80s/it] 67%|██████▋   | 125/186 [22:03<10:58, 10.80s/it] 68%|██████▊   | 126/186 [22:13<10:45, 10.76s/it] 68%|██████▊   | 127/186 [22:24<10:32, 10.71s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
 69%|██████▉   | 128/186 [22:35<10:23, 10.74s/it] 69%|██████▉   | 129/186 [22:46<10:16, 10.81s/it] 70%|██████▉   | 130/186 [22:57<10:07, 10.84s/it] 70%|███████   | 131/186 [23:07<09:55, 10.82s/it] 71%|███████   | 132/186 [23:18<09:47, 10.87s/it] 72%|███████▏  | 133/186 [23:29<09:33, 10.83s/it] 72%|███████▏  | 134/186 [23:40<09:22, 10.82s/it] 73%|███████▎  | 135/186 [23:51<09:10, 10.79s/it] 73%|███████▎  | 136/186 [24:01<09:00, 10.80s/it] 74%|███████▎  | 137/186 [24:12<08:50, 10.83s/it] 74%|███████▍  | 138/186 [24:23<08:38, 10.80s/it] 75%|███████▍  | 139/186 [24:34<08:34, 10.96s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 75%|███████▌  | 140/186 [24:45<08:19, 10.85s/it] 76%|███████▌  | 141/186 [24:56<08:08, 10.87s/it] 76%|███████▋  | 142/186 [25:06<07:53, 10.77s/it] 77%|███████▋  | 143/186 [25:17<07:44, 10.80s/it] 77%|███████▋  | 144/186 [25:28<07:31, 10.75s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 78%|███████▊  | 145/186 [25:39<07:24, 10.85s/it] 78%|███████▊  | 146/186 [25:50<07:16, 10.91s/it] 79%|███████▉  | 147/186 [26:01<07:02, 10.82s/it] 80%|███████▉  | 148/186 [26:11<06:50, 10.80s/it] 80%|████████  | 149/186 [26:22<06:38, 10.78s/it] 81%|████████  | 150/186 [26:33<06:30, 10.83s/it] 81%|████████  | 151/186 [26:44<06:19, 10.83s/it] 82%|████████▏ | 152/186 [26:55<06:06, 10.79s/it] 82%|████████▏ | 153/186 [27:06<06:00, 10.92s/it] 83%|████████▎ | 154/186 [27:17<05:51, 10.99s/it] 83%|████████▎ | 155/186 [27:28<05:39, 10.97s/it] 84%|████████▍ | 156/186 [27:39<05:29, 10.99s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 84%|████████▍ | 157/186 [27:50<05:19, 11.01s/it] 85%|████████▍ | 158/186 [28:01<05:07, 11.00s/it] 85%|████████▌ | 159/186 [28:12<04:55, 10.93s/it] 86%|████████▌ | 160/186 [28:23<04:43, 10.91s/it] 87%|████████▋ | 161/186 [28:33<04:32, 10.89s/it] 87%|████████▋ | 162/186 [28:45<04:24, 11.04s/it] 88%|████████▊ | 163/186 [28:56<04:14, 11.08s/it] 88%|████████▊ | 164/186 [29:07<04:05, 11.14s/it] 89%|████████▊ | 165/186 [29:18<03:51, 11.04s/it] 89%|████████▉ | 166/186 [29:29<03:39, 10.95s/it] 90%|████████▉ | 167/186 [29:40<03:27, 10.94s/it] 90%|█████████ | 168/186 [29:51<03:16, 10.89s/it] 91%|█████████ | 169/186 [30:01<03:04, 10.85s/it] 91%|█████████▏| 170/186 [30:12<02:54, 10.92s/it] 92%|█████████▏| 171/186 [30:24<02:44, 10.98s/it] 92%|█████████▏| 172/186 [30:34<02:33, 10.94s/it] 93%|█████████▎| 173/186 [30:45<02:21, 10.91s/it] 94%|█████████▎| 174/186 [30:56<02:11, 10.95s/it] 94%|█████████▍| 175/186 [31:07<02:00, 10.96s/it] 95%|█████████▍| 176/186 [31:18<01:49, 10.95s/it] 95%|█████████▌| 177/186 [31:29<01:38, 10.91s/it] 96%|█████████▌| 178/186 [31:40<01:27, 10.92s/it] 96%|█████████▌| 179/186 [31:51<01:16, 10.88s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 97%|█████████▋| 180/186 [32:02<01:05, 10.91s/it] 97%|█████████▋| 181/186 [32:13<00:54, 10.98s/it] 98%|█████████▊| 182/186 [32:24<00:43, 10.93s/it] 98%|█████████▊| 183/186 [32:35<00:32, 10.91s/it] 99%|█████████▉| 184/186 [32:45<00:21, 10.93s/it] 99%|█████████▉| 185/186 [32:56<00:10, 10.94s/it]100%|██████████| 186/186 [33:08<00:00, 11.08s/it]                                                 {'train_runtime': 1988.3595, 'train_samples_per_second': 6.757, 'train_steps_per_second': 0.094, 'train_loss': 0.1325713639618248, 'epoch': 1.0}
100%|██████████| 186/186 [33:08<00:00, 11.08s/it]100%|██████████| 186/186 [33:08<00:00, 10.69s/it]
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-2
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33msliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-2[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251017_094057-4l1dopcs/logs[0m
[2025-10-17 10:15:48,287] [INFO] [launch.py:347:main] Process 2209967 exits successfully.
[2025-10-17 10:15:48,287] [INFO] [launch.py:347:main] Process 2209968 exits successfully.
[2025-10-17 10:15:48,287] [INFO] [launch.py:347:main] Process 2209966 exits successfully.
[2025-10-17 10:15:48,288] [INFO] [launch.py:347:main] Process 2209965 exits successfully.
[2025-10-17 10:15:49,289] [INFO] [launch.py:347:main] Process 2209964 exits successfully.
[2025-10-17 10:15:53,293] [INFO] [launch.py:347:main] Process 2209963 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 10:15:56,480] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 10:15:57,034] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-17 10:15:57,035] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12423 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-3 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-3 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 0.5 --use_anchor False --use_sample_weight True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 10:15:58,505] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 10:15:59,092] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-17 10:15:59,092] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-17 10:15:59,092] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-17 10:15:59,092] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-17 10:15:59,092] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 10:16:02,867] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 10:16:02,980] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 10:16:02,981] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 10:16:03,178] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 10:16:03,178] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-10-17 10:16:03,196] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 10:16:03,290] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 10:16:03,322] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 10:16:03,510] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
[2025-10-17 10:16:04,327] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
[2025-10-17 10:16:04,874] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 10:16:04,987] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 10:16:05,444] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251017_101605-pg9zcjgx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-3
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/pg9zcjgx
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.03s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.58s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.68s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.14s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.42s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.54s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.84s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.63s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.93s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.98s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.86s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.53s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  4.00s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.69s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.02s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.51s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.82s/it]
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.34s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.79s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.24s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.87s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.24s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.67s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.99s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.16s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.83s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.65s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.99s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.64s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.50s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.82s/it]
Formatting inputs...Skip in lazy mode
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.68s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.03s/it]
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:5')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:4')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:1')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:2')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:0')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:3')
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.010689735412597656 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.011046171188354492 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.010366439819335938 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...

Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.012994050979614258 seconds
Loading extension module fused_adam...
Loading extension module fused_adam...
Time to load fused_adam op: 0.10193777084350586 seconds
Time to load fused_adam op: 0.10151886940002441 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
  0%|          | 0/186 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1074 > 1048). Running this sequence through the model will result in indexing errors
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/186 [00:10<31:06, 10.09s/it]  1%|          | 2/186 [00:19<29:24,  9.59s/it]  2%|▏         | 3/186 [00:28<29:09,  9.56s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1075 > 1048). Running this sequence through the model will result in indexing errors
  2%|▏         | 4/186 [00:38<29:36,  9.76s/it]  3%|▎         | 5/186 [00:48<29:39,  9.83s/it]  3%|▎         | 6/186 [00:58<29:43,  9.91s/it]  4%|▍         | 7/186 [01:09<29:49, 10.00s/it]  4%|▍         | 8/186 [01:19<29:56, 10.09s/it]  5%|▍         | 9/186 [01:29<29:51, 10.12s/it]  5%|▌         | 10/186 [01:39<29:48, 10.16s/it]  6%|▌         | 11/186 [01:49<29:34, 10.14s/it]  6%|▋         | 12/186 [02:00<29:34, 10.20s/it]  7%|▋         | 13/186 [02:10<29:34, 10.26s/it]  8%|▊         | 14/186 [02:20<29:26, 10.27s/it]  8%|▊         | 15/186 [02:31<29:15, 10.27s/it]  9%|▊         | 16/186 [02:41<29:10, 10.30s/it]  9%|▉         | 17/186 [02:51<29:04, 10.32s/it] 10%|▉         | 18/186 [03:02<28:55, 10.33s/it] 10%|█         | 19/186 [03:12<28:45, 10.33s/it] 11%|█         | 20/186 [03:22<28:30, 10.30s/it] 11%|█▏        | 21/186 [03:33<28:24, 10.33s/it] 12%|█▏        | 22/186 [03:43<28:14, 10.33s/it] 12%|█▏        | 23/186 [03:54<28:08, 10.36s/it] 13%|█▎        | 24/186 [04:04<28:15, 10.47s/it] 13%|█▎        | 25/186 [04:15<28:04, 10.46s/it] 14%|█▍        | 26/186 [04:25<27:51, 10.45s/it]WARNING: tokenization mismatch: 1 vs. 48. (ignored)
 15%|█▍        | 27/186 [04:36<27:41, 10.45s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
 15%|█▌        | 28/186 [04:46<27:33, 10.46s/it] 16%|█▌        | 29/186 [04:57<27:27, 10.49s/it] 16%|█▌        | 30/186 [05:07<27:12, 10.46s/it] 17%|█▋        | 31/186 [05:18<27:05, 10.48s/it] 17%|█▋        | 32/186 [05:28<26:51, 10.47s/it] 18%|█▊        | 33/186 [05:39<26:45, 10.49s/it] 18%|█▊        | 34/186 [05:49<26:37, 10.51s/it] 19%|█▉        | 35/186 [06:00<26:39, 10.59s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1072 > 1048). Running this sequence through the model will result in indexing errors
 19%|█▉        | 36/186 [06:11<26:32, 10.62s/it] 20%|█▉        | 37/186 [06:21<26:18, 10.59s/it] 20%|██        | 38/186 [06:32<26:06, 10.59s/it] 21%|██        | 39/186 [06:42<25:59, 10.61s/it] 22%|██▏       | 40/186 [06:53<25:47, 10.60s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1263 > 1048). Running this sequence through the model will result in indexing errors
 22%|██▏       | 41/186 [07:04<25:42, 10.64s/it] 23%|██▎       | 42/186 [07:14<25:36, 10.67s/it] 23%|██▎       | 43/186 [07:25<25:25, 10.67s/it] 24%|██▎       | 44/186 [07:36<25:08, 10.62s/it] 24%|██▍       | 45/186 [07:46<24:54, 10.60s/it] 25%|██▍       | 46/186 [07:57<24:50, 10.65s/it] 25%|██▌       | 47/186 [08:08<24:50, 10.72s/it] 26%|██▌       | 48/186 [08:18<24:36, 10.70s/it] 26%|██▋       | 49/186 [08:29<24:18, 10.65s/it] 27%|██▋       | 50/186 [08:39<24:01, 10.60s/it] 27%|██▋       | 51/186 [08:50<23:40, 10.52s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 28%|██▊       | 52/186 [09:00<23:35, 10.56s/it] 28%|██▊       | 53/186 [09:11<23:21, 10.53s/it] 29%|██▉       | 54/186 [09:21<23:09, 10.53s/it] 30%|██▉       | 55/186 [09:32<22:56, 10.51s/it] 30%|███       | 56/186 [09:43<23:00, 10.62s/it] 31%|███       | 57/186 [09:53<22:48, 10.61s/it] 31%|███       | 58/186 [10:04<22:52, 10.72s/it] 32%|███▏      | 59/186 [10:15<22:45, 10.75s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 32%|███▏      | 60/186 [10:26<22:31, 10.72s/it] 33%|███▎      | 61/186 [10:36<22:10, 10.65s/it] 33%|███▎      | 62/186 [10:47<22:04, 10.68s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 34%|███▍      | 63/186 [10:58<21:48, 10.64s/it] 34%|███▍      | 64/186 [11:08<21:34, 10.61s/it] 35%|███▍      | 65/186 [11:19<21:22, 10.60s/it] 35%|███▌      | 66/186 [11:29<21:13, 10.61s/it] 36%|███▌      | 67/186 [11:40<20:57, 10.57s/it] 37%|███▋      | 68/186 [11:50<20:50, 10.60s/it] 37%|███▋      | 69/186 [12:01<20:55, 10.73s/it] 38%|███▊      | 70/186 [12:12<20:47, 10.75s/it] 38%|███▊      | 71/186 [12:23<20:34, 10.73s/it] 39%|███▊      | 72/186 [12:34<20:17, 10.68s/it] 39%|███▉      | 73/186 [12:44<20:13, 10.74s/it] 40%|███▉      | 74/186 [12:56<20:18, 10.88s/it] 40%|████      | 75/186 [13:07<20:11, 10.92s/it] 41%|████      | 76/186 [13:18<20:06, 10.97s/it] 41%|████▏     | 77/186 [13:28<19:47, 10.89s/it] 42%|████▏     | 78/186 [13:39<19:35, 10.88s/it] 42%|████▏     | 79/186 [13:50<19:24, 10.89s/it] 43%|████▎     | 80/186 [14:01<19:08, 10.83s/it] 44%|████▎     | 81/186 [14:12<18:56, 10.82s/it] 44%|████▍     | 82/186 [14:22<18:43, 10.81s/it] 45%|████▍     | 83/186 [14:33<18:35, 10.83s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1102 > 1048). Running this sequence through the model will result in indexing errors
 45%|████▌     | 84/186 [14:44<18:32, 10.91s/it] 46%|████▌     | 85/186 [14:55<18:20, 10.90s/it] 46%|████▌     | 86/186 [15:06<18:06, 10.87s/it] 47%|████▋     | 87/186 [15:18<18:13, 11.05s/it] 47%|████▋     | 88/186 [15:29<18:02, 11.05s/it] 48%|████▊     | 89/186 [15:40<18:03, 11.17s/it] 48%|████▊     | 90/186 [15:52<17:59, 11.25s/it]WARNING: tokenization mismatch: 1 vs. 47. (ignored)
 49%|████▉     | 91/186 [16:03<17:42, 11.18s/it] 49%|████▉     | 92/186 [16:14<17:31, 11.18s/it] 50%|█████     | 93/186 [16:25<17:30, 11.29s/it] 51%|█████     | 94/186 [16:36<17:01, 11.10s/it] 51%|█████     | 95/186 [16:47<16:38, 10.97s/it] 52%|█████▏    | 96/186 [16:57<16:18, 10.87s/it] 52%|█████▏    | 97/186 [17:12<17:51, 12.04s/it] 53%|█████▎    | 98/186 [17:29<19:41, 13.42s/it] 53%|█████▎    | 99/186 [17:45<20:40, 14.26s/it] 54%|█████▍    | 100/186 [18:01<21:15, 14.83s/it] 54%|█████▍    | 101/186 [18:17<21:40, 15.30s/it] 55%|█████▍    | 102/186 [18:34<21:50, 15.60s/it] 55%|█████▌    | 103/186 [18:50<22:03, 15.95s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 56%|█████▌    | 104/186 [19:07<21:53, 16.02s/it] 56%|█████▋    | 105/186 [19:23<21:46, 16.13s/it] 57%|█████▋    | 106/186 [19:39<21:33, 16.17s/it] 58%|█████▊    | 107/186 [19:56<21:24, 16.25s/it] 58%|█████▊    | 108/186 [20:12<21:09, 16.28s/it] 59%|█████▊    | 109/186 [20:28<20:49, 16.23s/it] 59%|█████▉    | 110/186 [20:44<20:34, 16.24s/it] 60%|█████▉    | 111/186 [21:01<20:16, 16.22s/it] 60%|██████    | 112/186 [21:17<19:56, 16.16s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 61%|██████    | 113/186 [21:33<19:38, 16.15s/it] 61%|██████▏   | 114/186 [21:49<19:30, 16.26s/it] 62%|██████▏   | 115/186 [22:06<19:14, 16.26s/it] 62%|██████▏   | 116/186 [22:23<19:13, 16.48s/it] 63%|██████▎   | 117/186 [22:39<18:48, 16.36s/it] 63%|██████▎   | 118/186 [22:55<18:34, 16.39s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1263 > 1048). Running this sequence through the model will result in indexing errors
 64%|██████▍   | 119/186 [23:12<18:27, 16.52s/it] 65%|██████▍   | 120/186 [23:29<18:17, 16.64s/it] 65%|██████▌   | 121/186 [23:45<17:56, 16.55s/it] 66%|██████▌   | 122/186 [24:02<17:47, 16.67s/it] 66%|██████▌   | 123/186 [24:19<17:29, 16.66s/it] 67%|██████▋   | 124/186 [24:36<17:18, 16.76s/it] 67%|██████▋   | 125/186 [24:52<16:48, 16.54s/it] 68%|██████▊   | 126/186 [25:08<16:18, 16.30s/it] 68%|██████▊   | 127/186 [25:23<15:53, 16.16s/it] 69%|██████▉   | 128/186 [25:39<15:21, 15.88s/it] 69%|██████▉   | 129/186 [25:55<15:05, 15.89s/it] 70%|██████▉   | 130/186 [26:11<14:59, 16.06s/it] 70%|███████   | 131/186 [26:28<14:55, 16.28s/it] 71%|███████   | 132/186 [26:41<13:43, 15.25s/it] 72%|███████▏  | 133/186 [26:52<12:20, 13.97s/it] 72%|███████▏  | 134/186 [27:02<11:16, 13.01s/it] 73%|███████▎  | 135/186 [27:13<10:26, 12.29s/it] 73%|███████▎  | 136/186 [27:24<09:48, 11.77s/it] 74%|███████▎  | 137/186 [27:35<09:39, 11.82s/it] 74%|███████▍  | 138/186 [27:46<09:04, 11.33s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 75%|███████▍  | 139/186 [27:56<08:41, 11.10s/it] 75%|███████▌  | 140/186 [28:07<08:29, 11.07s/it] 76%|███████▌  | 141/186 [28:18<08:18, 11.07s/it] 76%|███████▋  | 142/186 [28:29<08:02, 10.96s/it] 77%|███████▋  | 143/186 [28:40<07:54, 11.04s/it] 77%|███████▋  | 144/186 [28:52<07:47, 11.12s/it] 78%|███████▊  | 145/186 [29:02<07:25, 10.86s/it] 78%|███████▊  | 146/186 [29:12<07:09, 10.75s/it] 79%|███████▉  | 147/186 [29:24<07:12, 11.08s/it] 80%|███████▉  | 148/186 [29:40<07:53, 12.46s/it] 80%|████████  | 149/186 [29:56<08:19, 13.50s/it] 81%|████████  | 150/186 [30:12<08:35, 14.31s/it] 81%|████████  | 151/186 [30:28<08:38, 14.83s/it] 82%|████████▏ | 152/186 [30:44<08:37, 15.23s/it] 82%|████████▏ | 153/186 [31:00<08:31, 15.51s/it] 83%|████████▎ | 154/186 [31:17<08:25, 15.79s/it] 83%|████████▎ | 155/186 [31:33<08:18, 16.07s/it] 84%|████████▍ | 156/186 [31:51<08:13, 16.47s/it] 84%|████████▍ | 157/186 [32:07<07:57, 16.46s/it] 85%|████████▍ | 158/186 [32:24<07:39, 16.43s/it] 85%|████████▌ | 159/186 [32:40<07:20, 16.30s/it] 86%|████████▌ | 160/186 [32:57<07:11, 16.60s/it] 87%|████████▋ | 161/186 [33:13<06:51, 16.46s/it] 87%|████████▋ | 162/186 [33:30<06:34, 16.46s/it] 88%|████████▊ | 163/186 [33:46<06:18, 16.47s/it] 88%|████████▊ | 164/186 [34:02<06:01, 16.42s/it] 89%|████████▊ | 165/186 [34:19<05:45, 16.47s/it] 89%|████████▉ | 166/186 [34:35<05:29, 16.45s/it] 90%|████████▉ | 167/186 [34:52<05:11, 16.39s/it] 90%|█████████ | 168/186 [35:08<04:54, 16.35s/it] 91%|█████████ | 169/186 [35:24<04:36, 16.29s/it] 91%|█████████▏| 170/186 [35:40<04:20, 16.28s/it] 92%|█████████▏| 171/186 [35:57<04:04, 16.32s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 92%|█████████▏| 172/186 [36:13<03:48, 16.33s/it] 93%|█████████▎| 173/186 [36:29<03:30, 16.16s/it] 94%|█████████▎| 174/186 [36:45<03:14, 16.18s/it] 94%|█████████▍| 175/186 [37:01<02:58, 16.21s/it] 95%|█████████▍| 176/186 [37:18<02:42, 16.25s/it] 95%|█████████▌| 177/186 [37:33<02:24, 16.05s/it] 96%|█████████▌| 178/186 [37:49<02:08, 16.00s/it] 96%|█████████▌| 179/186 [38:05<01:50, 15.83s/it] 97%|█████████▋| 180/186 [38:20<01:34, 15.69s/it] 97%|█████████▋| 181/186 [38:36<01:18, 15.78s/it] 98%|█████████▊| 182/186 [38:52<01:03, 15.99s/it] 98%|█████████▊| 183/186 [39:03<00:43, 14.39s/it] 99%|█████████▉| 184/186 [39:14<00:26, 13.26s/it] 99%|█████████▉| 185/186 [39:24<00:12, 12.53s/it]100%|██████████| 186/186 [39:35<00:00, 11.93s/it]                                                 {'train_runtime': 2375.5254, 'train_samples_per_second': 5.656, 'train_steps_per_second': 0.078, 'train_loss': 0.16209579795919438, 'epoch': 1.0}
100%|██████████| 186/186 [39:35<00:00, 11.93s/it]100%|██████████| 186/186 [39:35<00:00, 12.77s/it]
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-3
[2025-10-17 10:57:26,959] [INFO] [launch.py:347:main] Process 2211709 exits successfully.
[2025-10-17 10:57:27,961] [INFO] [launch.py:347:main] Process 2211710 exits successfully.
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33msliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-3[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251017_101605-pg9zcjgx/logs[0m
[2025-10-17 10:57:29,963] [INFO] [launch.py:347:main] Process 2211706 exits successfully.
[2025-10-17 10:57:29,964] [INFO] [launch.py:347:main] Process 2211707 exits successfully.
[2025-10-17 10:57:29,964] [INFO] [launch.py:347:main] Process 2211708 exits successfully.
[2025-10-17 10:57:34,970] [INFO] [launch.py:347:main] Process 2211705 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 10:57:39,191] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 10:57:40,016] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-17 10:57:40,016] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12423 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-3 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-3 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 1 --use_anchor False --use_sample_weight True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 10:57:41,733] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 10:57:42,481] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-17 10:57:42,482] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-17 10:57:42,482] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-17 10:57:42,482] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-17 10:57:42,482] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 10:57:46,190] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 10:57:46,235] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 10:57:46,517] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 10:57:46,517] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 10:57:46,544] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 10:57:46,553] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 10:57:46,642] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-10-17 10:57:46,761] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 10:57:46,761] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 10:57:46,872] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 10:57:46,992] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-10-17 10:57:47,104] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 10:57:47,104] [INFO] [comm.py:637:init_distributed] cdb=None
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251017_105747-6q1tq3uk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-3
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/6q1tq3uk
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.09s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.99s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.30s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.38s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.09s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.12s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.13s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.16s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.82s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.16s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.89s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.22s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.27s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.99s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.31s/it]
to bfloat16...
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.88s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.76s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.68s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.86s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.87s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.10s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.49s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.06s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.33s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.91s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.41s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.03s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.41s/it]
Formatting inputs...Skip in lazy mode
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:1')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:5')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:4')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:3')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:2')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:0')
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.010306358337402344 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.012558460235595703 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.1013023853302002 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.011973857879638672 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10125207901000977 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.1016073226928711 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/186 [00:17<53:36, 17.39s/it]  1%|          | 2/186 [00:32<49:55, 16.28s/it]  2%|▏         | 3/186 [00:48<49:03, 16.09s/it]  2%|▏         | 4/186 [01:04<48:35, 16.02s/it]  3%|▎         | 5/186 [01:21<48:48, 16.18s/it]  3%|▎         | 6/186 [01:37<48:15, 16.09s/it]  4%|▍         | 7/186 [01:53<48:04, 16.12s/it]  4%|▍         | 8/186 [02:09<47:58, 16.17s/it]  5%|▍         | 9/186 [02:25<47:44, 16.18s/it]  5%|▌         | 10/186 [02:42<47:35, 16.23s/it]  6%|▌         | 11/186 [02:58<47:29, 16.28s/it]  6%|▋         | 12/186 [03:14<47:16, 16.30s/it]  7%|▋         | 13/186 [03:31<47:01, 16.31s/it]  8%|▊         | 14/186 [03:47<46:43, 16.30s/it]  8%|▊         | 15/186 [04:03<46:41, 16.38s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1125 > 1048). Running this sequence through the model will result in indexing errors
  9%|▊         | 16/186 [04:20<46:46, 16.51s/it]  9%|▉         | 17/186 [04:37<46:17, 16.44s/it] 10%|▉         | 18/186 [04:53<45:52, 16.38s/it] 10%|█         | 19/186 [05:09<45:42, 16.42s/it] 11%|█         | 20/186 [05:25<45:02, 16.28s/it] 11%|█▏        | 21/186 [05:41<44:41, 16.25s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1075 > 1048). Running this sequence through the model will result in indexing errors
 12%|█▏        | 22/186 [05:58<44:44, 16.37s/it] 12%|█▏        | 23/186 [06:15<44:56, 16.54s/it] 13%|█▎        | 24/186 [06:32<45:12, 16.74s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1080 > 1048). Running this sequence through the model will result in indexing errors
 13%|█▎        | 25/186 [06:50<45:43, 17.04s/it] 14%|█▍        | 26/186 [07:06<44:56, 16.85s/it] 15%|█▍        | 27/186 [07:23<44:40, 16.86s/it] 15%|█▌        | 28/186 [07:39<43:42, 16.60s/it] 16%|█▌        | 29/186 [07:56<43:24, 16.59s/it] 16%|█▌        | 30/186 [08:11<42:18, 16.27s/it] 17%|█▋        | 31/186 [08:27<41:21, 16.01s/it] 17%|█▋        | 32/186 [08:42<40:44, 15.87s/it] 18%|█▊        | 33/186 [08:58<40:19, 15.81s/it] 18%|█▊        | 34/186 [09:15<40:42, 16.07s/it] 19%|█▉        | 35/186 [09:27<37:16, 14.81s/it] 19%|█▉        | 36/186 [09:37<33:57, 13.58s/it] 20%|█▉        | 37/186 [09:48<31:35, 12.72s/it] 20%|██        | 38/186 [09:59<29:53, 12.12s/it] 21%|██        | 39/186 [10:09<28:36, 11.68s/it] 22%|██▏       | 40/186 [10:22<29:27, 12.11s/it] 22%|██▏       | 41/186 [10:33<27:52, 11.53s/it] 23%|██▎       | 42/186 [10:44<27:20, 11.40s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 23%|██▎       | 43/186 [10:55<27:00, 11.33s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
Token indices sequence length is longer than the specified maximum sequence length for this model (1263 > 1048). Running this sequence through the model will result in indexing errors
 24%|██▎       | 44/186 [11:06<26:43, 11.30s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1160 > 1048). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1153 > 1048). Running this sequence through the model will result in indexing errors
 24%|██▍       | 45/186 [11:17<26:05, 11.10s/it] 25%|██▍       | 46/186 [11:28<26:11, 11.23s/it] 25%|██▌       | 47/186 [11:40<26:02, 11.24s/it] 26%|██▌       | 48/186 [11:50<25:27, 11.07s/it] 26%|██▋       | 49/186 [12:01<25:02, 10.97s/it] 27%|██▋       | 50/186 [12:13<25:51, 11.41s/it] 27%|██▋       | 51/186 [12:30<28:55, 12.85s/it] 28%|██▊       | 52/186 [12:46<31:24, 14.07s/it] 28%|██▊       | 53/186 [13:03<32:39, 14.73s/it] 29%|██▉       | 54/186 [13:19<33:26, 15.20s/it] 30%|██▉       | 55/186 [13:36<34:03, 15.60s/it] 30%|███       | 56/186 [13:52<34:16, 15.82s/it] 31%|███       | 57/186 [14:09<34:32, 16.06s/it] 31%|███       | 58/186 [14:26<34:59, 16.40s/it] 32%|███▏      | 59/186 [14:42<34:54, 16.49s/it] 32%|███▏      | 60/186 [14:59<34:47, 16.57s/it] 33%|███▎      | 61/186 [15:15<34:15, 16.44s/it] 33%|███▎      | 62/186 [15:32<33:52, 16.39s/it] 34%|███▍      | 63/186 [15:49<33:56, 16.56s/it] 34%|███▍      | 64/186 [16:05<33:48, 16.63s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 35%|███▍      | 65/186 [16:22<33:24, 16.57s/it] 35%|███▌      | 66/186 [16:38<33:08, 16.57s/it] 36%|███▌      | 67/186 [16:55<32:47, 16.53s/it] 37%|███▋      | 68/186 [17:11<32:16, 16.41s/it] 37%|███▋      | 69/186 [17:28<32:11, 16.51s/it] 38%|███▊      | 70/186 [17:44<31:55, 16.51s/it] 38%|███▊      | 71/186 [18:01<31:34, 16.47s/it] 39%|███▊      | 72/186 [18:17<31:11, 16.42s/it] 39%|███▉      | 73/186 [18:33<30:46, 16.34s/it] 40%|███▉      | 74/186 [18:49<30:23, 16.28s/it] 40%|████      | 75/186 [19:06<30:22, 16.42s/it] 41%|████      | 76/186 [19:22<30:03, 16.39s/it] 41%|████▏     | 77/186 [19:38<29:38, 16.31s/it] 42%|████▏     | 78/186 [19:55<29:19, 16.29s/it] 42%|████▏     | 79/186 [20:10<28:42, 16.10s/it] 43%|████▎     | 80/186 [20:26<28:18, 16.02s/it] 44%|████▎     | 81/186 [20:42<27:59, 16.00s/it] 44%|████▍     | 82/186 [20:58<27:39, 15.96s/it] 45%|████▍     | 83/186 [21:15<27:49, 16.21s/it] 45%|████▌     | 84/186 [21:31<27:31, 16.19s/it] 46%|████▌     | 85/186 [21:44<25:58, 15.43s/it] 46%|████▌     | 86/186 [21:55<23:22, 14.03s/it] 47%|████▋     | 87/186 [22:06<21:29, 13.03s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 47%|████▋     | 88/186 [22:17<20:05, 12.30s/it] 48%|████▊     | 89/186 [22:27<19:09, 11.85s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 48%|████▊     | 90/186 [22:39<18:49, 11.76s/it] 49%|████▉     | 91/186 [22:50<18:22, 11.61s/it] 49%|████▉     | 92/186 [23:01<17:42, 11.31s/it] 50%|█████     | 93/186 [23:12<17:24, 11.23s/it] 51%|█████     | 94/186 [23:23<16:59, 11.08s/it] 51%|█████     | 95/186 [23:33<16:38, 10.97s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 52%|█████▏    | 96/186 [23:44<16:28, 10.98s/it] 52%|█████▏    | 97/186 [23:55<16:12, 10.93s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 53%|█████▎    | 98/186 [24:06<15:50, 10.80s/it] 53%|█████▎    | 99/186 [24:16<15:25, 10.64s/it] 54%|█████▍    | 100/186 [24:26<14:57, 10.44s/it] 54%|█████▍    | 101/186 [24:36<14:43, 10.40s/it] 55%|█████▍    | 102/186 [24:46<14:25, 10.31s/it] 55%|█████▌    | 103/186 [24:57<14:20, 10.36s/it] 56%|█████▌    | 104/186 [25:07<14:07, 10.34s/it] 56%|█████▋    | 105/186 [25:17<13:51, 10.27s/it] 57%|█████▋    | 106/186 [25:27<13:34, 10.18s/it] 58%|█████▊    | 107/186 [25:37<13:23, 10.17s/it] 58%|█████▊    | 108/186 [25:47<13:11, 10.15s/it] 59%|█████▊    | 109/186 [25:58<13:04, 10.18s/it] 59%|█████▉    | 110/186 [26:08<13:02, 10.30s/it] 60%|█████▉    | 111/186 [26:18<12:52, 10.30s/it] 60%|██████    | 112/186 [26:29<12:42, 10.30s/it] 61%|██████    | 113/186 [26:39<12:33, 10.32s/it] 61%|██████▏   | 114/186 [26:50<12:27, 10.38s/it] 62%|██████▏   | 115/186 [27:00<12:15, 10.37s/it] 62%|██████▏   | 116/186 [27:10<12:05, 10.37s/it] 63%|██████▎   | 117/186 [27:21<12:00, 10.44s/it] 63%|██████▎   | 118/186 [27:32<11:54, 10.51s/it] 64%|██████▍   | 119/186 [27:42<11:43, 10.50s/it] 65%|██████▍   | 120/186 [27:53<11:32, 10.50s/it] 65%|██████▌   | 121/186 [28:03<11:20, 10.48s/it]WARNING: tokenization mismatch: 1 vs. 48. (ignored)
 66%|██████▌   | 122/186 [28:14<11:12, 10.51s/it] 66%|██████▌   | 123/186 [28:24<10:59, 10.47s/it] 67%|██████▋   | 124/186 [28:34<10:49, 10.48s/it] 67%|██████▋   | 125/186 [28:45<10:38, 10.46s/it] 68%|██████▊   | 126/186 [28:55<10:29, 10.50s/it] 68%|██████▊   | 127/186 [29:06<10:14, 10.41s/it] 69%|██████▉   | 128/186 [29:16<10:03, 10.41s/it] 69%|██████▉   | 129/186 [29:27<09:53, 10.42s/it] 70%|██████▉   | 130/186 [29:37<09:41, 10.38s/it] 70%|███████   | 131/186 [29:47<09:30, 10.37s/it] 71%|███████   | 132/186 [29:58<09:22, 10.42s/it] 72%|███████▏  | 133/186 [30:08<09:13, 10.45s/it] 72%|███████▏  | 134/186 [30:19<09:03, 10.45s/it] 73%|███████▎  | 135/186 [30:29<08:57, 10.54s/it] 73%|███████▎  | 136/186 [30:40<08:45, 10.52s/it] 74%|███████▎  | 137/186 [30:51<08:38, 10.58s/it] 74%|███████▍  | 138/186 [31:01<08:26, 10.55s/it] 75%|███████▍  | 139/186 [31:12<08:21, 10.66s/it] 75%|███████▌  | 140/186 [31:23<08:09, 10.65s/it] 76%|███████▌  | 141/186 [31:33<07:58, 10.63s/it] 76%|███████▋  | 142/186 [31:44<07:49, 10.68s/it] 77%|███████▋  | 143/186 [31:55<07:41, 10.73s/it] 77%|███████▋  | 144/186 [32:06<07:34, 10.83s/it] 78%|███████▊  | 145/186 [32:17<07:23, 10.82s/it] 78%|███████▊  | 146/186 [32:27<07:11, 10.78s/it] 79%|███████▉  | 147/186 [32:38<06:56, 10.68s/it] 80%|███████▉  | 148/186 [32:49<06:49, 10.76s/it] 80%|████████  | 149/186 [32:59<06:34, 10.65s/it] 81%|████████  | 150/186 [33:10<06:23, 10.65s/it] 81%|████████  | 151/186 [33:21<06:15, 10.73s/it] 82%|████████▏ | 152/186 [33:32<06:08, 10.84s/it] 82%|████████▏ | 153/186 [33:43<05:58, 10.85s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 83%|████████▎ | 154/186 [33:54<05:54, 11.09s/it] 83%|████████▎ | 155/186 [34:05<05:39, 10.94s/it] 84%|████████▍ | 156/186 [34:16<05:26, 10.89s/it] 84%|████████▍ | 157/186 [34:27<05:17, 10.95s/it] 85%|████████▍ | 158/186 [34:41<05:31, 11.85s/it] 85%|████████▌ | 159/186 [34:57<05:54, 13.13s/it] 86%|████████▌ | 160/186 [35:13<06:07, 14.15s/it] 87%|████████▋ | 161/186 [35:26<05:40, 13.63s/it] 87%|████████▋ | 162/186 [35:36<05:03, 12.65s/it] 88%|████████▊ | 163/186 [35:47<04:35, 11.96s/it] 88%|████████▊ | 164/186 [35:57<04:12, 11.47s/it] 89%|████████▊ | 165/186 [36:07<03:53, 11.14s/it] 89%|████████▉ | 166/186 [36:18<03:38, 10.95s/it] 90%|████████▉ | 167/186 [36:28<03:24, 10.77s/it] 90%|█████████ | 168/186 [36:39<03:13, 10.78s/it] 91%|█████████ | 169/186 [36:50<03:05, 10.92s/it] 91%|█████████▏| 170/186 [37:01<02:54, 10.90s/it] 92%|█████████▏| 171/186 [37:13<02:49, 11.30s/it] 92%|█████████▏| 172/186 [37:29<02:56, 12.63s/it] 93%|█████████▎| 173/186 [37:45<02:59, 13.79s/it] 94%|█████████▎| 174/186 [38:02<02:54, 14.51s/it] 94%|█████████▍| 175/186 [38:18<02:46, 15.11s/it] 95%|█████████▍| 176/186 [38:34<02:33, 15.39s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 95%|█████████▌| 177/186 [38:50<02:19, 15.53s/it] 96%|█████████▌| 178/186 [39:06<02:05, 15.69s/it] 96%|█████████▌| 179/186 [39:23<01:51, 16.00s/it] 97%|█████████▋| 180/186 [39:39<01:35, 15.94s/it] 97%|█████████▋| 181/186 [39:55<01:20, 16.16s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 98%|█████████▊| 182/186 [40:12<01:04, 16.21s/it] 98%|█████████▊| 183/186 [40:28<00:48, 16.20s/it] 99%|█████████▉| 184/186 [40:44<00:32, 16.08s/it] 99%|█████████▉| 185/186 [41:00<00:16, 16.26s/it]100%|██████████| 186/186 [41:17<00:00, 16.30s/it]                                                 {'train_runtime': 2477.2278, 'train_samples_per_second': 5.423, 'train_steps_per_second': 0.075, 'train_loss': 0.19158564331710978, 'epoch': 1.0}
100%|██████████| 186/186 [41:17<00:00, 16.30s/it]100%|██████████| 186/186 [41:17<00:00, 13.32s/it]
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-3
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33msliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-3[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251017_105747-6q1tq3uk/logs[0m
[2025-10-17 11:40:58,541] [INFO] [launch.py:347:main] Process 2239835 exits successfully.
[2025-10-17 11:40:58,542] [INFO] [launch.py:347:main] Process 2239832 exits successfully.
[2025-10-17 11:40:58,542] [INFO] [launch.py:347:main] Process 2239836 exits successfully.
[2025-10-17 11:40:58,542] [INFO] [launch.py:347:main] Process 2239834 exits successfully.
[2025-10-17 11:40:58,542] [INFO] [launch.py:347:main] Process 2239833 exits successfully.
[2025-10-17 11:41:03,548] [INFO] [launch.py:347:main] Process 2239831 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 11:41:07,645] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 11:41:08,519] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-17 11:41:08,519] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12423 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-3 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-3 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 2 --use_anchor False --use_sample_weight True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 11:41:10,073] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 11:41:10,673] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-17 11:41:10,673] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-17 11:41:10,673] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-17 11:41:10,673] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-17 11:41:10,673] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 11:41:14,527] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-17 11:41:14,726] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 11:41:14,927] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 11:41:15,009] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 11:41:15,097] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 11:41:15,105] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 11:41:15,334] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 11:41:15,448] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 11:41:15,448] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-10-17 11:41:15,849] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 11:41:15,858] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 11:41:16,174] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-10-17 11:41:16,204] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: creating run
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251017_114116-vblfppky
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-3
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/vblfppky
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.68s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.19s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.56s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.91s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.56s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.48s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.51s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.55s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.91s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.42s/it]to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.70s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.25s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.89s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.28s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.91s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.29s/it]
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.57s/it]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.49s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.80s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.66s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.38s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.73s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.99s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.92s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.75s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.74s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.24s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.64s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.16s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.56s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.18s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.56s/it]
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:1')
Formatting inputs...Skip in lazy mode
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:4')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:3')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:2')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:5')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:0')
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.014109611511230469 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.009700298309326172 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10110759735107422 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.013136625289916992 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10159897804260254 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10158658027648926 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/186 [00:10<32:45, 10.63s/it]  1%|          | 2/186 [00:19<29:22,  9.58s/it]  2%|▏         | 3/186 [00:31<32:58, 10.81s/it]WARNING: tokenization mismatch: 1 vs. 48. (ignored)
  2%|▏         | 4/186 [00:47<39:15, 12.94s/it]  3%|▎         | 5/186 [01:03<42:12, 13.99s/it]  3%|▎         | 6/186 [01:20<44:18, 14.77s/it]  4%|▍         | 7/186 [01:36<45:12, 15.15s/it]  4%|▍         | 8/186 [01:52<45:48, 15.44s/it]  5%|▍         | 9/186 [02:08<46:02, 15.61s/it]  5%|▌         | 10/186 [02:23<45:54, 15.65s/it]  6%|▌         | 11/186 [02:39<45:35, 15.63s/it]  6%|▋         | 12/186 [02:55<45:35, 15.72s/it]  7%|▋         | 13/186 [03:11<45:39, 15.83s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
  8%|▊         | 14/186 [03:27<45:24, 15.84s/it]  8%|▊         | 15/186 [03:43<45:14, 15.87s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1240 > 1048). Running this sequence through the model will result in indexing errors
  9%|▊         | 16/186 [03:59<45:34, 16.09s/it]  9%|▉         | 17/186 [04:16<45:26, 16.14s/it] 10%|▉         | 18/186 [04:32<45:32, 16.27s/it] 10%|█         | 19/186 [04:48<45:13, 16.25s/it] 11%|█         | 20/186 [05:05<45:06, 16.30s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 11%|█▏        | 21/186 [05:21<44:43, 16.26s/it] 12%|█▏        | 22/186 [05:37<44:32, 16.29s/it] 12%|█▏        | 23/186 [05:53<44:04, 16.23s/it] 13%|█▎        | 24/186 [06:10<44:08, 16.35s/it] 13%|█▎        | 25/186 [06:26<43:44, 16.30s/it] 14%|█▍        | 26/186 [06:43<43:37, 16.36s/it] 15%|█▍        | 27/186 [06:53<38:54, 14.68s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1073 > 1048). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1263 > 1048). Running this sequence through the model will result in indexing errors
 15%|█▌        | 28/186 [07:05<36:09, 13.73s/it] 16%|█▌        | 29/186 [07:15<33:07, 12.66s/it] 16%|█▌        | 30/186 [07:26<31:45, 12.21s/it] 17%|█▋        | 31/186 [07:37<30:41, 11.88s/it] 17%|█▋        | 32/186 [07:48<29:33, 11.52s/it] 18%|█▊        | 33/186 [08:01<30:47, 12.08s/it] 18%|█▊        | 34/186 [08:17<33:34, 13.25s/it] 19%|█▉        | 35/186 [08:34<35:28, 14.09s/it] 19%|█▉        | 36/186 [08:50<36:42, 14.69s/it] 20%|█▉        | 37/186 [09:06<37:34, 15.13s/it] 20%|██        | 38/186 [09:22<37:57, 15.39s/it] 21%|██        | 39/186 [09:38<38:13, 15.60s/it] 22%|██▏       | 40/186 [09:54<38:10, 15.69s/it] 22%|██▏       | 41/186 [10:10<38:09, 15.79s/it] 23%|██▎       | 42/186 [10:26<38:12, 15.92s/it] 23%|██▎       | 43/186 [10:42<38:19, 16.08s/it] 24%|██▎       | 44/186 [10:59<38:18, 16.18s/it] 24%|██▍       | 45/186 [11:15<38:16, 16.28s/it] 25%|██▍       | 46/186 [11:32<38:05, 16.33s/it] 25%|██▌       | 47/186 [11:48<37:52, 16.35s/it] 26%|██▌       | 48/186 [12:04<37:24, 16.26s/it] 26%|██▋       | 49/186 [12:21<37:45, 16.54s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1240 > 1048). Running this sequence through the model will result in indexing errors
 27%|██▋       | 50/186 [12:38<37:41, 16.63s/it] 27%|██▋       | 51/186 [12:55<37:25, 16.63s/it] 28%|██▊       | 52/186 [13:11<36:48, 16.48s/it] 28%|██▊       | 53/186 [13:27<36:21, 16.40s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1240 > 1048). Running this sequence through the model will result in indexing errors
 29%|██▉       | 54/186 [13:44<36:19, 16.51s/it] 30%|██▉       | 55/186 [14:01<36:39, 16.79s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
 30%|███       | 56/186 [14:18<35:55, 16.58s/it] 31%|███       | 57/186 [14:29<32:07, 14.94s/it] 31%|███       | 58/186 [14:39<29:12, 13.69s/it] 32%|███▏      | 59/186 [14:51<27:20, 12.92s/it] 32%|███▏      | 60/186 [15:02<25:54, 12.34s/it] 33%|███▎      | 61/186 [15:12<24:46, 11.89s/it] 33%|███▎      | 62/186 [15:24<24:31, 11.87s/it] 34%|███▍      | 63/186 [15:40<26:53, 13.12s/it] 34%|███▍      | 64/186 [15:56<28:27, 13.99s/it] 35%|███▍      | 65/186 [16:12<29:27, 14.61s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 35%|███▌      | 66/186 [16:29<30:22, 15.19s/it] 36%|███▌      | 67/186 [16:45<30:42, 15.48s/it] 37%|███▋      | 68/186 [17:01<30:45, 15.64s/it] 37%|███▋      | 69/186 [17:17<30:38, 15.71s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1200 > 1048). Running this sequence through the model will result in indexing errors
 38%|███▊      | 70/186 [17:34<30:58, 16.02s/it] 38%|███▊      | 71/186 [17:50<30:58, 16.16s/it] 39%|███▊      | 72/186 [18:06<30:49, 16.22s/it] 39%|███▉      | 73/186 [18:23<30:35, 16.25s/it] 40%|███▉      | 74/186 [18:39<30:24, 16.29s/it] 40%|████      | 75/186 [18:55<29:59, 16.21s/it] 41%|████      | 76/186 [19:11<29:36, 16.15s/it] 41%|████▏     | 77/186 [19:27<29:16, 16.11s/it] 42%|████▏     | 78/186 [19:44<29:14, 16.25s/it] 42%|████▏     | 79/186 [20:00<28:57, 16.24s/it] 43%|████▎     | 80/186 [20:16<28:47, 16.30s/it] 44%|████▎     | 81/186 [20:33<28:37, 16.35s/it] 44%|████▍     | 82/186 [20:49<28:25, 16.40s/it] 45%|████▍     | 83/186 [21:06<28:14, 16.45s/it] 45%|████▌     | 84/186 [21:23<28:08, 16.56s/it] 46%|████▌     | 85/186 [21:40<28:15, 16.79s/it] 46%|████▌     | 86/186 [21:53<25:53, 15.54s/it]WARNING: tokenization mismatch: 1 vs. 48. (ignored)
 47%|████▋     | 87/186 [22:04<23:38, 14.33s/it] 47%|████▋     | 88/186 [22:15<21:31, 13.18s/it] 48%|████▊     | 89/186 [22:26<20:16, 12.54s/it] 48%|████▊     | 90/186 [22:37<19:23, 12.12s/it] 49%|████▉     | 91/186 [22:49<18:57, 11.97s/it] 49%|████▉     | 92/186 [23:04<20:27, 13.06s/it] 50%|█████     | 93/186 [23:20<21:37, 13.95s/it] 51%|█████     | 94/186 [23:37<22:29, 14.67s/it] 51%|█████     | 95/186 [23:52<22:46, 15.02s/it] 52%|█████▏    | 96/186 [24:08<23:00, 15.34s/it] 52%|█████▏    | 97/186 [24:24<22:58, 15.49s/it] 53%|█████▎    | 98/186 [24:40<22:58, 15.66s/it] 53%|█████▎    | 99/186 [24:56<22:47, 15.72s/it] 54%|█████▍    | 100/186 [25:12<22:33, 15.74s/it] 54%|█████▍    | 101/186 [25:28<22:31, 15.90s/it] 55%|█████▍    | 102/186 [25:45<22:39, 16.19s/it] 55%|█████▌    | 103/186 [26:02<22:36, 16.34s/it] 56%|█████▌    | 104/186 [26:18<22:23, 16.39s/it] 56%|█████▋    | 105/186 [26:35<22:03, 16.35s/it] 57%|█████▋    | 106/186 [26:51<21:42, 16.28s/it] 58%|█████▊    | 107/186 [27:07<21:32, 16.36s/it] 58%|█████▊    | 108/186 [27:24<21:30, 16.54s/it] 59%|█████▊    | 109/186 [27:41<21:11, 16.51s/it] 59%|█████▉    | 110/186 [27:57<20:54, 16.50s/it] 60%|█████▉    | 111/186 [28:13<20:27, 16.37s/it] 60%|██████    | 112/186 [28:29<20:08, 16.33s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 61%|██████    | 113/186 [28:46<20:04, 16.51s/it] 61%|██████▏   | 114/186 [29:03<19:56, 16.62s/it] 62%|██████▏   | 115/186 [29:15<17:49, 15.06s/it] 62%|██████▏   | 116/186 [29:26<16:15, 13.94s/it] 63%|██████▎   | 117/186 [29:36<14:49, 12.90s/it] 63%|██████▎   | 118/186 [29:47<13:56, 12.29s/it] 64%|██████▍   | 119/186 [29:58<13:19, 11.94s/it] 65%|██████▍   | 120/186 [30:09<12:42, 11.55s/it] 65%|██████▌   | 121/186 [30:23<13:22, 12.34s/it] 66%|██████▌   | 122/186 [30:39<14:18, 13.42s/it] 66%|██████▌   | 123/186 [30:55<14:53, 14.19s/it] 67%|██████▋   | 124/186 [31:11<15:11, 14.71s/it] 67%|██████▋   | 125/186 [31:27<15:19, 15.08s/it] 68%|██████▊   | 126/186 [31:43<15:20, 15.34s/it] 68%|██████▊   | 127/186 [31:59<15:14, 15.51s/it] 69%|██████▉   | 128/186 [32:15<15:08, 15.67s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 69%|██████▉   | 129/186 [32:31<14:54, 15.69s/it] 70%|██████▉   | 130/186 [32:47<14:44, 15.80s/it] 70%|███████   | 131/186 [33:03<14:36, 15.94s/it] 71%|███████   | 132/186 [33:19<14:28, 16.08s/it] 72%|███████▏  | 133/186 [33:36<14:23, 16.29s/it] 72%|███████▏  | 134/186 [33:53<14:06, 16.28s/it] 73%|███████▎  | 135/186 [34:10<14:05, 16.58s/it] 73%|███████▎  | 136/186 [34:26<13:46, 16.52s/it] 74%|███████▎  | 137/186 [34:43<13:35, 16.63s/it] 74%|███████▍  | 138/186 [34:59<13:12, 16.50s/it] 75%|███████▍  | 139/186 [35:16<12:56, 16.53s/it] 75%|███████▌  | 140/186 [35:32<12:35, 16.43s/it] 76%|███████▌  | 141/186 [35:48<12:13, 16.30s/it] 76%|███████▋  | 142/186 [36:04<11:58, 16.33s/it] 77%|███████▋  | 143/186 [36:21<11:44, 16.39s/it] 77%|███████▋  | 144/186 [36:37<11:25, 16.31s/it] 78%|███████▊  | 145/186 [36:48<10:03, 14.72s/it] 78%|███████▊  | 146/186 [37:00<09:09, 13.73s/it] 79%|███████▉  | 147/186 [37:10<08:17, 12.75s/it] 80%|███████▉  | 148/186 [37:21<07:46, 12.27s/it] 80%|████████  | 149/186 [37:33<07:24, 12.01s/it] 81%|████████  | 150/186 [37:44<07:06, 11.84s/it] 81%|████████  | 151/186 [38:00<07:37, 13.07s/it] 82%|████████▏ | 152/186 [38:16<07:58, 14.06s/it] 82%|████████▏ | 153/186 [38:33<08:10, 14.86s/it] 83%|████████▎ | 154/186 [38:49<08:10, 15.34s/it] 83%|████████▎ | 155/186 [39:06<08:02, 15.57s/it] 84%|████████▍ | 156/186 [39:22<07:51, 15.70s/it] 84%|████████▍ | 157/186 [39:38<07:38, 15.82s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 85%|████████▍ | 158/186 [39:54<07:29, 16.06s/it] 85%|████████▌ | 159/186 [40:11<07:15, 16.13s/it] 86%|████████▌ | 160/186 [40:27<07:02, 16.25s/it] 87%|████████▋ | 161/186 [40:44<06:47, 16.30s/it] 87%|████████▋ | 162/186 [41:00<06:34, 16.45s/it] 88%|████████▊ | 163/186 [41:17<06:17, 16.40s/it] 88%|████████▊ | 164/186 [41:33<06:00, 16.41s/it] 89%|████████▊ | 165/186 [41:49<05:44, 16.38s/it] 89%|████████▉ | 166/186 [42:06<05:28, 16.45s/it] 90%|████████▉ | 167/186 [42:22<05:12, 16.44s/it] 90%|█████████ | 168/186 [42:39<04:56, 16.46s/it] 91%|█████████ | 169/186 [42:55<04:38, 16.40s/it] 91%|█████████▏| 170/186 [43:11<04:21, 16.32s/it] 92%|█████████▏| 171/186 [43:28<04:05, 16.37s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 92%|█████████▏| 172/186 [43:45<03:53, 16.65s/it] 93%|█████████▎| 173/186 [44:02<03:37, 16.70s/it] 94%|█████████▎| 174/186 [44:15<03:06, 15.51s/it] 94%|█████████▍| 175/186 [44:26<02:35, 14.18s/it] 95%|█████████▍| 176/186 [44:36<02:11, 13.11s/it] 95%|█████████▌| 177/186 [44:48<01:52, 12.53s/it] 96%|█████████▌| 178/186 [44:58<01:36, 12.02s/it] 96%|█████████▌| 179/186 [45:09<01:21, 11.65s/it] 97%|█████████▋| 180/186 [45:25<01:17, 12.88s/it] 97%|█████████▋| 181/186 [45:41<01:09, 13.91s/it] 98%|█████████▊| 182/186 [45:58<00:58, 14.68s/it] 98%|█████████▊| 183/186 [46:15<00:46, 15.50s/it] 99%|█████████▉| 184/186 [46:31<00:31, 15.71s/it] 99%|█████████▉| 185/186 [46:48<00:15, 15.89s/it]100%|██████████| 186/186 [47:04<00:00, 16.10s/it]                                                 {'train_runtime': 2824.6946, 'train_samples_per_second': 4.756, 'train_steps_per_second': 0.066, 'train_loss': 0.20776317965599797, 'epoch': 1.0}
100%|██████████| 186/186 [47:04<00:00, 16.10s/it]100%|██████████| 186/186 [47:04<00:00, 15.19s/it]
[2025-10-17 12:30:08,008] [INFO] [launch.py:347:main] Process 2281183 exits successfully.
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-3
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33msliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-3[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251017_114116-vblfppky/logs[0m
[2025-10-17 12:30:14,015] [INFO] [launch.py:347:main] Process 2281184 exits successfully.
[2025-10-17 12:30:14,016] [INFO] [launch.py:347:main] Process 2281181 exits successfully.
[2025-10-17 12:30:14,016] [INFO] [launch.py:347:main] Process 2281185 exits successfully.
[2025-10-17 12:30:14,016] [INFO] [launch.py:347:main] Process 2281182 exits successfully.
[2025-10-17 12:30:18,021] [INFO] [launch.py:347:main] Process 2281180 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 12:30:22,252] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 12:30:23,235] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-17 12:30:23,236] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12423 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1.5e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-3 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-3 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 0.5 --use_anchor False --use_sample_weight True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 12:30:25,768] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 12:30:26,699] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-17 12:30:26,699] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-17 12:30:26,699] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-17 12:30:26,699] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-17 12:30:26,699] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 12:30:30,393] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-17 12:30:30,605] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 12:30:30,733] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 12:30:30,945] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 12:30:31,332] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-17 12:30:31,780] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 12:30:31,912] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 12:30:32,003] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 12:30:32,245] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 12:30:32,487] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 12:30:32,587] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 12:30:33,040] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 12:30:33,040] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: creating run
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251017_123034-23x15z0l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-3
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/23x15z0l
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.66s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.69s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.98s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.26s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.62s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.84s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.06s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.38s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.19s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.51s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.86s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.70s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.06s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.30s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.87s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.22s/it]
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.76s/it]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.50s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.84s/it]
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.06s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.29s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.26s/it][MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:4')
Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.27s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.95s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.45s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.91s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.37s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.56s/it]
Formatting inputs...Skip in lazy mode
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.07s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.56s/it]
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:1')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:3')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:0')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:2')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:5')
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.010594367980957031 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.008817911148071289 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.011652946472167969 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.20344114303588867 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.20183396339416504 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.2014751434326172 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/186 [00:16<51:44, 16.78s/it]  1%|          | 2/186 [00:32<49:44, 16.22s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1221 > 1048). Running this sequence through the model will result in indexing errors
  2%|▏         | 3/186 [00:49<49:54, 16.37s/it]  2%|▏         | 4/186 [01:05<49:46, 16.41s/it]  3%|▎         | 5/186 [01:16<43:31, 14.43s/it]  3%|▎         | 6/186 [01:27<39:33, 13.18s/it]  4%|▍         | 7/186 [01:37<36:32, 12.25s/it]  4%|▍         | 8/186 [01:49<35:40, 12.03s/it]  5%|▍         | 9/186 [02:00<34:36, 11.73s/it]  5%|▌         | 10/186 [02:11<33:35, 11.45s/it]  6%|▌         | 11/186 [02:25<36:21, 12.47s/it]  6%|▋         | 12/186 [02:42<39:38, 13.67s/it]  7%|▋         | 13/186 [02:58<41:29, 14.39s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1165 > 1048). Running this sequence through the model will result in indexing errors
  8%|▊         | 14/186 [03:14<42:59, 15.00s/it]  8%|▊         | 15/186 [03:30<43:43, 15.34s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
  9%|▊         | 16/186 [03:46<43:53, 15.49s/it]  9%|▉         | 17/186 [04:02<43:49, 15.56s/it] 10%|▉         | 18/186 [04:18<43:48, 15.65s/it] 10%|█         | 19/186 [04:34<43:45, 15.72s/it] 11%|█         | 20/186 [04:50<43:51, 15.86s/it] 11%|█▏        | 21/186 [05:06<43:49, 15.93s/it] 12%|█▏        | 22/186 [05:22<43:54, 16.06s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1074 > 1048). Running this sequence through the model will result in indexing errors
 12%|█▏        | 23/186 [05:39<43:50, 16.14s/it] 13%|█▎        | 24/186 [05:56<44:24, 16.45s/it] 13%|█▎        | 25/186 [06:12<44:07, 16.44s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 14%|█▍        | 26/186 [06:29<43:45, 16.41s/it] 15%|█▍        | 27/186 [06:45<43:36, 16.46s/it] 15%|█▌        | 28/186 [07:02<43:31, 16.53s/it] 16%|█▌        | 29/186 [07:18<42:51, 16.38s/it] 16%|█▌        | 30/186 [07:34<42:09, 16.21s/it] 17%|█▋        | 31/186 [07:50<41:56, 16.24s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1263 > 1048). Running this sequence through the model will result in indexing errors
 17%|█▋        | 32/186 [08:07<42:17, 16.48s/it] 18%|█▊        | 33/186 [08:24<42:00, 16.48s/it] 18%|█▊        | 34/186 [08:36<38:24, 15.16s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 19%|█▉        | 35/186 [08:47<35:10, 13.97s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1153 > 1048). Running this sequence through the model will result in indexing errors
 19%|█▉        | 36/186 [08:57<32:04, 12.83s/it] 20%|█▉        | 37/186 [09:08<30:25, 12.25s/it] 20%|██        | 38/186 [09:19<29:26, 11.93s/it] 21%|██        | 39/186 [09:30<28:33, 11.65s/it] 22%|██▏       | 40/186 [09:44<30:01, 12.34s/it] 22%|██▏       | 41/186 [10:00<32:24, 13.41s/it] 23%|██▎       | 42/186 [10:16<34:01, 14.18s/it] 23%|██▎       | 43/186 [10:32<35:10, 14.76s/it] 24%|██▎       | 44/186 [10:48<35:54, 15.17s/it] 24%|██▍       | 45/186 [11:05<36:55, 15.72s/it] 25%|██▍       | 46/186 [11:22<37:11, 15.94s/it] 25%|██▌       | 47/186 [11:38<37:04, 16.00s/it] 26%|██▌       | 48/186 [11:54<37:02, 16.11s/it] 26%|██▋       | 49/186 [12:10<36:44, 16.09s/it] 27%|██▋       | 50/186 [12:27<36:40, 16.18s/it] 27%|██▋       | 51/186 [12:43<36:26, 16.19s/it] 28%|██▊       | 52/186 [12:59<36:29, 16.34s/it] 28%|██▊       | 53/186 [13:16<36:33, 16.49s/it] 29%|██▉       | 54/186 [13:33<36:10, 16.44s/it] 30%|██▉       | 55/186 [13:49<35:57, 16.47s/it] 30%|███       | 56/186 [14:06<36:10, 16.69s/it] 31%|███       | 57/186 [14:23<35:47, 16.65s/it] 31%|███       | 58/186 [14:39<35:26, 16.62s/it] 32%|███▏      | 59/186 [14:56<35:05, 16.58s/it] 32%|███▏      | 60/186 [15:12<34:36, 16.48s/it] 33%|███▎      | 61/186 [15:29<34:25, 16.52s/it] 33%|███▎      | 62/186 [15:46<34:36, 16.74s/it] 34%|███▍      | 63/186 [16:00<32:41, 15.95s/it] 34%|███▍      | 64/186 [16:12<29:50, 14.68s/it] 35%|███▍      | 65/186 [16:22<27:00, 13.39s/it] 35%|███▌      | 66/186 [16:33<25:03, 12.53s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1073 > 1048). Running this sequence through the model will result in indexing errors
 36%|███▌      | 67/186 [16:44<23:48, 12.01s/it] 37%|███▋      | 68/186 [16:54<22:43, 11.55s/it] 37%|███▋      | 69/186 [17:05<22:08, 11.35s/it] 38%|███▊      | 70/186 [17:16<21:42, 11.23s/it] 38%|███▊      | 71/186 [17:26<21:09, 11.04s/it]WARNING: tokenization mismatch: 1 vs. 47. (ignored)
 39%|███▊      | 72/186 [17:37<20:40, 10.88s/it] 39%|███▉      | 73/186 [17:48<20:20, 10.80s/it] 40%|███▉      | 74/186 [17:58<20:09, 10.80s/it] 40%|████      | 75/186 [18:09<19:57, 10.79s/it] 41%|████      | 76/186 [18:20<19:43, 10.76s/it] 41%|████▏     | 77/186 [18:30<19:24, 10.69s/it] 42%|████▏     | 78/186 [18:41<19:12, 10.67s/it] 42%|████▏     | 79/186 [18:52<19:07, 10.72s/it] 43%|████▎     | 80/186 [19:02<18:52, 10.69s/it] 44%|████▎     | 81/186 [19:13<18:39, 10.67s/it] 44%|████▍     | 82/186 [19:24<18:24, 10.62s/it] 45%|████▍     | 83/186 [19:34<18:20, 10.68s/it] 45%|████▌     | 84/186 [19:45<18:10, 10.69s/it] 46%|████▌     | 85/186 [19:56<17:52, 10.62s/it] 46%|████▌     | 86/186 [20:06<17:42, 10.62s/it] 47%|████▋     | 87/186 [20:17<17:31, 10.62s/it] 47%|████▋     | 88/186 [20:27<17:20, 10.62s/it] 48%|████▊     | 89/186 [20:38<17:13, 10.66s/it] 48%|████▊     | 90/186 [20:49<17:14, 10.78s/it] 49%|████▉     | 91/186 [21:00<17:00, 10.74s/it] 49%|████▉     | 92/186 [21:11<17:00, 10.86s/it] 50%|█████     | 93/186 [21:22<16:57, 10.94s/it] 51%|█████     | 94/186 [21:33<16:43, 10.91s/it] 51%|█████     | 95/186 [21:44<16:30, 10.88s/it] 52%|█████▏    | 96/186 [21:55<16:15, 10.84s/it] 52%|█████▏    | 97/186 [22:05<16:03, 10.83s/it] 53%|█████▎    | 98/186 [22:16<15:55, 10.86s/it] 53%|█████▎    | 99/186 [22:27<15:44, 10.86s/it] 54%|█████▍    | 100/186 [22:38<15:32, 10.85s/it] 54%|█████▍    | 101/186 [22:49<15:26, 10.90s/it] 55%|█████▍    | 102/186 [23:00<15:12, 10.86s/it] 55%|█████▌    | 103/186 [23:11<15:16, 11.04s/it] 56%|█████▌    | 104/186 [23:22<15:05, 11.04s/it] 56%|█████▋    | 105/186 [23:33<14:50, 11.00s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 57%|█████▋    | 106/186 [23:44<14:41, 11.02s/it] 58%|█████▊    | 107/186 [23:55<14:25, 10.96s/it] 58%|█████▊    | 108/186 [24:06<14:15, 10.97s/it] 59%|█████▊    | 109/186 [24:17<14:00, 10.92s/it] 59%|█████▉    | 110/186 [24:28<13:45, 10.86s/it] 60%|█████▉    | 111/186 [24:38<13:32, 10.84s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 60%|██████    | 112/186 [24:49<13:23, 10.86s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 61%|██████    | 113/186 [25:00<13:11, 10.85s/it] 61%|██████▏   | 114/186 [25:11<13:02, 10.87s/it] 62%|██████▏   | 115/186 [25:22<12:51, 10.87s/it] 62%|██████▏   | 116/186 [25:33<12:51, 11.02s/it] 63%|██████▎   | 117/186 [25:44<12:38, 10.99s/it] 63%|██████▎   | 118/186 [25:55<12:25, 10.96s/it] 64%|██████▍   | 119/186 [26:06<12:12, 10.93s/it] 65%|██████▍   | 120/186 [26:17<12:00, 10.92s/it] 65%|██████▌   | 121/186 [26:28<11:45, 10.86s/it] 66%|██████▌   | 122/186 [26:38<11:36, 10.88s/it] 66%|██████▌   | 123/186 [26:49<11:25, 10.88s/it] 67%|██████▋   | 124/186 [27:00<11:11, 10.84s/it] 67%|██████▋   | 125/186 [27:11<10:58, 10.79s/it] 68%|██████▊   | 126/186 [27:22<10:48, 10.81s/it] 68%|██████▊   | 127/186 [27:33<10:40, 10.86s/it] 69%|██████▉   | 128/186 [27:43<10:29, 10.85s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
 69%|██████▉   | 129/186 [27:54<10:17, 10.83s/it] 70%|██████▉   | 130/186 [28:05<10:04, 10.79s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 70%|███████   | 131/186 [28:16<09:53, 10.79s/it] 71%|███████   | 132/186 [28:26<09:42, 10.79s/it] 72%|███████▏  | 133/186 [28:37<09:34, 10.84s/it] 72%|███████▏  | 134/186 [28:48<09:25, 10.88s/it] 73%|███████▎  | 135/186 [28:59<09:12, 10.84s/it] 73%|███████▎  | 136/186 [29:10<09:05, 10.91s/it] 74%|███████▎  | 137/186 [29:22<09:00, 11.04s/it] 74%|███████▍  | 138/186 [29:32<08:47, 11.00s/it] 75%|███████▍  | 139/186 [29:43<08:36, 10.99s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 75%|███████▌  | 140/186 [29:55<08:32, 11.15s/it] 76%|███████▌  | 141/186 [30:06<08:22, 11.18s/it] 76%|███████▋  | 142/186 [30:17<08:11, 11.18s/it] 77%|███████▋  | 143/186 [30:28<07:58, 11.13s/it] 77%|███████▋  | 144/186 [30:39<07:45, 11.07s/it] 78%|███████▊  | 145/186 [30:50<07:31, 11.01s/it] 78%|███████▊  | 146/186 [31:01<07:18, 10.95s/it] 79%|███████▉  | 147/186 [31:12<07:08, 10.98s/it] 80%|███████▉  | 148/186 [31:23<06:56, 10.96s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 80%|████████  | 149/186 [31:34<06:47, 11.00s/it] 81%|████████  | 150/186 [31:45<06:36, 11.01s/it] 81%|████████  | 151/186 [31:56<06:24, 11.00s/it] 82%|████████▏ | 152/186 [32:07<06:14, 11.00s/it] 82%|████████▏ | 153/186 [32:18<06:02, 11.00s/it] 83%|████████▎ | 154/186 [32:29<05:51, 11.00s/it] 83%|████████▎ | 155/186 [32:40<05:39, 10.95s/it] 84%|████████▍ | 156/186 [32:51<05:27, 10.93s/it] 84%|████████▍ | 157/186 [33:02<05:15, 10.87s/it] 85%|████████▍ | 158/186 [33:13<05:07, 10.97s/it] 85%|████████▌ | 159/186 [33:24<04:57, 11.02s/it] 86%|████████▌ | 160/186 [33:35<04:50, 11.18s/it] 87%|████████▋ | 161/186 [33:46<04:36, 11.08s/it] 87%|████████▋ | 162/186 [33:57<04:26, 11.12s/it] 88%|████████▊ | 163/186 [34:09<04:15, 11.10s/it] 88%|████████▊ | 164/186 [34:19<04:01, 10.99s/it] 89%|████████▊ | 165/186 [34:31<03:53, 11.11s/it] 89%|████████▉ | 166/186 [34:42<03:40, 11.04s/it] 90%|████████▉ | 167/186 [34:52<03:29, 11.00s/it] 90%|█████████ | 168/186 [35:03<03:17, 10.97s/it] 91%|█████████ | 169/186 [35:14<03:06, 10.95s/it] 91%|█████████▏| 170/186 [35:25<02:54, 10.93s/it] 92%|█████████▏| 171/186 [35:36<02:43, 10.93s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
 92%|█████████▏| 172/186 [35:47<02:32, 10.87s/it] 93%|█████████▎| 173/186 [35:58<02:20, 10.85s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 94%|█████████▎| 174/186 [36:09<02:10, 10.91s/it] 94%|█████████▍| 175/186 [36:19<01:59, 10.88s/it] 95%|█████████▍| 176/186 [36:30<01:48, 10.87s/it] 95%|█████████▌| 177/186 [36:41<01:38, 10.94s/it] 96%|█████████▌| 178/186 [36:52<01:27, 10.95s/it] 96%|█████████▌| 179/186 [37:03<01:16, 10.91s/it] 97%|█████████▋| 180/186 [37:14<01:05, 10.90s/it] 97%|█████████▋| 181/186 [37:25<00:55, 11.02s/it] 98%|█████████▊| 182/186 [37:36<00:44, 11.05s/it] 98%|█████████▊| 183/186 [37:47<00:32, 10.99s/it] 99%|█████████▉| 184/186 [37:58<00:21, 11.00s/it] 99%|█████████▉| 185/186 [38:10<00:11, 11.06s/it]100%|██████████| 186/186 [38:20<00:00, 10.98s/it]                                                 {'train_runtime': 2300.8363, 'train_samples_per_second': 5.839, 'train_steps_per_second': 0.081, 'train_loss': 0.14375281590287403, 'epoch': 1.0}
100%|██████████| 186/186 [38:20<00:00, 10.98s/it]100%|██████████| 186/186 [38:20<00:00, 12.37s/it]
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-3
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33msliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-3[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251017_123034-23x15z0l/logs[0m
[2025-10-17 13:10:51,508] [INFO] [launch.py:347:main] Process 2331021 exits successfully.
[2025-10-17 13:10:51,509] [INFO] [launch.py:347:main] Process 2331020 exits successfully.
[2025-10-17 13:10:51,509] [INFO] [launch.py:347:main] Process 2331024 exits successfully.
[2025-10-17 13:10:52,510] [INFO] [launch.py:347:main] Process 2331022 exits successfully.
[2025-10-17 13:10:52,510] [INFO] [launch.py:347:main] Process 2331023 exits successfully.
[2025-10-17 13:10:56,515] [INFO] [launch.py:347:main] Process 2331019 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 13:10:59,809] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 13:11:00,360] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-17 13:11:00,360] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12423 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1.5e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-3 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-3 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 1 --use_anchor False --use_sample_weight True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 13:11:02,069] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 13:11:02,646] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-17 13:11:02,646] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-17 13:11:02,646] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-17 13:11:02,646] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-17 13:11:02,646] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 13:11:06,485] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 13:11:06,578] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-17 13:11:06,725] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 13:11:06,803] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 13:11:06,803] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-17 13:11:06,887] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 13:11:06,935] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 13:11:07,032] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 13:11:07,078] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 13:11:07,267] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
[2025-10-17 13:11:07,388] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
[2025-10-17 13:11:07,685] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 13:11:08,130] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251017_131108-049i6t9c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-3
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/049i6t9c
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.99s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.47s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.70s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.36s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.98s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.97s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.57s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.47s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.99s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.00s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.75s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.64s/it]
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.65s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.57s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.48s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.31s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.75s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.82s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.19s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.95s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.72s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.59s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.92s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.59s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.91s/it]
Formatting inputs...Skip in lazy mode
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:1')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:4')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:5')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:3')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:0')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:2')
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.010488748550415039 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Loading extension module fused_adam...
Time to load fused_adam op: 0.012372970581054688 seconds
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.009921073913574219 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10235118865966797 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.013111591339111328 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.01119685173034668 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/186 [00:09<30:17,  9.83s/it]  1%|          | 2/186 [00:18<28:10,  9.19s/it]  2%|▏         | 3/186 [00:28<28:31,  9.35s/it]  2%|▏         | 4/186 [00:37<28:38,  9.44s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
  3%|▎         | 5/186 [00:47<28:50,  9.56s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
  3%|▎         | 6/186 [00:57<28:58,  9.66s/it]  4%|▍         | 7/186 [01:07<29:05,  9.75s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1125 > 1048). Running this sequence through the model will result in indexing errors
  4%|▍         | 8/186 [01:17<29:12,  9.85s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
  5%|▍         | 9/186 [01:27<29:12,  9.90s/it]  5%|▌         | 10/186 [01:37<29:10,  9.95s/it]  6%|▌         | 11/186 [01:47<29:16, 10.04s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1065 > 1048). Running this sequence through the model will result in indexing errors
  6%|▋         | 12/186 [01:57<29:13, 10.08s/it]  7%|▋         | 13/186 [02:08<29:13, 10.13s/it]  8%|▊         | 14/186 [02:18<28:54, 10.09s/it]  8%|▊         | 15/186 [02:28<28:50, 10.12s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1073 > 1048). Running this sequence through the model will result in indexing errors
WARNING: tokenization mismatch: 1 vs. 48. (ignored)
  9%|▊         | 16/186 [02:38<28:54, 10.20s/it]  9%|▉         | 17/186 [02:48<28:42, 10.19s/it] 10%|▉         | 18/186 [02:59<28:34, 10.20s/it] 10%|█         | 19/186 [03:09<28:31, 10.25s/it] 11%|█         | 20/186 [03:19<28:20, 10.24s/it] 11%|█▏        | 21/186 [03:29<28:11, 10.25s/it] 12%|█▏        | 22/186 [03:40<28:06, 10.29s/it] 12%|█▏        | 23/186 [03:50<28:00, 10.31s/it] 13%|█▎        | 24/186 [04:01<28:12, 10.45s/it] 13%|█▎        | 25/186 [04:12<28:19, 10.55s/it] 14%|█▍        | 26/186 [04:22<28:09, 10.56s/it] 15%|█▍        | 27/186 [04:33<27:54, 10.53s/it] 15%|█▌        | 28/186 [04:43<27:36, 10.49s/it] 16%|█▌        | 29/186 [04:53<27:15, 10.42s/it] 16%|█▌        | 30/186 [05:04<27:09, 10.44s/it] 17%|█▋        | 31/186 [05:14<26:52, 10.40s/it] 17%|█▋        | 32/186 [05:25<26:45, 10.43s/it] 18%|█▊        | 33/186 [05:35<26:36, 10.43s/it] 18%|█▊        | 34/186 [05:46<26:30, 10.47s/it] 19%|█▉        | 35/186 [05:56<26:18, 10.45s/it] 19%|█▉        | 36/186 [06:06<26:03, 10.42s/it] 20%|█▉        | 37/186 [06:17<26:00, 10.47s/it] 20%|██        | 38/186 [06:28<25:54, 10.50s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1165 > 1048). Running this sequence through the model will result in indexing errors
 21%|██        | 39/186 [06:38<26:01, 10.62s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1297 > 1048). Running this sequence through the model will result in indexing errors
 22%|██▏       | 40/186 [06:49<25:50, 10.62s/it] 22%|██▏       | 41/186 [07:00<25:30, 10.56s/it] 23%|██▎       | 42/186 [07:10<25:09, 10.48s/it] 23%|██▎       | 43/186 [07:20<24:53, 10.45s/it] 24%|██▎       | 44/186 [07:31<24:48, 10.48s/it] 24%|██▍       | 45/186 [07:41<24:38, 10.49s/it] 25%|██▍       | 46/186 [07:52<24:18, 10.42s/it] 25%|██▌       | 47/186 [08:02<24:19, 10.50s/it] 26%|██▌       | 48/186 [08:12<23:59, 10.43s/it] 26%|██▋       | 49/186 [08:23<23:50, 10.44s/it] 27%|██▋       | 50/186 [08:34<23:49, 10.51s/it] 27%|██▋       | 51/186 [08:44<23:38, 10.51s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1150 > 1048). Running this sequence through the model will result in indexing errors
 28%|██▊       | 52/186 [08:55<23:40, 10.60s/it] 28%|██▊       | 53/186 [09:05<23:27, 10.59s/it] 29%|██▉       | 54/186 [09:16<23:17, 10.59s/it] 30%|██▉       | 55/186 [09:27<23:14, 10.64s/it] 30%|███       | 56/186 [09:38<23:07, 10.67s/it] 31%|███       | 57/186 [09:49<23:12, 10.80s/it] 31%|███       | 58/186 [09:59<22:59, 10.77s/it] 32%|███▏      | 59/186 [10:10<22:54, 10.82s/it] 32%|███▏      | 60/186 [10:21<22:37, 10.78s/it] 33%|███▎      | 61/186 [10:32<22:20, 10.72s/it] 33%|███▎      | 62/186 [10:43<22:17, 10.79s/it] 34%|███▍      | 63/186 [10:53<22:12, 10.83s/it] 34%|███▍      | 64/186 [11:04<21:58, 10.81s/it] 35%|███▍      | 65/186 [11:15<21:53, 10.86s/it] 35%|███▌      | 66/186 [11:26<21:39, 10.83s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
 36%|███▌      | 67/186 [11:36<21:18, 10.74s/it] 37%|███▋      | 68/186 [11:47<20:56, 10.65s/it] 37%|███▋      | 69/186 [11:57<20:43, 10.63s/it] 38%|███▊      | 70/186 [12:08<20:41, 10.70s/it] 38%|███▊      | 71/186 [12:19<20:24, 10.65s/it] 39%|███▊      | 72/186 [12:29<20:05, 10.58s/it] 39%|███▉      | 73/186 [12:40<19:55, 10.58s/it] 40%|███▉      | 74/186 [12:51<19:54, 10.67s/it] 40%|████      | 75/186 [13:02<19:47, 10.70s/it] 41%|████      | 76/186 [13:12<19:36, 10.69s/it] 41%|████▏     | 77/186 [13:23<19:21, 10.66s/it] 42%|████▏     | 78/186 [13:33<19:09, 10.65s/it] 42%|████▏     | 79/186 [13:44<19:04, 10.69s/it] 43%|████▎     | 80/186 [13:55<18:57, 10.73s/it] 44%|████▎     | 81/186 [14:06<18:46, 10.73s/it] 44%|████▍     | 82/186 [14:17<18:40, 10.78s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 45%|████▍     | 83/186 [14:28<18:33, 10.81s/it] 45%|████▌     | 84/186 [14:39<18:40, 10.98s/it] 46%|████▌     | 85/186 [14:50<18:18, 10.88s/it] 46%|████▌     | 86/186 [15:00<18:05, 10.85s/it] 47%|████▋     | 87/186 [15:11<17:47, 10.78s/it] 47%|████▋     | 88/186 [15:22<17:35, 10.77s/it] 48%|████▊     | 89/186 [15:32<17:24, 10.77s/it] 48%|████▊     | 90/186 [15:43<17:10, 10.74s/it] 49%|████▉     | 91/186 [15:54<17:14, 10.89s/it] 49%|████▉     | 92/186 [16:05<17:02, 10.88s/it] 50%|█████     | 93/186 [16:16<16:56, 10.93s/it] 51%|█████     | 94/186 [16:27<16:44, 10.92s/it] 51%|█████     | 95/186 [16:38<16:37, 10.96s/it] 52%|█████▏    | 96/186 [16:49<16:25, 10.95s/it] 52%|█████▏    | 97/186 [17:00<16:11, 10.91s/it] 53%|█████▎    | 98/186 [17:11<15:53, 10.84s/it] 53%|█████▎    | 99/186 [17:21<15:42, 10.84s/it] 54%|█████▍    | 100/186 [17:32<15:32, 10.84s/it] 54%|█████▍    | 101/186 [17:43<15:19, 10.81s/it] 55%|█████▍    | 102/186 [17:54<15:13, 10.88s/it] 55%|█████▌    | 103/186 [18:05<14:59, 10.83s/it] 56%|█████▌    | 104/186 [18:15<14:41, 10.75s/it] 56%|█████▋    | 105/186 [18:26<14:30, 10.75s/it] 57%|█████▋    | 106/186 [18:37<14:21, 10.77s/it] 58%|█████▊    | 107/186 [18:48<14:07, 10.73s/it] 58%|█████▊    | 108/186 [18:58<13:53, 10.68s/it] 59%|█████▊    | 109/186 [19:09<13:47, 10.74s/it] 59%|█████▉    | 110/186 [19:20<13:35, 10.73s/it] 60%|█████▉    | 111/186 [19:31<13:25, 10.74s/it] 60%|██████    | 112/186 [19:41<13:14, 10.74s/it] 61%|██████    | 113/186 [19:52<13:03, 10.73s/it] 61%|██████▏   | 114/186 [20:03<12:53, 10.75s/it] 62%|██████▏   | 115/186 [20:13<12:41, 10.72s/it] 62%|██████▏   | 116/186 [20:24<12:35, 10.79s/it] 63%|██████▎   | 117/186 [20:35<12:23, 10.77s/it] 63%|██████▎   | 118/186 [20:46<12:09, 10.73s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 64%|██████▍   | 119/186 [20:57<12:00, 10.75s/it] 65%|██████▍   | 120/186 [21:07<11:50, 10.77s/it] 65%|██████▌   | 121/186 [21:18<11:40, 10.77s/it] 66%|██████▌   | 122/186 [21:29<11:35, 10.86s/it] 66%|██████▌   | 123/186 [21:40<11:23, 10.85s/it] 67%|██████▋   | 124/186 [21:51<11:14, 10.89s/it] 67%|██████▋   | 125/186 [22:02<11:04, 10.89s/it] 68%|██████▊   | 126/186 [22:13<10:50, 10.84s/it] 68%|██████▊   | 127/186 [22:24<10:41, 10.87s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 69%|██████▉   | 128/186 [22:34<10:28, 10.83s/it] 69%|██████▉   | 129/186 [22:45<10:17, 10.83s/it] 70%|██████▉   | 130/186 [22:56<10:04, 10.80s/it] 70%|███████   | 131/186 [23:07<09:51, 10.76s/it] 71%|███████   | 132/186 [23:18<09:49, 10.92s/it] 72%|███████▏  | 133/186 [23:28<09:34, 10.85s/it] 72%|███████▏  | 134/186 [23:39<09:24, 10.86s/it] 73%|███████▎  | 135/186 [23:50<09:13, 10.84s/it] 73%|███████▎  | 136/186 [24:01<09:00, 10.81s/it] 74%|███████▎  | 137/186 [24:12<08:52, 10.86s/it] 74%|███████▍  | 138/186 [24:23<08:42, 10.88s/it] 75%|███████▍  | 139/186 [24:34<08:34, 10.95s/it] 75%|███████▌  | 140/186 [24:45<08:22, 10.93s/it] 76%|███████▌  | 141/186 [24:56<08:08, 10.87s/it] 76%|███████▋  | 142/186 [25:06<07:55, 10.81s/it] 77%|███████▋  | 143/186 [25:17<07:45, 10.83s/it] 77%|███████▋  | 144/186 [25:28<07:31, 10.76s/it] 78%|███████▊  | 145/186 [25:39<07:22, 10.79s/it] 78%|███████▊  | 146/186 [25:49<07:11, 10.78s/it] 79%|███████▉  | 147/186 [26:00<07:01, 10.82s/it] 80%|███████▉  | 148/186 [26:11<06:53, 10.89s/it] 80%|████████  | 149/186 [26:22<06:39, 10.81s/it] 81%|████████  | 150/186 [26:33<06:27, 10.76s/it] 81%|████████  | 151/186 [26:43<06:16, 10.74s/it] 82%|████████▏ | 152/186 [26:54<06:06, 10.77s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 82%|████████▏ | 153/186 [27:05<05:58, 10.86s/it] 83%|████████▎ | 154/186 [27:16<05:50, 10.94s/it] 83%|████████▎ | 155/186 [27:27<05:38, 10.92s/it] 84%|████████▍ | 156/186 [27:39<05:31, 11.06s/it] 84%|████████▍ | 157/186 [27:49<05:18, 10.99s/it] 85%|████████▍ | 158/186 [28:00<05:06, 10.93s/it] 85%|████████▌ | 159/186 [28:11<04:53, 10.89s/it] 86%|████████▌ | 160/186 [28:22<04:42, 10.87s/it] 87%|████████▋ | 161/186 [28:33<04:33, 10.94s/it] 87%|████████▋ | 162/186 [28:44<04:22, 10.95s/it] 88%|████████▊ | 163/186 [28:55<04:10, 10.89s/it] 88%|████████▊ | 164/186 [29:06<04:00, 10.92s/it] 89%|████████▊ | 165/186 [29:16<03:49, 10.92s/it] 89%|████████▉ | 166/186 [29:28<03:39, 10.97s/it] 90%|████████▉ | 167/186 [29:38<03:27, 10.90s/it] 90%|█████████ | 168/186 [29:49<03:17, 10.98s/it] 91%|█████████ | 169/186 [30:00<03:05, 10.94s/it] 91%|█████████▏| 170/186 [30:11<02:53, 10.87s/it] 92%|█████████▏| 171/186 [30:22<02:43, 10.91s/it] 92%|█████████▏| 172/186 [30:33<02:32, 10.87s/it] 93%|█████████▎| 173/186 [30:44<02:21, 10.86s/it] 94%|█████████▎| 174/186 [30:54<02:09, 10.79s/it] 94%|█████████▍| 175/186 [31:05<01:58, 10.76s/it] 95%|█████████▍| 176/186 [31:16<01:47, 10.77s/it] 95%|█████████▌| 177/186 [31:27<01:37, 10.80s/it] 96%|█████████▌| 178/186 [31:38<01:27, 10.88s/it] 96%|█████████▌| 179/186 [31:49<01:16, 10.89s/it] 97%|█████████▋| 180/186 [31:59<01:05, 10.88s/it] 97%|█████████▋| 181/186 [32:10<00:54, 10.91s/it] 98%|█████████▊| 182/186 [32:21<00:43, 10.88s/it] 98%|█████████▊| 183/186 [32:32<00:32, 10.85s/it] 99%|█████████▉| 184/186 [32:43<00:21, 10.84s/it] 99%|█████████▉| 185/186 [32:54<00:11, 11.00s/it]100%|██████████| 186/186 [33:05<00:00, 10.91s/it]                                                 {'train_runtime': 1985.4403, 'train_samples_per_second': 6.767, 'train_steps_per_second': 0.094, 'train_loss': 0.11749891055527553, 'epoch': 1.0}
100%|██████████| 186/186 [33:05<00:00, 10.91s/it]100%|██████████| 186/186 [33:05<00:00, 10.67s/it]
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-3
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33msliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-3[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251017_131108-049i6t9c/logs[0m
[2025-10-17 13:45:59,137] [INFO] [launch.py:347:main] Process 2359393 exits successfully.
[2025-10-17 13:45:59,137] [INFO] [launch.py:347:main] Process 2359396 exits successfully.
[2025-10-17 13:46:00,139] [INFO] [launch.py:347:main] Process 2359394 exits successfully.
[2025-10-17 13:46:00,139] [INFO] [launch.py:347:main] Process 2359395 exits successfully.
[2025-10-17 13:46:00,139] [INFO] [launch.py:347:main] Process 2359392 exits successfully.
[2025-10-17 13:46:04,144] [INFO] [launch.py:347:main] Process 2359391 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 13:46:07,504] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 13:46:08,055] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-17 13:46:08,055] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12423 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1.5e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-3 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-3 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 2 --use_anchor False --use_sample_weight True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 13:46:09,752] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 13:46:10,353] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-17 13:46:10,353] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-17 13:46:10,353] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-17 13:46:10,353] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-17 13:46:10,353] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 13:46:14,216] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 13:46:14,555] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 13:46:14,714] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 13:46:14,763] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 13:46:15,039] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 13:46:15,093] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 13:46:15,471] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-17 13:46:15,785] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 13:46:15,816] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 13:46:15,816] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 13:46:16,156] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 13:46:16,209] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
[2025-10-17 13:46:16,672] [INFO] [comm.py:637:init_distributed] cdb=None
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251017_134616-mhr04vrr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-3
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/mhr04vrr
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.14s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.57s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.75s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.73s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.09s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.06s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.44s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.50s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.49s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.63s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.66s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.69s/it]to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.50s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.82s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.67s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.98s/it]
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.56s/it]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.59s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.88s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.51s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.97s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.73s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.13s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.82s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.16s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.60s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.95s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.60s/it]
Formatting inputs...Skip in lazy mode
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:2')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:1')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:4')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:3')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:0')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:5')
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.011616706848144531 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.009735107421875 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.010746240615844727 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10166072845458984 secondsLoading extension module fused_adam...

Time to load fused_adam op: 0.10123825073242188 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10117411613464355 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/186 [00:09<29:45,  9.65s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1073 > 1048). Running this sequence through the model will result in indexing errors
  1%|          | 2/186 [00:18<28:13,  9.21s/it]  2%|▏         | 3/186 [00:27<27:37,  9.06s/it]  2%|▏         | 4/186 [00:36<27:39,  9.12s/it]  3%|▎         | 5/186 [00:46<28:01,  9.29s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1160 > 1048). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1297 > 1048). Running this sequence through the model will result in indexing errors
  3%|▎         | 6/186 [00:56<28:47,  9.60s/it]  4%|▍         | 7/186 [01:06<28:51,  9.67s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1233 > 1048). Running this sequence through the model will result in indexing errors
  4%|▍         | 8/186 [01:16<29:10,  9.83s/it]  5%|▍         | 9/186 [01:26<29:10,  9.89s/it]  5%|▌         | 10/186 [01:36<29:19, 10.00s/it]  6%|▌         | 11/186 [01:46<29:23, 10.08s/it]  6%|▋         | 12/186 [01:56<29:10, 10.06s/it]  7%|▋         | 13/186 [02:07<29:03, 10.08s/it]  8%|▊         | 14/186 [02:17<29:00, 10.12s/it]  8%|▊         | 15/186 [02:27<28:51, 10.12s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
  9%|▊         | 16/186 [02:37<28:49, 10.17s/it]  9%|▉         | 17/186 [02:48<28:53, 10.26s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1072 > 1048). Running this sequence through the model will result in indexing errors
WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 10%|▉         | 18/186 [02:58<29:01, 10.36s/it] 10%|█         | 19/186 [03:09<28:50, 10.36s/it] 11%|█         | 20/186 [03:19<28:36, 10.34s/it] 11%|█▏        | 21/186 [03:29<28:34, 10.39s/it] 12%|█▏        | 22/186 [03:40<28:16, 10.34s/it] 12%|█▏        | 23/186 [03:50<28:05, 10.34s/it] 13%|█▎        | 24/186 [04:01<28:03, 10.39s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1297 > 1048). Running this sequence through the model will result in indexing errors
 13%|█▎        | 25/186 [04:11<27:57, 10.42s/it] 14%|█▍        | 26/186 [04:21<27:41, 10.38s/it] 15%|█▍        | 27/186 [04:32<27:52, 10.52s/it] 15%|█▌        | 28/186 [04:43<27:40, 10.51s/it] 16%|█▌        | 29/186 [04:53<27:26, 10.48s/it] 16%|█▌        | 30/186 [05:03<27:07, 10.43s/it] 17%|█▋        | 31/186 [05:14<27:05, 10.49s/it] 17%|█▋        | 32/186 [05:25<26:58, 10.51s/it] 18%|█▊        | 33/186 [05:35<26:52, 10.54s/it] 18%|█▊        | 34/186 [05:46<26:50, 10.59s/it] 19%|█▉        | 35/186 [05:56<26:40, 10.60s/it] 19%|█▉        | 36/186 [06:07<26:33, 10.62s/it] 20%|█▉        | 37/186 [06:18<26:21, 10.62s/it] 20%|██        | 38/186 [06:28<26:12, 10.63s/it] 21%|██        | 39/186 [06:39<26:04, 10.64s/it] 22%|██▏       | 40/186 [06:50<25:59, 10.68s/it] 22%|██▏       | 41/186 [07:00<25:44, 10.65s/it] 23%|██▎       | 42/186 [07:11<25:30, 10.63s/it] 23%|██▎       | 43/186 [07:22<25:31, 10.71s/it] 24%|██▎       | 44/186 [07:33<25:16, 10.68s/it] 24%|██▍       | 45/186 [07:43<25:07, 10.69s/it] 25%|██▍       | 46/186 [07:54<24:56, 10.69s/it] 25%|██▌       | 47/186 [08:05<24:55, 10.76s/it]WARNING: tokenization mismatch: 1 vs. 48. (ignored)
 26%|██▌       | 48/186 [08:15<24:38, 10.71s/it] 26%|██▋       | 49/186 [08:26<24:17, 10.64s/it] 27%|██▋       | 50/186 [08:36<24:04, 10.62s/it] 27%|██▋       | 51/186 [08:47<23:52, 10.61s/it] 28%|██▊       | 52/186 [08:58<23:49, 10.67s/it] 28%|██▊       | 53/186 [09:09<23:40, 10.68s/it] 29%|██▉       | 54/186 [09:19<23:36, 10.73s/it] 30%|██▉       | 55/186 [09:30<23:29, 10.76s/it] 30%|███       | 56/186 [09:41<23:07, 10.68s/it] 31%|███       | 57/186 [09:51<22:54, 10.65s/it] 31%|███       | 58/186 [10:02<22:44, 10.66s/it] 32%|███▏      | 59/186 [10:13<22:40, 10.71s/it] 32%|███▏      | 60/186 [10:23<22:21, 10.64s/it] 33%|███▎      | 61/186 [10:34<22:05, 10.61s/it] 33%|███▎      | 62/186 [10:45<22:14, 10.76s/it] 34%|███▍      | 63/186 [10:56<22:07, 10.79s/it] 34%|███▍      | 64/186 [11:06<21:45, 10.70s/it] 35%|███▍      | 65/186 [11:17<21:36, 10.71s/it] 35%|███▌      | 66/186 [11:28<21:42, 10.85s/it] 36%|███▌      | 67/186 [11:39<21:22, 10.78s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 37%|███▋      | 68/186 [11:50<21:09, 10.76s/it] 37%|███▋      | 69/186 [12:00<20:57, 10.75s/it] 38%|███▊      | 70/186 [12:11<21:00, 10.86s/it] 38%|███▊      | 71/186 [12:22<20:41, 10.80s/it] 39%|███▊      | 72/186 [12:33<20:22, 10.72s/it] 39%|███▉      | 73/186 [12:43<20:07, 10.68s/it] 40%|███▉      | 74/186 [12:54<19:58, 10.70s/it] 40%|████      | 75/186 [13:05<19:50, 10.72s/it] 41%|████      | 76/186 [13:15<19:38, 10.71s/it] 41%|████▏     | 77/186 [13:26<19:29, 10.73s/it] 42%|████▏     | 78/186 [13:37<19:21, 10.76s/it] 42%|████▏     | 79/186 [13:48<19:09, 10.74s/it] 43%|████▎     | 80/186 [13:59<19:04, 10.79s/it] 44%|████▎     | 81/186 [14:09<18:55, 10.81s/it] 44%|████▍     | 82/186 [14:20<18:39, 10.76s/it] 45%|████▍     | 83/186 [14:31<18:26, 10.75s/it] 45%|████▌     | 84/186 [14:42<18:13, 10.72s/it] 46%|████▌     | 85/186 [14:52<18:00, 10.70s/it] 46%|████▌     | 86/186 [15:03<17:50, 10.71s/it] 47%|████▋     | 87/186 [15:14<17:41, 10.72s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
 47%|████▋     | 88/186 [15:24<17:28, 10.70s/it] 48%|████▊     | 89/186 [15:35<17:21, 10.74s/it] 48%|████▊     | 90/186 [15:46<17:08, 10.71s/it] 49%|████▉     | 91/186 [15:57<16:59, 10.73s/it] 49%|████▉     | 92/186 [16:08<16:57, 10.83s/it] 50%|█████     | 93/186 [16:19<16:59, 10.97s/it]WARNING: tokenization mismatch: 1 vs. 47. (ignored)
 51%|█████     | 94/186 [16:30<16:49, 10.98s/it] 51%|█████     | 95/186 [16:41<16:47, 11.08s/it] 52%|█████▏    | 96/186 [16:52<16:35, 11.07s/it] 52%|█████▏    | 97/186 [17:03<16:25, 11.07s/it] 53%|█████▎    | 98/186 [17:14<16:07, 11.00s/it] 53%|█████▎    | 99/186 [17:25<15:58, 11.02s/it] 54%|█████▍    | 100/186 [17:36<15:51, 11.06s/it] 54%|█████▍    | 101/186 [17:47<15:39, 11.06s/it] 55%|█████▍    | 102/186 [17:58<15:23, 10.99s/it] 55%|█████▌    | 103/186 [18:09<15:18, 11.06s/it] 56%|█████▌    | 104/186 [18:21<15:10, 11.11s/it] 56%|█████▋    | 105/186 [18:32<14:57, 11.08s/it] 57%|█████▋    | 106/186 [18:43<14:51, 11.14s/it] 58%|█████▊    | 107/186 [18:54<14:34, 11.06s/it] 58%|█████▊    | 108/186 [19:05<14:25, 11.09s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 59%|█████▊    | 109/186 [19:16<14:06, 11.00s/it] 59%|█████▉    | 110/186 [19:27<13:56, 11.00s/it] 60%|█████▉    | 111/186 [19:38<13:39, 10.93s/it] 60%|██████    | 112/186 [19:48<13:28, 10.92s/it] 61%|██████    | 113/186 [20:00<13:19, 10.95s/it] 61%|██████▏   | 114/186 [20:11<13:11, 11.00s/it] 62%|██████▏   | 115/186 [20:21<12:57, 10.95s/it] 62%|██████▏   | 116/186 [20:33<12:52, 11.04s/it] 63%|██████▎   | 117/186 [20:44<12:38, 10.99s/it] 63%|██████▎   | 118/186 [20:55<12:31, 11.05s/it] 64%|██████▍   | 119/186 [21:06<12:16, 10.99s/it] 65%|██████▍   | 120/186 [21:17<12:04, 10.98s/it] 65%|██████▌   | 121/186 [21:28<11:56, 11.03s/it] 66%|██████▌   | 122/186 [21:39<11:51, 11.11s/it] 66%|██████▌   | 123/186 [21:50<11:41, 11.14s/it] 67%|██████▋   | 124/186 [22:01<11:32, 11.18s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 67%|██████▋   | 125/186 [22:13<11:20, 11.16s/it] 68%|██████▊   | 126/186 [22:23<11:02, 11.04s/it] 68%|██████▊   | 127/186 [22:34<10:48, 11.00s/it] 69%|██████▉   | 128/186 [22:45<10:35, 10.96s/it] 69%|██████▉   | 129/186 [22:56<10:29, 11.04s/it] 70%|██████▉   | 130/186 [23:07<10:17, 11.03s/it] 70%|███████   | 131/186 [23:18<10:05, 11.00s/it] 71%|███████   | 132/186 [23:29<09:55, 11.02s/it] 72%|███████▏  | 133/186 [23:41<09:45, 11.05s/it] 72%|███████▏  | 134/186 [23:51<09:31, 11.00s/it] 73%|███████▎  | 135/186 [24:02<09:18, 10.96s/it] 73%|███████▎  | 136/186 [24:13<09:10, 11.01s/it] 74%|███████▎  | 137/186 [24:25<09:05, 11.14s/it] 74%|███████▍  | 138/186 [24:36<08:50, 11.05s/it] 75%|███████▍  | 139/186 [24:47<08:45, 11.18s/it] 75%|███████▌  | 140/186 [24:58<08:31, 11.11s/it] 76%|███████▌  | 141/186 [25:09<08:16, 11.03s/it] 76%|███████▋  | 142/186 [25:20<08:06, 11.05s/it] 77%|███████▋  | 143/186 [25:31<07:55, 11.05s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 77%|███████▋  | 144/186 [25:42<07:43, 11.03s/it] 78%|███████▊  | 145/186 [25:53<07:29, 10.97s/it] 78%|███████▊  | 146/186 [26:04<07:19, 10.99s/it] 79%|███████▉  | 147/186 [26:15<07:07, 10.97s/it] 80%|███████▉  | 148/186 [26:26<07:00, 11.05s/it] 80%|████████  | 149/186 [26:37<06:49, 11.07s/it] 81%|████████  | 150/186 [26:49<06:41, 11.15s/it] 81%|████████  | 151/186 [27:00<06:31, 11.19s/it] 82%|████████▏ | 152/186 [27:11<06:21, 11.23s/it] 82%|████████▏ | 153/186 [27:23<06:12, 11.28s/it] 83%|████████▎ | 154/186 [27:33<05:56, 11.15s/it] 83%|████████▎ | 155/186 [27:45<05:46, 11.18s/it] 84%|████████▍ | 156/186 [27:56<05:39, 11.31s/it] 84%|████████▍ | 157/186 [28:07<05:27, 11.28s/it] 85%|████████▍ | 158/186 [28:18<05:12, 11.15s/it] 85%|████████▌ | 159/186 [28:29<05:00, 11.12s/it] 86%|████████▌ | 160/186 [28:40<04:47, 11.08s/it] 87%|████████▋ | 161/186 [28:52<04:37, 11.12s/it] 87%|████████▋ | 162/186 [29:03<04:27, 11.13s/it] 88%|████████▊ | 163/186 [29:13<04:13, 11.02s/it] 88%|████████▊ | 164/186 [29:24<04:01, 10.97s/it] 89%|████████▊ | 165/186 [29:35<03:50, 10.97s/it] 89%|████████▉ | 166/186 [29:46<03:39, 10.96s/it] 90%|████████▉ | 167/186 [29:57<03:27, 10.94s/it] 90%|█████████ | 168/186 [30:09<03:19, 11.07s/it] 91%|█████████ | 169/186 [30:19<03:07, 11.01s/it] 91%|█████████▏| 170/186 [30:31<02:56, 11.04s/it] 92%|█████████▏| 171/186 [30:42<02:46, 11.11s/it] 92%|█████████▏| 172/186 [30:53<02:35, 11.08s/it] 93%|█████████▎| 173/186 [31:04<02:23, 11.06s/it] 94%|█████████▎| 174/186 [31:15<02:12, 11.02s/it] 94%|█████████▍| 175/186 [31:26<02:01, 11.05s/it] 95%|█████████▍| 176/186 [31:37<01:50, 11.06s/it] 95%|█████████▌| 177/186 [31:48<01:39, 11.04s/it] 96%|█████████▌| 178/186 [31:59<01:28, 11.07s/it] 96%|█████████▌| 179/186 [32:10<01:17, 11.03s/it] 97%|█████████▋| 180/186 [32:21<01:06, 11.03s/it] 97%|█████████▋| 181/186 [32:32<00:55, 11.05s/it] 98%|█████████▊| 182/186 [32:43<00:44, 11.04s/it] 98%|█████████▊| 183/186 [32:54<00:33, 11.05s/it] 99%|█████████▉| 184/186 [33:05<00:22, 11.06s/it] 99%|█████████▉| 185/186 [33:17<00:11, 11.14s/it]100%|██████████| 186/186 [33:28<00:00, 11.07s/it]                                                 {'train_runtime': 2008.0315, 'train_samples_per_second': 6.691, 'train_steps_per_second': 0.093, 'train_loss': 0.14995342172602172, 'epoch': 1.0}
100%|██████████| 186/186 [33:28<00:00, 11.07s/it]100%|██████████| 186/186 [33:28<00:00, 10.80s/it]
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-3
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33msliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-3[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251017_134616-mhr04vrr/logs[0m
[2025-10-17 14:21:28,830] [INFO] [launch.py:347:main] Process 2375182 exits successfully.
[2025-10-17 14:21:29,831] [INFO] [launch.py:347:main] Process 2375184 exits successfully.
[2025-10-17 14:21:29,832] [INFO] [launch.py:347:main] Process 2375183 exits successfully.
[2025-10-17 14:21:29,832] [INFO] [launch.py:347:main] Process 2375181 exits successfully.
[2025-10-17 14:21:29,832] [INFO] [launch.py:347:main] Process 2375185 exits successfully.
[2025-10-17 14:21:34,838] [INFO] [launch.py:347:main] Process 2375180 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 14:21:38,001] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 14:21:38,555] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-17 14:21:38,555] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12423 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-4 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-4 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 0.5 --use_anchor False --use_sample_weight True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 14:21:40,252] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 14:21:40,857] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-17 14:21:40,858] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-17 14:21:40,858] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-17 14:21:40,858] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-17 14:21:40,858] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 14:21:44,549] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 14:21:44,550] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-17 14:21:44,707] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 14:21:44,736] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-17 14:21:44,897] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 14:21:44,899] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 14:21:44,919] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 14:21:45,011] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 14:21:45,051] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 14:21:45,251] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 14:21:45,251] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-10-17 14:21:45,730] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 14:21:46,135] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251017_142146-1s0uvv3k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-4
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/1s0uvv3k
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.61s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.58s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.06s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.44s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.07s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.44s/it]
to bfloat16...
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.02s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.73s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.61s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.61s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.92s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.57s/it]
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.75s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.71s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.12s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.36s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.13s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.41s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.96s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.39s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.75s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.71s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.07s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.86s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.53s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.81s/it]
Formatting inputs...Skip in lazy mode
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:5')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:4')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:3')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:2')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:1')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:0')
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)

Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.011148214340209961 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.012307405471801758 seconds
Loading extension module fused_adam...
Loading extension module fused_adam...
Time to load fused_adam op: 0.10126852989196777 secondsTime to load fused_adam op: 0.10152912139892578 seconds

Loading extension module fused_adam...Loading extension module fused_adam...

Time to load fused_adam op: 0.10136055946350098 seconds
Time to load fused_adam op: 0.10324621200561523 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
WARNING: tokenization mismatch: 1 vs. 49. (ignored)
  1%|          | 1/186 [00:09<29:57,  9.72s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1221 > 1048). Running this sequence through the model will result in indexing errors
  1%|          | 2/186 [00:19<29:29,  9.62s/it]  2%|▏         | 3/186 [00:29<29:32,  9.69s/it]  2%|▏         | 4/186 [00:39<29:43,  9.80s/it]  3%|▎         | 5/186 [00:49<30:19, 10.06s/it]  3%|▎         | 6/186 [00:59<30:22, 10.13s/it]  4%|▍         | 7/186 [01:09<30:08, 10.11s/it]  4%|▍         | 8/186 [01:20<30:08, 10.16s/it]  5%|▍         | 9/186 [01:30<30:03, 10.19s/it]  5%|▌         | 10/186 [01:40<29:57, 10.21s/it]  6%|▌         | 11/186 [01:50<29:53, 10.25s/it]  6%|▋         | 12/186 [02:01<29:53, 10.31s/it]  7%|▋         | 13/186 [02:11<29:52, 10.36s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
  8%|▊         | 14/186 [02:22<29:36, 10.33s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1245 > 1048). Running this sequence through the model will result in indexing errors
  8%|▊         | 15/186 [02:32<29:38, 10.40s/it]  9%|▊         | 16/186 [02:43<29:42, 10.49s/it]  9%|▉         | 17/186 [02:54<29:39, 10.53s/it] 10%|▉         | 18/186 [03:04<29:34, 10.56s/it] 10%|█         | 19/186 [03:15<29:23, 10.56s/it] 11%|█         | 20/186 [03:25<29:10, 10.55s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 11%|█▏        | 21/186 [03:36<29:06, 10.59s/it] 12%|█▏        | 22/186 [03:47<29:05, 10.65s/it] 12%|█▏        | 23/186 [03:57<28:56, 10.65s/it] 13%|█▎        | 24/186 [04:08<28:54, 10.71s/it] 13%|█▎        | 25/186 [04:19<28:54, 10.78s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 14%|█▍        | 26/186 [04:30<28:33, 10.71s/it] 15%|█▍        | 27/186 [04:41<28:27, 10.74s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1065 > 1048). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1165 > 1048). Running this sequence through the model will result in indexing errors
 15%|█▌        | 28/186 [04:52<28:34, 10.85s/it] 16%|█▌        | 29/186 [05:03<28:30, 10.90s/it] 16%|█▌        | 30/186 [05:13<28:02, 10.79s/it] 17%|█▋        | 31/186 [05:24<27:51, 10.78s/it]WARNING: tokenization mismatch: 1 vs. 48. (ignored)
 17%|█▋        | 32/186 [05:35<27:31, 10.72s/it] 18%|█▊        | 33/186 [05:45<27:28, 10.78s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1075 > 1048). Running this sequence through the model will result in indexing errors
 18%|█▊        | 34/186 [05:56<27:25, 10.82s/it] 19%|█▉        | 35/186 [06:07<27:20, 10.87s/it] 19%|█▉        | 36/186 [06:18<27:07, 10.85s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1263 > 1048). Running this sequence through the model will result in indexing errors
 20%|█▉        | 37/186 [06:29<27:00, 10.88s/it] 20%|██        | 38/186 [06:40<27:01, 10.96s/it] 21%|██        | 39/186 [06:51<26:46, 10.93s/it] 22%|██▏       | 40/186 [07:02<26:24, 10.85s/it] 22%|██▏       | 41/186 [07:12<26:01, 10.77s/it] 23%|██▎       | 42/186 [07:23<25:46, 10.74s/it] 23%|██▎       | 43/186 [07:34<25:41, 10.78s/it] 24%|██▎       | 44/186 [07:44<25:23, 10.73s/it] 24%|██▍       | 45/186 [07:55<25:05, 10.68s/it] 25%|██▍       | 46/186 [08:06<24:56, 10.69s/it] 25%|██▌       | 47/186 [08:17<25:04, 10.82s/it] 26%|██▌       | 48/186 [08:28<25:06, 10.92s/it] 26%|██▋       | 49/186 [08:39<24:50, 10.88s/it] 27%|██▋       | 50/186 [08:50<24:36, 10.85s/it] 27%|██▋       | 51/186 [09:00<24:24, 10.85s/it] 28%|██▊       | 52/186 [09:11<23:59, 10.74s/it] 28%|██▊       | 53/186 [09:22<24:01, 10.84s/it] 29%|██▉       | 54/186 [09:33<23:52, 10.85s/it] 30%|██▉       | 55/186 [09:44<23:42, 10.86s/it] 30%|███       | 56/186 [09:54<23:26, 10.82s/it] 31%|███       | 57/186 [10:05<23:09, 10.77s/it] 31%|███       | 58/186 [10:16<23:02, 10.80s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 32%|███▏      | 59/186 [10:28<23:24, 11.06s/it] 32%|███▏      | 60/186 [10:38<23:03, 10.98s/it] 33%|███▎      | 61/186 [10:50<23:01, 11.05s/it] 33%|███▎      | 62/186 [11:01<22:46, 11.02s/it] 34%|███▍      | 63/186 [11:11<22:25, 10.94s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 34%|███▍      | 64/186 [11:22<22:08, 10.89s/it] 35%|███▍      | 65/186 [11:33<22:06, 10.96s/it] 35%|███▌      | 66/186 [11:44<22:01, 11.01s/it] 36%|███▌      | 67/186 [11:55<21:45, 10.97s/it] 37%|███▋      | 68/186 [12:06<21:38, 11.01s/it] 37%|███▋      | 69/186 [12:18<21:40, 11.12s/it] 38%|███▊      | 70/186 [12:29<21:24, 11.07s/it] 38%|███▊      | 71/186 [12:40<21:15, 11.09s/it] 39%|███▊      | 72/186 [12:51<21:00, 11.05s/it] 39%|███▉      | 73/186 [13:02<20:47, 11.04s/it] 40%|███▉      | 74/186 [13:13<20:37, 11.05s/it] 40%|████      | 75/186 [13:24<20:18, 10.97s/it] 41%|████      | 76/186 [13:35<20:05, 10.96s/it] 41%|████▏     | 77/186 [13:46<19:52, 10.94s/it] 42%|████▏     | 78/186 [13:56<19:39, 10.92s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 42%|████▏     | 79/186 [14:07<19:22, 10.86s/it] 43%|████▎     | 80/186 [14:18<19:14, 10.90s/it] 44%|████▎     | 81/186 [14:29<19:04, 10.90s/it] 44%|████▍     | 82/186 [14:40<18:49, 10.86s/it] 45%|████▍     | 83/186 [14:51<18:36, 10.84s/it] 45%|████▌     | 84/186 [15:01<18:22, 10.81s/it] 46%|████▌     | 85/186 [15:12<18:12, 10.82s/it] 46%|████▌     | 86/186 [15:23<18:06, 10.86s/it] 47%|████▋     | 87/186 [15:34<18:00, 10.91s/it] 47%|████▋     | 88/186 [15:45<17:48, 10.91s/it] 48%|████▊     | 89/186 [15:56<17:38, 10.91s/it] 48%|████▊     | 90/186 [16:06<17:15, 10.79s/it] 49%|████▉     | 91/186 [16:18<17:15, 10.90s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 49%|████▉     | 92/186 [16:29<17:08, 10.95s/it] 50%|█████     | 93/186 [16:40<17:03, 11.00s/it] 51%|█████     | 94/186 [16:51<16:48, 10.96s/it] 51%|█████     | 95/186 [17:02<16:38, 10.98s/it] 52%|█████▏    | 96/186 [17:12<16:23, 10.92s/it] 52%|█████▏    | 97/186 [17:23<16:14, 10.95s/it] 53%|█████▎    | 98/186 [17:34<16:01, 10.92s/it] 53%|█████▎    | 99/186 [17:45<15:48, 10.90s/it] 54%|█████▍    | 100/186 [17:57<15:49, 11.05s/it] 54%|█████▍    | 101/186 [18:08<15:37, 11.03s/it] 55%|█████▍    | 102/186 [18:19<15:31, 11.09s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 55%|█████▌    | 103/186 [18:30<15:31, 11.22s/it] 56%|█████▌    | 104/186 [18:41<15:15, 11.17s/it] 56%|█████▋    | 105/186 [18:52<15:00, 11.12s/it] 57%|█████▋    | 106/186 [19:03<14:46, 11.08s/it] 58%|█████▊    | 107/186 [19:14<14:28, 11.00s/it] 58%|█████▊    | 108/186 [19:25<14:22, 11.06s/it] 59%|█████▊    | 109/186 [19:36<14:07, 11.01s/it] 59%|█████▉    | 110/186 [19:47<13:52, 10.95s/it] 60%|█████▉    | 111/186 [19:58<13:51, 11.08s/it] 60%|██████    | 112/186 [20:09<13:36, 11.03s/it] 61%|██████    | 113/186 [20:20<13:19, 10.95s/it] 61%|██████▏   | 114/186 [20:31<13:10, 10.99s/it] 62%|██████▏   | 115/186 [20:42<13:03, 11.03s/it] 62%|██████▏   | 116/186 [20:53<12:53, 11.05s/it] 63%|██████▎   | 117/186 [21:04<12:39, 11.01s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 63%|██████▎   | 118/186 [21:15<12:22, 10.92s/it] 64%|██████▍   | 119/186 [21:26<12:11, 10.92s/it] 65%|██████▍   | 120/186 [21:37<12:02, 10.94s/it] 65%|██████▌   | 121/186 [21:48<11:51, 10.94s/it] 66%|██████▌   | 122/186 [21:59<11:45, 11.02s/it] 66%|██████▌   | 123/186 [22:10<11:38, 11.09s/it] 67%|██████▋   | 124/186 [22:21<11:26, 11.07s/it] 67%|██████▋   | 125/186 [22:32<11:14, 11.06s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 68%|██████▊   | 126/186 [22:43<10:59, 11.00s/it] 68%|██████▊   | 127/186 [22:54<10:50, 11.02s/it] 69%|██████▉   | 128/186 [23:06<10:45, 11.13s/it] 69%|██████▉   | 129/186 [23:17<10:33, 11.11s/it] 70%|██████▉   | 130/186 [23:28<10:21, 11.09s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 70%|███████   | 131/186 [23:39<10:10, 11.10s/it] 71%|███████   | 132/186 [23:51<10:06, 11.23s/it] 72%|███████▏  | 133/186 [24:02<09:51, 11.17s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 72%|███████▏  | 134/186 [24:13<09:39, 11.14s/it] 73%|███████▎  | 135/186 [24:24<09:26, 11.11s/it] 73%|███████▎  | 136/186 [24:35<09:16, 11.14s/it] 74%|███████▎  | 137/186 [24:46<09:06, 11.16s/it] 74%|███████▍  | 138/186 [24:58<09:02, 11.30s/it] 75%|███████▍  | 139/186 [25:09<08:52, 11.34s/it] 75%|███████▌  | 140/186 [25:20<08:40, 11.32s/it] 76%|███████▌  | 141/186 [25:32<08:33, 11.40s/it] 76%|███████▋  | 142/186 [25:43<08:17, 11.30s/it] 77%|███████▋  | 143/186 [25:54<08:01, 11.19s/it] 77%|███████▋  | 144/186 [26:05<07:48, 11.16s/it] 78%|███████▊  | 145/186 [26:16<07:37, 11.17s/it] 78%|███████▊  | 146/186 [26:27<07:23, 11.09s/it] 79%|███████▉  | 147/186 [26:38<07:10, 11.05s/it] 80%|███████▉  | 148/186 [26:49<07:02, 11.11s/it] 80%|████████  | 149/186 [27:01<06:52, 11.16s/it] 81%|████████  | 150/186 [27:12<06:39, 11.11s/it] 81%|████████  | 151/186 [27:23<06:28, 11.10s/it] 82%|████████▏ | 152/186 [27:34<06:16, 11.07s/it] 82%|████████▏ | 153/186 [27:45<06:03, 11.03s/it] 83%|████████▎ | 154/186 [27:56<05:53, 11.06s/it] 83%|████████▎ | 155/186 [28:07<05:44, 11.11s/it] 84%|████████▍ | 156/186 [28:19<05:37, 11.25s/it] 84%|████████▍ | 157/186 [28:30<05:23, 11.14s/it] 85%|████████▍ | 158/186 [28:41<05:12, 11.17s/it] 85%|████████▌ | 159/186 [28:52<04:59, 11.10s/it] 86%|████████▌ | 160/186 [29:03<04:51, 11.22s/it] 87%|████████▋ | 161/186 [29:14<04:39, 11.19s/it] 87%|████████▋ | 162/186 [29:26<04:30, 11.29s/it] 88%|████████▊ | 163/186 [29:37<04:19, 11.27s/it] 88%|████████▊ | 164/186 [29:48<04:07, 11.26s/it] 89%|████████▊ | 165/186 [30:00<03:56, 11.27s/it] 89%|████████▉ | 166/186 [30:11<03:43, 11.18s/it] 90%|████████▉ | 167/186 [30:22<03:31, 11.15s/it] 90%|█████████ | 168/186 [30:33<03:19, 11.09s/it] 91%|█████████ | 169/186 [30:43<03:07, 11.01s/it] 91%|█████████▏| 170/186 [30:54<02:55, 11.00s/it] 92%|█████████▏| 171/186 [31:06<02:46, 11.12s/it] 92%|█████████▏| 172/186 [31:17<02:36, 11.16s/it] 93%|█████████▎| 173/186 [31:28<02:23, 11.06s/it] 94%|█████████▎| 174/186 [31:39<02:12, 11.04s/it] 94%|█████████▍| 175/186 [31:50<02:01, 11.06s/it] 95%|█████████▍| 176/186 [32:01<01:50, 11.09s/it] 95%|█████████▌| 177/186 [32:12<01:39, 11.06s/it] 96%|█████████▌| 178/186 [32:23<01:29, 11.13s/it] 96%|█████████▌| 179/186 [32:34<01:17, 11.10s/it]WARNING: tokenization mismatch: 1 vs. 48. (ignored)
 97%|█████████▋| 180/186 [32:46<01:06, 11.10s/it] 97%|█████████▋| 181/186 [32:57<00:55, 11.18s/it] 98%|█████████▊| 182/186 [33:08<00:44, 11.12s/it] 98%|█████████▊| 183/186 [33:19<00:33, 11.15s/it] 99%|█████████▉| 184/186 [33:30<00:22, 11.12s/it] 99%|█████████▉| 185/186 [33:42<00:11, 11.22s/it]100%|██████████| 186/186 [33:53<00:00, 11.29s/it]                                                 {'train_runtime': 2033.5876, 'train_samples_per_second': 6.607, 'train_steps_per_second': 0.091, 'train_loss': 0.15830619360810966, 'epoch': 1.0}
100%|██████████| 186/186 [33:53<00:00, 11.29s/it]100%|██████████| 186/186 [33:53<00:00, 10.93s/it]
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-4
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33msliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-4[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251017_142146-1s0uvv3k/logs[0m
[2025-10-17 14:57:29,359] [INFO] [launch.py:347:main] Process 2392208 exits successfully.
[2025-10-17 14:57:29,360] [INFO] [launch.py:347:main] Process 2392207 exits successfully.
[2025-10-17 14:57:29,360] [INFO] [launch.py:347:main] Process 2392209 exits successfully.
[2025-10-17 14:57:29,360] [INFO] [launch.py:347:main] Process 2392206 exits successfully.
[2025-10-17 14:57:29,360] [INFO] [launch.py:347:main] Process 2392210 exits successfully.
[2025-10-17 14:57:35,367] [INFO] [launch.py:347:main] Process 2392205 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 14:57:38,789] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 14:57:39,342] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-17 14:57:39,342] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12423 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-4 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-4 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 1 --use_anchor False --use_sample_weight True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 14:57:41,033] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 14:57:41,635] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-17 14:57:41,635] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-17 14:57:41,635] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-17 14:57:41,635] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-17 14:57:41,635] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 14:57:45,848] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 14:57:45,975] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-17 14:57:46,046] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 14:57:46,182] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 14:57:46,293] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 14:57:46,374] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 14:57:47,018] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 14:57:47,070] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 14:57:47,358] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 14:57:47,395] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 14:57:47,395] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-10-17 14:57:47,546] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 14:57:48,007] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251017_145748-iqjaq6ch
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-4
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/iqjaq6ch
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.91s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.18s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.44s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.17s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.21s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.50s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.33s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.13s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.20s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.84s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.21s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.82s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.16s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.20s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.61s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.92s/it]
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.68s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.06s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.30s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.85s/it]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.23s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.48s/it]
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:1')
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.94s/it][MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:3')
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.35s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.04s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.27s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.76s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.80s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.28s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.73s/it]
Formatting inputs...Skip in lazy mode
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:4')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:0')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:5')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:2')
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.011886835098266602 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.009621143341064453 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10126733779907227 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10241556167602539 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.013102054595947266 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10108089447021484 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/186 [00:09<29:58,  9.72s/it]  1%|          | 2/186 [00:18<27:47,  9.06s/it]  2%|▏         | 3/186 [00:27<27:58,  9.17s/it]  2%|▏         | 4/186 [00:36<27:50,  9.18s/it]  3%|▎         | 5/186 [00:46<28:19,  9.39s/it]  3%|▎         | 6/186 [00:56<28:46,  9.59s/it]  4%|▍         | 7/186 [01:06<28:48,  9.66s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1089 > 1048). Running this sequence through the model will result in indexing errors
  4%|▍         | 8/186 [01:16<29:06,  9.81s/it]  5%|▍         | 9/186 [01:26<29:04,  9.86s/it]  5%|▌         | 10/186 [01:36<29:13,  9.96s/it]  6%|▌         | 11/186 [01:47<29:34, 10.14s/it]  6%|▋         | 12/186 [01:57<29:58, 10.33s/it]  7%|▋         | 13/186 [02:08<29:51, 10.36s/it]  8%|▊         | 14/186 [02:18<29:43, 10.37s/it]  8%|▊         | 15/186 [02:29<29:59, 10.53s/it]  9%|▊         | 16/186 [02:40<29:55, 10.56s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1069 > 1048). Running this sequence through the model will result in indexing errors
  9%|▉         | 17/186 [02:51<30:03, 10.67s/it] 10%|▉         | 18/186 [03:01<29:32, 10.55s/it] 10%|█         | 19/186 [03:12<29:52, 10.74s/it] 11%|█         | 20/186 [03:23<29:52, 10.80s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1125 > 1048). Running this sequence through the model will result in indexing errors
 11%|█▏        | 21/186 [03:34<29:43, 10.81s/it] 12%|█▏        | 22/186 [03:49<32:41, 11.96s/it] 12%|█▏        | 23/186 [04:05<35:50, 13.19s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
 13%|█▎        | 24/186 [04:21<38:00, 14.08s/it] 13%|█▎        | 25/186 [04:37<39:28, 14.71s/it] 14%|█▍        | 26/186 [04:53<40:06, 15.04s/it] 15%|█▍        | 27/186 [05:09<40:43, 15.37s/it] 15%|█▌        | 28/186 [05:25<40:52, 15.52s/it] 16%|█▌        | 29/186 [05:41<40:53, 15.63s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
WARNING: tokenization mismatch: 1 vs. 47. (ignored)
 16%|█▌        | 30/186 [05:56<40:44, 15.67s/it] 17%|█▋        | 31/186 [06:13<40:51, 15.82s/it] 17%|█▋        | 32/186 [06:29<41:04, 16.00s/it] 18%|█▊        | 33/186 [06:45<41:02, 16.10s/it] 18%|█▊        | 34/186 [07:02<41:01, 16.19s/it] 19%|█▉        | 35/186 [07:19<41:12, 16.38s/it] 19%|█▉        | 36/186 [07:35<41:00, 16.40s/it] 20%|█▉        | 37/186 [07:51<40:43, 16.40s/it] 20%|██        | 38/186 [08:08<40:26, 16.40s/it] 21%|██        | 39/186 [08:24<39:53, 16.28s/it] 22%|██▏       | 40/186 [08:40<39:18, 16.16s/it] 22%|██▏       | 41/186 [08:56<39:13, 16.23s/it] 23%|██▎       | 42/186 [09:12<38:56, 16.23s/it] 23%|██▎       | 43/186 [09:29<38:44, 16.26s/it] 24%|██▎       | 44/186 [09:45<38:26, 16.25s/it] 24%|██▍       | 45/186 [09:59<36:36, 15.58s/it] 25%|██▍       | 46/186 [10:10<33:16, 14.26s/it] 25%|██▌       | 47/186 [10:21<30:30, 13.17s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 26%|██▌       | 48/186 [10:31<28:30, 12.40s/it]WARNING: tokenization mismatch: 1 vs. 47. (ignored)
 26%|██▋       | 49/186 [10:42<27:09, 11.89s/it] 27%|██▋       | 50/186 [10:53<26:02, 11.49s/it] 27%|██▋       | 51/186 [11:04<25:33, 11.36s/it] 28%|██▊       | 52/186 [11:19<27:52, 12.48s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1159 > 1048). Running this sequence through the model will result in indexing errors
 28%|██▊       | 53/186 [11:30<26:56, 12.16s/it] 29%|██▉       | 54/186 [11:41<25:43, 11.69s/it] 30%|██▉       | 55/186 [11:52<25:06, 11.50s/it] 30%|███       | 56/186 [12:03<24:32, 11.33s/it] 31%|███       | 57/186 [12:13<23:57, 11.14s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
 31%|███       | 58/186 [12:28<25:51, 12.12s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1233 > 1048). Running this sequence through the model will result in indexing errors
 32%|███▏      | 59/186 [12:45<28:56, 13.68s/it] 32%|███▏      | 60/186 [13:02<30:31, 14.54s/it] 33%|███▎      | 61/186 [13:18<31:20, 15.05s/it] 33%|███▎      | 62/186 [13:34<31:46, 15.38s/it] 34%|███▍      | 63/186 [13:50<31:59, 15.61s/it] 34%|███▍      | 64/186 [14:06<32:06, 15.79s/it] 35%|███▍      | 65/186 [14:22<31:48, 15.77s/it] 35%|███▌      | 66/186 [14:39<31:54, 15.95s/it] 36%|███▌      | 67/186 [14:55<31:53, 16.08s/it] 37%|███▋      | 68/186 [15:11<31:40, 16.11s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 37%|███▋      | 69/186 [15:27<31:33, 16.19s/it] 38%|███▊      | 70/186 [15:44<31:28, 16.28s/it] 38%|███▊      | 71/186 [16:00<31:11, 16.28s/it] 39%|███▊      | 72/186 [16:16<30:47, 16.21s/it] 39%|███▉      | 73/186 [16:33<30:38, 16.27s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 40%|███▉      | 74/186 [16:49<30:22, 16.27s/it] 40%|████      | 75/186 [17:05<29:55, 16.18s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1159 > 1048). Running this sequence through the model will result in indexing errors
 41%|████      | 76/186 [17:22<30:04, 16.40s/it] 41%|████▏     | 77/186 [17:38<29:38, 16.31s/it] 42%|████▏     | 78/186 [17:54<29:09, 16.20s/it] 42%|████▏     | 79/186 [18:10<28:59, 16.26s/it] 43%|████▎     | 80/186 [18:28<29:25, 16.66s/it] 44%|████▎     | 81/186 [18:42<28:04, 16.04s/it] 44%|████▍     | 82/186 [18:54<25:20, 14.62s/it] 45%|████▍     | 83/186 [19:04<22:57, 13.37s/it] 45%|████▌     | 84/186 [19:15<21:30, 12.65s/it] 46%|████▌     | 85/186 [19:26<20:26, 12.14s/it] 46%|████▌     | 86/186 [19:37<19:28, 11.68s/it] 47%|████▋     | 87/186 [19:48<19:02, 11.54s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 47%|████▋     | 88/186 [20:04<21:02, 12.88s/it] 48%|████▊     | 89/186 [20:20<22:26, 13.88s/it] 48%|████▊     | 90/186 [20:36<23:13, 14.52s/it] 49%|████▉     | 91/186 [20:53<23:53, 15.09s/it] 49%|████▉     | 92/186 [21:09<24:05, 15.38s/it] 50%|█████     | 93/186 [21:25<24:12, 15.62s/it] 51%|█████     | 94/186 [21:41<24:02, 15.68s/it] 51%|█████     | 95/186 [21:56<23:49, 15.71s/it] 52%|█████▏    | 96/186 [22:12<23:43, 15.81s/it] 52%|█████▏    | 97/186 [22:28<23:30, 15.85s/it] 53%|█████▎    | 98/186 [22:45<23:24, 15.96s/it] 53%|█████▎    | 99/186 [23:01<23:14, 16.03s/it] 54%|█████▍    | 100/186 [23:17<23:03, 16.08s/it] 54%|█████▍    | 101/186 [23:33<22:50, 16.13s/it] 55%|█████▍    | 102/186 [23:50<22:37, 16.16s/it] 55%|█████▌    | 103/186 [24:06<22:32, 16.30s/it] 56%|█████▌    | 104/186 [24:23<22:19, 16.33s/it] 56%|█████▋    | 105/186 [24:39<21:58, 16.28s/it] 57%|█████▋    | 106/186 [24:55<21:42, 16.29s/it] 58%|█████▊    | 107/186 [25:11<21:30, 16.33s/it] 58%|█████▊    | 108/186 [25:28<21:10, 16.29s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 59%|█████▊    | 109/186 [25:44<20:56, 16.31s/it] 59%|█████▉    | 110/186 [26:01<20:53, 16.49s/it] 60%|█████▉    | 111/186 [26:12<18:39, 14.92s/it] 60%|██████    | 112/186 [26:23<16:49, 13.64s/it] 61%|██████    | 113/186 [26:34<15:34, 12.80s/it] 61%|██████▏   | 114/186 [26:45<14:40, 12.24s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 62%|██████▏   | 115/186 [26:55<13:55, 11.77s/it] 62%|██████▏   | 116/186 [27:06<13:18, 11.41s/it] 63%|██████▎   | 117/186 [27:20<14:09, 12.32s/it] 63%|██████▎   | 118/186 [27:31<13:18, 11.74s/it] 64%|██████▍   | 119/186 [27:41<12:36, 11.29s/it] 65%|██████▍   | 120/186 [27:51<12:06, 11.01s/it] 65%|██████▌   | 121/186 [28:03<12:05, 11.16s/it] 66%|██████▌   | 122/186 [28:14<12:01, 11.27s/it] 66%|██████▌   | 123/186 [28:25<11:42, 11.15s/it] 67%|██████▋   | 124/186 [28:37<11:44, 11.36s/it] 67%|██████▋   | 125/186 [28:48<11:27, 11.27s/it] 68%|██████▊   | 126/186 [28:59<11:14, 11.24s/it] 68%|██████▊   | 127/186 [29:10<10:50, 11.03s/it] 69%|██████▉   | 128/186 [29:20<10:31, 10.88s/it] 69%|██████▉   | 129/186 [29:33<10:43, 11.29s/it] 70%|██████▉   | 130/186 [29:49<11:52, 12.73s/it] 70%|███████   | 131/186 [30:05<12:34, 13.73s/it] 71%|███████   | 132/186 [30:21<13:05, 14.54s/it] 72%|███████▏  | 133/186 [30:37<13:16, 15.03s/it] 72%|███████▏  | 134/186 [30:54<13:22, 15.43s/it] 73%|███████▎  | 135/186 [31:10<13:23, 15.75s/it] 73%|███████▎  | 136/186 [31:26<13:07, 15.74s/it] 74%|███████▎  | 137/186 [31:42<12:58, 15.89s/it] 74%|███████▍  | 138/186 [31:58<12:47, 15.99s/it] 75%|███████▍  | 139/186 [32:15<12:37, 16.11s/it] 75%|███████▌  | 140/186 [32:31<12:25, 16.21s/it] 76%|███████▌  | 141/186 [32:48<12:15, 16.34s/it] 76%|███████▋  | 142/186 [33:04<11:57, 16.31s/it] 77%|███████▋  | 143/186 [33:20<11:40, 16.29s/it] 77%|███████▋  | 144/186 [33:37<11:26, 16.34s/it] 78%|███████▊  | 145/186 [33:54<11:19, 16.58s/it] 78%|███████▊  | 146/186 [34:10<10:58, 16.45s/it] 79%|███████▉  | 147/186 [34:26<10:41, 16.45s/it] 80%|███████▉  | 148/186 [34:43<10:24, 16.43s/it] 80%|████████  | 149/186 [34:59<10:04, 16.34s/it] 81%|████████  | 150/186 [35:15<09:48, 16.35s/it] 81%|████████  | 151/186 [35:32<09:39, 16.57s/it] 82%|████████▏ | 152/186 [35:49<09:19, 16.46s/it] 82%|████████▏ | 153/186 [36:05<09:00, 16.38s/it] 83%|████████▎ | 154/186 [36:22<08:50, 16.57s/it] 83%|████████▎ | 155/186 [36:38<08:31, 16.50s/it] 84%|████████▍ | 156/186 [36:55<08:13, 16.46s/it] 84%|████████▍ | 157/186 [37:11<07:58, 16.50s/it] 85%|████████▍ | 158/186 [37:28<07:42, 16.52s/it] 85%|████████▌ | 159/186 [37:44<07:26, 16.55s/it] 86%|████████▌ | 160/186 [38:01<07:07, 16.46s/it] 87%|████████▋ | 161/186 [38:17<06:54, 16.58s/it] 87%|████████▋ | 162/186 [38:34<06:38, 16.59s/it] 88%|████████▊ | 163/186 [38:50<06:20, 16.52s/it] 88%|████████▊ | 164/186 [39:07<06:05, 16.60s/it] 89%|████████▊ | 165/186 [39:24<05:47, 16.53s/it] 89%|████████▉ | 166/186 [39:40<05:30, 16.52s/it] 90%|████████▉ | 167/186 [39:56<05:08, 16.24s/it] 90%|█████████ | 168/186 [40:11<04:48, 16.00s/it] 91%|█████████ | 169/186 [40:27<04:28, 15.82s/it] 91%|█████████▏| 170/186 [40:42<04:10, 15.68s/it] 92%|█████████▏| 171/186 [40:58<03:56, 15.79s/it] 92%|█████████▏| 172/186 [41:15<03:44, 16.05s/it] 93%|█████████▎| 173/186 [41:28<03:18, 15.29s/it] 94%|█████████▎| 174/186 [41:39<02:47, 14.00s/it] 94%|█████████▍| 175/186 [41:50<02:23, 13.07s/it] 95%|█████████▍| 176/186 [42:01<02:04, 12.41s/it] 95%|█████████▌| 177/186 [42:12<01:47, 11.89s/it] 96%|█████████▌| 178/186 [42:22<01:32, 11.54s/it] 96%|█████████▌| 179/186 [42:34<01:22, 11.72s/it] 97%|█████████▋| 180/186 [42:45<01:07, 11.29s/it] 97%|█████████▋| 181/186 [42:55<00:55, 11.14s/it] 98%|█████████▊| 182/186 [43:06<00:44, 11.02s/it] 98%|█████████▊| 183/186 [43:17<00:32, 10.97s/it] 99%|█████████▉| 184/186 [43:28<00:21, 10.95s/it] 99%|█████████▉| 185/186 [43:40<00:11, 11.18s/it]100%|██████████| 186/186 [43:51<00:00, 11.26s/it]                                                 {'train_runtime': 2631.6264, 'train_samples_per_second': 5.105, 'train_steps_per_second': 0.071, 'train_loss': 0.16976060149490194, 'epoch': 1.0}
100%|██████████| 186/186 [43:51<00:00, 11.26s/it]100%|██████████| 186/186 [43:51<00:00, 14.15s/it]
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-4
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33msliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-4[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251017_145748-iqjaq6ch/logs[0m
[2025-10-17 15:43:26,816] [INFO] [launch.py:347:main] Process 2408840 exits successfully.
[2025-10-17 15:43:26,816] [INFO] [launch.py:347:main] Process 2408837 exits successfully.
[2025-10-17 15:43:26,816] [INFO] [launch.py:347:main] Process 2408841 exits successfully.
[2025-10-17 15:43:26,816] [INFO] [launch.py:347:main] Process 2408839 exits successfully.
[2025-10-17 15:43:26,817] [INFO] [launch.py:347:main] Process 2408838 exits successfully.
[2025-10-17 15:43:26,817] [INFO] [launch.py:347:main] Process 2408836 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 15:43:30,129] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 15:43:30,716] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-17 15:43:30,716] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12423 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-4 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-4 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 2 --use_anchor False --use_sample_weight True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 15:43:32,281] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 15:43:32,892] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-17 15:43:32,893] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-17 15:43:32,893] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-17 15:43:32,893] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-17 15:43:32,893] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 15:43:36,556] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-17 15:43:36,784] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 15:43:36,921] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 15:43:37,098] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 15:43:37,098] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-10-17 15:43:37,744] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 15:43:37,878] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-17 15:43:38,077] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 15:43:38,199] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 15:43:38,202] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 15:43:38,397] [INFO] [comm.py:637:init_distributed] cdb=None
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-10-17 15:43:38,638] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 15:43:39,152] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251017_154339-e62ldvzv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-4
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/e62ldvzv
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.04s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.33s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.89s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.45s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.45s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.21s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.90s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.44s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.81s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.95s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.33s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.96s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.34s/it]
to bfloat16...
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.15s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.95s/it]
to bfloat16...
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.10s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.06s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.14s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.81s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.25s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.43s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.35s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.44s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.45s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.31s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.99s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.36s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.14s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.48s/it]
Formatting inputs...Skip in lazy mode
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:1')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:3')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:2')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:0')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:5')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:4')
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.010796308517456055 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.01111292839050293 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.01195073127746582 seconds
Loading extension module fused_adam...Loading extension module fused_adam...

Time to load fused_adam op: 0.10122227668762207 secondsTime to load fused_adam op: 0.10130953788757324 seconds

Loading extension module fused_adam...
Time to load fused_adam op: 0.10205602645874023 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/186 [00:16<50:37, 16.42s/it]  1%|          | 2/186 [00:31<48:44, 15.89s/it]  2%|▏         | 3/186 [00:48<48:46, 15.99s/it]  2%|▏         | 4/186 [01:03<48:19, 15.93s/it]  3%|▎         | 5/186 [01:19<48:14, 15.99s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1150 > 1048). Running this sequence through the model will result in indexing errors
  3%|▎         | 6/186 [01:36<48:56, 16.32s/it]WARNING: tokenization mismatch: 1 vs. 47. (ignored)
  4%|▍         | 7/186 [01:53<48:41, 16.32s/it]  4%|▍         | 8/186 [02:09<48:17, 16.28s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1089 > 1048). Running this sequence through the model will result in indexing errors
  5%|▍         | 9/186 [02:26<48:51, 16.56s/it]  5%|▌         | 10/186 [02:43<48:31, 16.54s/it]  6%|▌         | 11/186 [02:59<48:04, 16.48s/it]  6%|▋         | 12/186 [03:15<47:35, 16.41s/it]  7%|▋         | 13/186 [03:32<47:18, 16.41s/it]  8%|▊         | 14/186 [03:48<46:52, 16.35s/it]  8%|▊         | 15/186 [04:04<46:31, 16.32s/it]  9%|▊         | 16/186 [04:20<46:11, 16.30s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1233 > 1048). Running this sequence through the model will result in indexing errors
  9%|▉         | 17/186 [04:37<46:30, 16.51s/it] 10%|▉         | 18/186 [04:53<45:44, 16.34s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1233 > 1048). Running this sequence through the model will result in indexing errors
 10%|█         | 19/186 [05:10<46:03, 16.55s/it] 11%|█         | 20/186 [05:27<45:38, 16.50s/it] 11%|█▏        | 21/186 [05:43<45:17, 16.47s/it] 12%|█▏        | 22/186 [05:59<44:53, 16.43s/it] 12%|█▏        | 23/186 [06:16<44:27, 16.37s/it] 13%|█▎        | 24/186 [06:32<44:22, 16.44s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 13%|█▎        | 25/186 [06:49<44:00, 16.40s/it] 14%|█▍        | 26/186 [07:05<43:35, 16.34s/it] 15%|█▍        | 27/186 [07:22<43:52, 16.56s/it] 15%|█▌        | 28/186 [07:38<43:16, 16.43s/it] 16%|█▌        | 29/186 [07:54<42:39, 16.30s/it] 16%|█▌        | 30/186 [08:10<42:01, 16.17s/it] 17%|█▋        | 31/186 [08:25<41:11, 15.94s/it] 17%|█▋        | 32/186 [08:41<40:39, 15.84s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1150 > 1048). Running this sequence through the model will result in indexing errors
 18%|█▊        | 33/186 [08:57<40:44, 15.97s/it] 18%|█▊        | 34/186 [09:13<40:07, 15.84s/it] 19%|█▉        | 35/186 [09:30<40:39, 16.15s/it] 19%|█▉        | 36/186 [09:43<38:22, 15.35s/it] 20%|█▉        | 37/186 [09:54<34:48, 14.02s/it] 20%|██        | 38/186 [10:05<32:31, 13.19s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1245 > 1048). Running this sequence through the model will result in indexing errors
 21%|██        | 39/186 [10:16<30:47, 12.57s/it] 22%|██▏       | 40/186 [10:28<29:54, 12.29s/it] 22%|██▏       | 41/186 [10:39<28:51, 11.94s/it] 23%|██▎       | 42/186 [10:51<28:47, 12.00s/it] 23%|██▎       | 43/186 [11:02<27:46, 11.65s/it] 24%|██▎       | 44/186 [11:14<27:32, 11.64s/it] 24%|██▍       | 45/186 [11:25<27:01, 11.50s/it] 25%|██▍       | 46/186 [11:36<26:41, 11.44s/it] 25%|██▌       | 47/186 [11:48<26:50, 11.58s/it] 26%|██▌       | 48/186 [12:00<26:36, 11.57s/it] 26%|██▋       | 49/186 [12:11<26:16, 11.51s/it] 27%|██▋       | 50/186 [12:22<25:46, 11.37s/it] 27%|██▋       | 51/186 [12:33<25:07, 11.17s/it] 28%|██▊       | 52/186 [12:45<25:22, 11.36s/it] 28%|██▊       | 53/186 [13:01<28:20, 12.79s/it] 29%|██▉       | 54/186 [13:17<30:10, 13.72s/it] 30%|██▉       | 55/186 [13:33<31:48, 14.57s/it] 30%|███       | 56/186 [13:49<32:31, 15.01s/it] 31%|███       | 57/186 [14:05<32:50, 15.28s/it] 31%|███       | 58/186 [14:21<33:04, 15.50s/it] 32%|███▏      | 59/186 [14:37<33:11, 15.68s/it] 32%|███▏      | 60/186 [14:53<33:05, 15.76s/it] 33%|███▎      | 61/186 [15:09<33:00, 15.84s/it] 33%|███▎      | 62/186 [15:26<33:06, 16.02s/it] 34%|███▍      | 63/186 [15:42<32:56, 16.07s/it] 34%|███▍      | 64/186 [15:58<32:54, 16.19s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 35%|███▍      | 65/186 [16:14<32:41, 16.21s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 35%|███▌      | 66/186 [16:31<32:30, 16.25s/it] 36%|███▌      | 67/186 [16:48<32:40, 16.48s/it] 37%|███▋      | 68/186 [17:04<32:22, 16.46s/it] 37%|███▋      | 69/186 [17:21<32:22, 16.60s/it] 38%|███▊      | 70/186 [17:38<32:04, 16.59s/it] 38%|███▊      | 71/186 [17:54<31:47, 16.58s/it] 39%|███▊      | 72/186 [18:11<31:39, 16.66s/it] 39%|███▉      | 73/186 [18:28<31:19, 16.63s/it] 40%|███▉      | 74/186 [18:44<31:04, 16.65s/it] 40%|████      | 75/186 [19:01<30:53, 16.70s/it] 41%|████      | 76/186 [19:18<30:31, 16.65s/it] 41%|████▏     | 77/186 [19:34<30:13, 16.63s/it] 42%|████▏     | 78/186 [19:51<29:59, 16.66s/it] 42%|████▏     | 79/186 [20:08<29:36, 16.60s/it] 43%|████▎     | 80/186 [20:24<29:18, 16.59s/it] 44%|████▎     | 81/186 [20:40<28:53, 16.51s/it] 44%|████▍     | 82/186 [20:57<28:28, 16.43s/it] 45%|████▍     | 83/186 [21:13<28:05, 16.36s/it] 45%|████▌     | 84/186 [21:29<27:46, 16.34s/it] 46%|████▌     | 85/186 [21:46<27:35, 16.39s/it] 46%|████▌     | 86/186 [22:03<27:41, 16.62s/it] 47%|████▋     | 87/186 [22:19<27:10, 16.47s/it] 47%|████▋     | 88/186 [22:35<26:40, 16.34s/it] 48%|████▊     | 89/186 [22:51<26:23, 16.33s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 48%|████▊     | 90/186 [23:07<25:51, 16.16s/it] 49%|████▉     | 91/186 [23:22<25:14, 15.94s/it] 49%|████▉     | 92/186 [23:38<24:40, 15.75s/it] 50%|█████     | 93/186 [23:54<24:31, 15.82s/it] 51%|█████     | 94/186 [24:09<24:11, 15.78s/it] 51%|█████     | 95/186 [24:26<24:07, 15.91s/it] 52%|█████▏    | 96/186 [24:39<22:44, 15.16s/it] 52%|█████▏    | 97/186 [24:50<20:29, 13.81s/it] 53%|█████▎    | 98/186 [25:00<18:48, 12.83s/it] 53%|█████▎    | 99/186 [25:11<17:43, 12.23s/it] 54%|█████▍    | 100/186 [25:22<16:51, 11.76s/it] 54%|█████▍    | 101/186 [25:32<16:10, 11.42s/it] 55%|█████▍    | 102/186 [25:45<16:23, 11.71s/it] 55%|█████▌    | 103/186 [25:55<15:41, 11.34s/it] 56%|█████▌    | 104/186 [26:06<15:13, 11.14s/it] 56%|█████▋    | 105/186 [26:17<15:10, 11.23s/it] 57%|█████▋    | 106/186 [26:28<14:55, 11.19s/it] 58%|█████▊    | 107/186 [26:39<14:32, 11.05s/it] 58%|█████▊    | 108/186 [26:50<14:23, 11.07s/it] 59%|█████▊    | 109/186 [27:02<14:19, 11.16s/it] 59%|█████▉    | 110/186 [27:12<13:57, 11.02s/it] 60%|█████▉    | 111/186 [27:23<13:40, 10.94s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 60%|██████    | 112/186 [27:34<13:22, 10.85s/it] 61%|██████    | 113/186 [27:47<13:59, 11.49s/it] 61%|██████▏   | 114/186 [28:03<15:27, 12.88s/it] 62%|██████▏   | 115/186 [28:19<16:16, 13.75s/it] 62%|██████▏   | 116/186 [28:35<17:02, 14.61s/it] 63%|██████▎   | 117/186 [28:51<17:13, 14.98s/it] 63%|██████▎   | 118/186 [29:07<17:16, 15.25s/it] 64%|██████▍   | 119/186 [29:23<17:14, 15.44s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 65%|██████▍   | 120/186 [29:38<16:56, 15.40s/it] 65%|██████▌   | 121/186 [29:54<16:53, 15.60s/it] 66%|██████▌   | 122/186 [30:10<16:46, 15.73s/it] 66%|██████▌   | 123/186 [30:27<16:42, 15.91s/it] 67%|██████▋   | 124/186 [30:43<16:39, 16.12s/it] 67%|██████▋   | 125/186 [31:00<16:41, 16.42s/it] 68%|██████▊   | 126/186 [31:17<16:28, 16.48s/it] 68%|██████▊   | 127/186 [31:33<16:09, 16.44s/it] 69%|██████▉   | 128/186 [31:50<15:52, 16.42s/it] 69%|██████▉   | 129/186 [32:06<15:35, 16.40s/it] 70%|██████▉   | 130/186 [32:22<15:18, 16.41s/it] 70%|███████   | 131/186 [32:39<15:03, 16.44s/it] 71%|███████   | 132/186 [32:56<14:51, 16.50s/it] 72%|███████▏  | 133/186 [33:12<14:33, 16.48s/it] 72%|███████▏  | 134/186 [33:29<14:21, 16.56s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 73%|███████▎  | 135/186 [33:45<14:01, 16.49s/it] 73%|███████▎  | 136/186 [34:02<13:47, 16.55s/it] 74%|███████▎  | 137/186 [34:18<13:31, 16.55s/it] 74%|███████▍  | 138/186 [34:35<13:10, 16.47s/it] 75%|███████▍  | 139/186 [34:51<12:58, 16.56s/it] 75%|███████▌  | 140/186 [35:08<12:38, 16.49s/it] 76%|███████▌  | 141/186 [35:24<12:21, 16.47s/it] 76%|███████▋  | 142/186 [35:40<12:00, 16.38s/it] 77%|███████▋  | 143/186 [35:56<11:41, 16.32s/it] 77%|███████▋  | 144/186 [36:13<11:27, 16.36s/it] 78%|███████▊  | 145/186 [36:30<11:13, 16.43s/it] 78%|███████▊  | 146/186 [36:46<10:53, 16.35s/it] 79%|███████▉  | 147/186 [37:02<10:37, 16.34s/it] 80%|███████▉  | 148/186 [37:19<10:22, 16.38s/it] 80%|████████  | 149/186 [37:35<10:10, 16.50s/it] 81%|████████  | 150/186 [37:52<09:51, 16.43s/it] 81%|████████  | 151/186 [38:07<09:26, 16.19s/it] 82%|████████▏ | 152/186 [38:23<09:02, 15.95s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 82%|████████▏ | 153/186 [38:38<08:40, 15.77s/it] 83%|████████▎ | 154/186 [38:53<08:19, 15.62s/it] 83%|████████▎ | 155/186 [39:09<08:02, 15.58s/it] 84%|████████▍ | 156/186 [39:25<07:53, 15.78s/it] 84%|████████▍ | 157/186 [39:39<07:26, 15.41s/it] 85%|████████▍ | 158/186 [39:50<06:33, 14.07s/it] 85%|████████▌ | 159/186 [40:01<05:53, 13.09s/it] 86%|████████▌ | 160/186 [40:13<05:26, 12.56s/it] 87%|████████▋ | 161/186 [40:23<04:59, 11.97s/it] 87%|████████▋ | 162/186 [40:34<04:38, 11.62s/it] 88%|████████▊ | 163/186 [40:46<04:30, 11.78s/it] 88%|████████▊ | 164/186 [40:57<04:11, 11.44s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 89%|████████▊ | 165/186 [41:07<03:52, 11.09s/it] 89%|████████▉ | 166/186 [41:18<03:39, 10.98s/it] 90%|████████▉ | 167/186 [41:29<03:29, 11.02s/it] 90%|█████████ | 168/186 [41:39<03:15, 10.88s/it] 91%|█████████ | 169/186 [41:51<03:06, 10.95s/it] 91%|█████████▏| 170/186 [42:01<02:54, 10.91s/it] 92%|█████████▏| 171/186 [42:13<02:45, 11.06s/it] 92%|█████████▏| 172/186 [42:23<02:33, 10.94s/it] 93%|█████████▎| 173/186 [42:34<02:20, 10.84s/it] 94%|█████████▎| 174/186 [42:45<02:10, 10.91s/it] 94%|█████████▍| 175/186 [43:01<02:15, 12.32s/it] 95%|█████████▍| 176/186 [43:17<02:15, 13.52s/it] 95%|█████████▌| 177/186 [43:33<02:07, 14.21s/it] 96%|█████████▌| 178/186 [43:49<01:58, 14.81s/it] 96%|█████████▌| 179/186 [44:05<01:46, 15.16s/it] 97%|█████████▋| 180/186 [44:21<01:32, 15.34s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
 97%|█████████▋| 181/186 [44:37<01:18, 15.66s/it] 98%|█████████▊| 182/186 [44:53<01:03, 15.84s/it] 98%|█████████▊| 183/186 [45:10<00:47, 15.97s/it] 99%|█████████▉| 184/186 [45:26<00:32, 16.03s/it] 99%|█████████▉| 185/186 [45:42<00:16, 16.21s/it]100%|██████████| 186/186 [45:59<00:00, 16.32s/it]                                                 {'train_runtime': 2759.5948, 'train_samples_per_second': 4.868, 'train_steps_per_second': 0.067, 'train_loss': 0.205948245140814, 'epoch': 1.0}
100%|██████████| 186/186 [45:59<00:00, 16.32s/it]100%|██████████| 186/186 [45:59<00:00, 14.84s/it]
[2025-10-17 16:31:32,166] [INFO] [launch.py:347:main] Process 2454173 exits successfully.
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-4
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33msliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-4[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251017_154339-e62ldvzv/logs[0m
[2025-10-17 16:31:37,171] [INFO] [launch.py:347:main] Process 2454172 exits successfully.
[2025-10-17 16:31:38,172] [INFO] [launch.py:347:main] Process 2454174 exits successfully.
[2025-10-17 16:31:38,172] [INFO] [launch.py:347:main] Process 2454175 exits successfully.
[2025-10-17 16:31:38,173] [INFO] [launch.py:347:main] Process 2454176 exits successfully.
[2025-10-17 16:31:42,177] [INFO] [launch.py:347:main] Process 2454171 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 16:31:45,445] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 16:31:46,029] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-17 16:31:46,029] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12423 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1.5e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-4 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-4 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 0.5 --use_anchor False --use_sample_weight True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 16:31:47,749] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 16:31:48,343] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-17 16:31:48,343] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-17 16:31:48,343] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-17 16:31:48,343] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-17 16:31:48,343] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 16:31:52,107] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 16:31:52,113] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 16:31:52,419] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 16:31:52,422] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 16:31:53,640] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-17 16:31:54,003] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 16:31:54,103] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-17 16:31:54,338] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 16:31:54,373] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 16:31:54,480] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 16:31:54,480] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 16:31:54,708] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 16:31:54,752] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251017_163155-iees6iw7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-4
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/iees6iw7
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.32s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.74s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.06s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.40s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.47s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.81s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.45s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.68s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.67s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.07s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.91s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.29s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.58s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.90s/it]
to bfloat16...
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.61s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.91s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.66s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.02s/it]
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.18s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.58s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.88s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.23s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.28s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.63s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.84s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.12s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.98s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.95s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.27s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.86s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.20s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.14s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.44s/it]
Formatting inputs...Skip in lazy mode
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:3')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:2')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:5')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:4')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:1')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:0')
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.010881423950195312 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.014227867126464844 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10107612609863281 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.1011199951171875 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.014396190643310547 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.014037609100341797 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/186 [00:17<52:51, 17.14s/it]WARNING: tokenization mismatch: 1 vs. 47. (ignored)
  1%|          | 2/186 [00:33<50:35, 16.50s/it]  2%|▏         | 3/186 [00:49<49:38, 16.28s/it]  2%|▏         | 4/186 [01:05<49:48, 16.42s/it]  3%|▎         | 5/186 [01:21<49:08, 16.29s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1125 > 1048). Running this sequence through the model will result in indexing errors
  3%|▎         | 6/186 [01:38<49:36, 16.54s/it]  4%|▍         | 7/186 [01:54<48:48, 16.36s/it]  4%|▍         | 8/186 [02:11<48:21, 16.30s/it]  5%|▍         | 9/186 [02:27<48:11, 16.34s/it]  5%|▌         | 10/186 [02:43<47:39, 16.25s/it]  6%|▌         | 11/186 [03:00<47:46, 16.38s/it]  6%|▋         | 12/186 [03:16<47:30, 16.38s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1158 > 1048). Running this sequence through the model will result in indexing errors
  7%|▋         | 13/186 [03:33<47:43, 16.55s/it]  8%|▊         | 14/186 [03:49<47:10, 16.46s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
  8%|▊         | 15/186 [04:05<46:20, 16.26s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1143 > 1048). Running this sequence through the model will result in indexing errors
  9%|▊         | 16/186 [04:21<46:03, 16.26s/it]  9%|▉         | 17/186 [04:37<44:56, 15.96s/it] 10%|▉         | 18/186 [04:52<44:25, 15.87s/it] 10%|█         | 19/186 [05:08<44:17, 15.91s/it] 11%|█         | 20/186 [05:25<44:38, 16.13s/it] 11%|█▏        | 21/186 [05:36<40:20, 14.67s/it] 12%|█▏        | 22/186 [05:47<37:13, 13.62s/it] 12%|█▏        | 23/186 [05:59<35:05, 12.92s/it] 13%|█▎        | 24/186 [06:10<33:59, 12.59s/it] 13%|█▎        | 25/186 [06:22<32:36, 12.15s/it] 14%|█▍        | 26/186 [06:34<32:48, 12.30s/it] 15%|█▍        | 27/186 [06:45<31:34, 11.91s/it] 15%|█▌        | 28/186 [06:56<30:50, 11.71s/it] 16%|█▌        | 29/186 [07:08<30:13, 11.55s/it] 16%|█▌        | 30/186 [07:19<29:44, 11.44s/it] 17%|█▋        | 31/186 [07:30<29:07, 11.27s/it] 17%|█▋        | 32/186 [07:41<29:08, 11.36s/it] 18%|█▊        | 33/186 [07:53<29:06, 11.42s/it] 18%|█▊        | 34/186 [08:04<28:49, 11.38s/it] 19%|█▉        | 35/186 [08:15<28:23, 11.28s/it] 19%|█▉        | 36/186 [08:26<27:58, 11.19s/it] 20%|█▉        | 37/186 [08:39<28:46, 11.58s/it] 20%|██        | 38/186 [08:55<32:13, 13.07s/it]WARNING: tokenization mismatch: 1 vs. 47. (ignored)
 21%|██        | 39/186 [09:11<34:22, 14.03s/it] 22%|██▏       | 40/186 [09:28<35:47, 14.71s/it] 22%|██▏       | 41/186 [09:44<36:48, 15.23s/it] 23%|██▎       | 42/186 [10:00<36:59, 15.41s/it] 23%|██▎       | 43/186 [10:16<37:10, 15.60s/it] 24%|██▎       | 44/186 [10:33<37:35, 15.89s/it] 24%|██▍       | 45/186 [10:49<37:28, 15.95s/it] 25%|██▍       | 46/186 [11:05<37:23, 16.03s/it] 25%|██▌       | 47/186 [11:22<37:47, 16.31s/it] 26%|██▌       | 48/186 [11:39<37:43, 16.40s/it] 26%|██▋       | 49/186 [11:55<37:25, 16.39s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 27%|██▋       | 50/186 [12:11<37:11, 16.41s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 27%|██▋       | 51/186 [12:28<36:48, 16.36s/it] 28%|██▊       | 52/186 [12:44<36:43, 16.44s/it] 28%|██▊       | 53/186 [13:00<36:06, 16.29s/it] 29%|██▉       | 54/186 [13:16<35:49, 16.28s/it] 30%|██▉       | 55/186 [13:33<35:37, 16.32s/it] 30%|███       | 56/186 [13:49<35:24, 16.34s/it] 31%|███       | 57/186 [14:06<35:07, 16.33s/it] 31%|███       | 58/186 [14:22<34:51, 16.34s/it] 32%|███▏      | 59/186 [14:38<34:37, 16.36s/it] 32%|███▏      | 60/186 [14:55<34:25, 16.39s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1150 > 1048). Running this sequence through the model will result in indexing errors
 33%|███▎      | 61/186 [15:12<34:34, 16.60s/it] 33%|███▎      | 62/186 [15:28<34:19, 16.61s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 34%|███▍      | 63/186 [15:45<34:04, 16.63s/it] 34%|███▍      | 64/186 [16:01<33:37, 16.54s/it] 35%|███▍      | 65/186 [16:18<33:20, 16.53s/it] 35%|███▌      | 66/186 [16:35<33:04, 16.54s/it] 36%|███▌      | 67/186 [16:51<32:45, 16.52s/it] 37%|███▋      | 68/186 [17:07<32:15, 16.40s/it] 37%|███▋      | 69/186 [17:24<32:05, 16.46s/it] 38%|███▊      | 70/186 [17:40<31:58, 16.54s/it] 38%|███▊      | 71/186 [17:57<31:37, 16.50s/it] 39%|███▊      | 72/186 [18:14<31:36, 16.64s/it] 39%|███▉      | 73/186 [18:31<31:25, 16.69s/it] 40%|███▉      | 74/186 [18:47<30:51, 16.53s/it] 40%|████      | 75/186 [19:03<30:14, 16.34s/it] 41%|████      | 76/186 [19:18<29:28, 16.08s/it] 41%|████▏     | 77/186 [19:34<28:54, 15.91s/it] 42%|████▏     | 78/186 [19:49<28:30, 15.83s/it] 42%|████▏     | 79/186 [20:05<28:11, 15.81s/it] 43%|████▎     | 80/186 [20:21<28:14, 15.98s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
 44%|████▎     | 81/186 [20:37<27:42, 15.83s/it] 44%|████▍     | 82/186 [20:48<24:57, 14.40s/it] 45%|████▍     | 83/186 [20:59<23:01, 13.41s/it] 45%|████▌     | 84/186 [21:11<21:46, 12.81s/it] 46%|████▌     | 85/186 [21:22<20:47, 12.35s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1072 > 1048). Running this sequence through the model will result in indexing errors
 46%|████▌     | 86/186 [21:33<20:05, 12.06s/it] 47%|████▋     | 87/186 [21:45<19:49, 12.02s/it] 47%|████▋     | 88/186 [21:56<19:02, 11.66s/it] 48%|████▊     | 89/186 [22:07<18:39, 11.54s/it] 48%|████▊     | 90/186 [22:18<18:16, 11.43s/it] 49%|████▉     | 91/186 [22:29<17:56, 11.33s/it] 49%|████▉     | 92/186 [22:41<17:50, 11.38s/it] 50%|█████     | 93/186 [22:52<17:31, 11.30s/it] 51%|█████     | 94/186 [23:03<17:22, 11.33s/it] 51%|█████     | 95/186 [23:14<16:56, 11.17s/it] 52%|█████▏    | 96/186 [23:25<16:34, 11.05s/it] 52%|█████▏    | 97/186 [23:36<16:26, 11.08s/it] 53%|█████▎    | 98/186 [23:52<18:08, 12.36s/it] 53%|█████▎    | 99/186 [24:08<19:37, 13.53s/it] 54%|█████▍    | 100/186 [24:24<20:32, 14.34s/it] 54%|█████▍    | 101/186 [24:40<21:07, 14.91s/it] 55%|█████▍    | 102/186 [24:57<21:40, 15.48s/it] 55%|█████▌    | 103/186 [25:14<21:52, 15.81s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 56%|█████▌    | 104/186 [25:30<21:42, 15.89s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
 56%|█████▋    | 105/186 [25:47<21:51, 16.19s/it] 57%|█████▋    | 106/186 [26:03<21:34, 16.19s/it] 58%|█████▊    | 107/186 [26:19<21:22, 16.23s/it] 58%|█████▊    | 108/186 [26:36<21:30, 16.54s/it] 59%|█████▊    | 109/186 [26:53<21:09, 16.48s/it] 59%|█████▉    | 110/186 [27:09<20:57, 16.54s/it] 60%|█████▉    | 111/186 [27:27<20:58, 16.78s/it] 60%|██████    | 112/186 [27:43<20:27, 16.59s/it] 61%|██████    | 113/186 [27:59<20:00, 16.45s/it] 61%|██████▏   | 114/186 [28:16<19:46, 16.49s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1143 > 1048). Running this sequence through the model will result in indexing errors
 62%|██████▏   | 115/186 [28:33<19:44, 16.68s/it] 62%|██████▏   | 116/186 [28:49<19:25, 16.65s/it] 63%|██████▎   | 117/186 [29:06<19:03, 16.57s/it] 63%|██████▎   | 118/186 [29:22<18:49, 16.61s/it] 64%|██████▍   | 119/186 [29:38<18:21, 16.44s/it] 65%|██████▍   | 120/186 [29:55<18:04, 16.43s/it]WARNING: tokenization mismatch: 1 vs. 48. (ignored)
 65%|██████▌   | 121/186 [30:12<18:02, 16.65s/it] 66%|██████▌   | 122/186 [30:28<17:37, 16.53s/it] 66%|██████▌   | 123/186 [30:45<17:22, 16.55s/it] 67%|██████▋   | 124/186 [31:02<17:14, 16.69s/it] 67%|██████▋   | 125/186 [31:19<16:57, 16.69s/it] 68%|██████▊   | 126/186 [31:35<16:42, 16.71s/it] 68%|██████▊   | 127/186 [31:52<16:25, 16.71s/it] 69%|██████▉   | 128/186 [32:09<16:07, 16.68s/it] 69%|██████▉   | 129/186 [32:25<15:50, 16.67s/it] 70%|██████▉   | 130/186 [32:42<15:40, 16.79s/it] 70%|███████   | 131/186 [32:59<15:17, 16.69s/it] 71%|███████   | 132/186 [33:15<14:58, 16.64s/it] 72%|███████▏  | 133/186 [33:32<14:36, 16.55s/it] 72%|███████▏  | 134/186 [33:48<14:21, 16.56s/it] 73%|███████▎  | 135/186 [34:04<13:57, 16.42s/it]WARNING: tokenization mismatch: 1 vs. 47. (ignored)
 73%|███████▎  | 136/186 [34:20<13:25, 16.12s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 74%|███████▎  | 137/186 [34:36<13:17, 16.28s/it] 74%|███████▍  | 138/186 [34:52<12:56, 16.17s/it] 75%|███████▍  | 139/186 [35:09<12:48, 16.36s/it] 75%|███████▌  | 140/186 [35:25<12:31, 16.35s/it] 76%|███████▌  | 141/186 [35:37<11:05, 14.79s/it] 76%|███████▋  | 142/186 [35:48<10:08, 13.84s/it] 77%|███████▋  | 143/186 [35:59<09:20, 13.04s/it] 77%|███████▋  | 144/186 [36:11<08:43, 12.46s/it] 78%|███████▊  | 145/186 [36:22<08:15, 12.10s/it] 78%|███████▊  | 146/186 [36:34<08:04, 12.12s/it] 79%|███████▉  | 147/186 [36:45<07:41, 11.84s/it] 80%|███████▉  | 148/186 [36:56<07:17, 11.53s/it] 80%|████████  | 149/186 [37:07<07:03, 11.44s/it] 81%|████████  | 150/186 [37:18<06:50, 11.40s/it] 81%|████████  | 151/186 [37:30<06:38, 11.39s/it] 82%|████████▏ | 152/186 [37:41<06:26, 11.36s/it] 82%|████████▏ | 153/186 [37:53<06:19, 11.51s/it] 83%|████████▎ | 154/186 [38:04<06:06, 11.45s/it] 83%|████████▎ | 155/186 [38:15<05:50, 11.30s/it] 84%|████████▍ | 156/186 [38:26<05:35, 11.17s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 84%|████████▍ | 157/186 [38:38<05:31, 11.42s/it] 85%|████████▍ | 158/186 [38:54<05:56, 12.74s/it] 85%|████████▌ | 159/186 [39:10<06:11, 13.77s/it] 86%|████████▌ | 160/186 [39:26<06:18, 14.54s/it] 87%|████████▋ | 161/186 [39:43<06:16, 15.06s/it] 87%|████████▋ | 162/186 [39:59<06:12, 15.51s/it] 88%|████████▊ | 163/186 [40:16<06:03, 15.80s/it] 88%|████████▊ | 164/186 [40:32<05:53, 16.05s/it] 89%|████████▊ | 165/186 [40:48<05:37, 16.06s/it] 89%|████████▉ | 166/186 [41:05<05:22, 16.12s/it] 90%|████████▉ | 167/186 [41:22<05:10, 16.36s/it] 90%|█████████ | 168/186 [41:38<04:55, 16.42s/it] 91%|█████████ | 169/186 [41:55<04:40, 16.47s/it] 91%|█████████▏| 170/186 [42:12<04:26, 16.67s/it] 92%|█████████▏| 171/186 [42:28<04:08, 16.56s/it] 92%|█████████▏| 172/186 [42:45<03:52, 16.64s/it] 93%|█████████▎| 173/186 [43:02<03:37, 16.77s/it] 94%|█████████▎| 174/186 [43:19<03:20, 16.69s/it] 94%|█████████▍| 175/186 [43:35<03:02, 16.62s/it] 95%|█████████▍| 176/186 [43:52<02:46, 16.60s/it] 95%|█████████▌| 177/186 [44:08<02:29, 16.59s/it] 96%|█████████▌| 178/186 [44:25<02:13, 16.63s/it] 96%|█████████▌| 179/186 [44:42<01:57, 16.73s/it] 97%|█████████▋| 180/186 [44:58<01:39, 16.65s/it] 97%|█████████▋| 181/186 [45:15<01:22, 16.57s/it] 98%|█████████▊| 182/186 [45:31<01:06, 16.60s/it] 98%|█████████▊| 183/186 [45:48<00:49, 16.59s/it] 99%|█████████▉| 184/186 [46:05<00:33, 16.63s/it] 99%|█████████▉| 185/186 [46:21<00:16, 16.56s/it]100%|██████████| 186/186 [46:38<00:00, 16.56s/it]                                                 {'train_runtime': 2798.1734, 'train_samples_per_second': 4.801, 'train_steps_per_second': 0.066, 'train_loss': 0.1371927979171917, 'epoch': 1.0}
100%|██████████| 186/186 [46:38<00:00, 16.56s/it]100%|██████████| 186/186 [46:38<00:00, 15.04s/it]
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-4
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33msliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-4[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251017_163155-iees6iw7/logs[0m
[2025-10-17 17:20:30,683] [INFO] [launch.py:347:main] Process 2500910 exits successfully.
[2025-10-17 17:20:30,684] [INFO] [launch.py:347:main] Process 2500907 exits successfully.
[2025-10-17 17:20:30,684] [INFO] [launch.py:347:main] Process 2500911 exits successfully.
[2025-10-17 17:20:30,684] [INFO] [launch.py:347:main] Process 2500909 exits successfully.
[2025-10-17 17:20:30,684] [INFO] [launch.py:347:main] Process 2500908 exits successfully.
[2025-10-17 17:20:31,686] [INFO] [launch.py:347:main] Process 2500906 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 17:20:35,017] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 17:20:35,575] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-17 17:20:35,575] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12423 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1.5e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-4 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-4 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 1 --use_anchor False --use_sample_weight True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 17:20:37,109] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 17:20:37,703] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-17 17:20:37,704] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-17 17:20:37,704] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-17 17:20:37,704] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-17 17:20:37,704] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 17:20:41,537] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 17:20:41,538] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 17:20:41,685] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 17:20:41,853] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 17:20:41,853] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-10-17 17:20:41,867] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 17:20:41,928] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 17:20:41,994] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-17 17:20:42,120] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 17:20:42,188] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 17:20:42,432] [INFO] [comm.py:637:init_distributed] cdb=None
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
[2025-10-17 17:20:42,928] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 17:20:43,554] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251017_172043-c7om0s7w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-4
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/c7om0s7w
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.96s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.46s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.24s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.37s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.19s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.20s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.24s/it]to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.88s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.54s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.89s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.91s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.26s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.72s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.05s/it]
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.82s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.05s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.76s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.47s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.24s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.66s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.27s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.36s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.74s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.75s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.13s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.54s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.87s/it]
Formatting inputs...Skip in lazy mode
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.88s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.22s/it]
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:3')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:1')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:5')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:0')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:2')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:4')
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.011365890502929688 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.011351823806762695 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10238003730773926 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10103726387023926 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.1010122299194336 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.01185297966003418 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Token indices sequence length is longer than the specified maximum sequence length for this model (1297 > 1048). Running this sequence through the model will result in indexing errors
  1%|          | 1/186 [00:16<52:07, 16.90s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
  1%|          | 2/186 [00:32<50:08, 16.35s/it]  2%|▏         | 3/186 [00:44<43:24, 14.23s/it]  2%|▏         | 4/186 [00:55<38:38, 12.74s/it]  3%|▎         | 5/186 [01:05<35:56, 11.91s/it]  3%|▎         | 6/186 [01:16<34:28, 11.49s/it]  4%|▍         | 7/186 [01:27<33:46, 11.32s/it]  4%|▍         | 8/186 [01:37<32:52, 11.08s/it]  5%|▍         | 9/186 [01:49<33:25, 11.33s/it]  5%|▌         | 10/186 [01:59<32:19, 11.02s/it]  6%|▌         | 11/186 [02:11<32:13, 11.05s/it]  6%|▋         | 12/186 [02:22<32:20, 11.15s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
  7%|▋         | 13/186 [02:33<31:54, 11.07s/it]  8%|▊         | 14/186 [02:44<31:56, 11.14s/it]  8%|▊         | 15/186 [02:56<32:19, 11.34s/it]  9%|▊         | 16/186 [03:07<31:55, 11.27s/it]  9%|▉         | 17/186 [03:18<31:12, 11.08s/it] 10%|▉         | 18/186 [03:28<30:46, 10.99s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 10%|█         | 19/186 [03:40<30:53, 11.10s/it] 11%|█         | 20/186 [03:55<34:32, 12.48s/it] 11%|█▏        | 21/186 [04:12<37:22, 13.59s/it]WARNING: tokenization mismatch: 1 vs. 47. (ignored)
 12%|█▏        | 22/186 [04:28<39:08, 14.32s/it] 12%|█▏        | 23/186 [04:44<40:22, 14.86s/it] 13%|█▎        | 24/186 [05:00<41:24, 15.33s/it] 13%|█▎        | 25/186 [05:16<41:35, 15.50s/it] 14%|█▍        | 26/186 [05:32<41:28, 15.55s/it] 15%|█▍        | 27/186 [05:48<41:33, 15.68s/it] 15%|█▌        | 28/186 [06:04<41:24, 15.73s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 16%|█▌        | 29/186 [06:20<41:28, 15.85s/it] 16%|█▌        | 30/186 [06:36<41:24, 15.93s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1073 > 1048). Running this sequence through the model will result in indexing errors
 17%|█▋        | 31/186 [06:53<41:46, 16.17s/it] 17%|█▋        | 32/186 [07:09<41:41, 16.25s/it] 18%|█▊        | 33/186 [07:25<41:24, 16.24s/it] 18%|█▊        | 34/186 [07:41<40:57, 16.17s/it] 19%|█▉        | 35/186 [07:58<40:56, 16.27s/it] 19%|█▉        | 36/186 [08:14<40:57, 16.39s/it] 20%|█▉        | 37/186 [08:31<40:31, 16.32s/it] 20%|██        | 38/186 [08:47<40:04, 16.25s/it] 21%|██        | 39/186 [09:03<39:46, 16.24s/it] 22%|██▏       | 40/186 [09:19<39:40, 16.30s/it] 22%|██▏       | 41/186 [09:36<39:22, 16.29s/it] 23%|██▎       | 42/186 [09:52<39:13, 16.34s/it] 23%|██▎       | 43/186 [10:08<39:00, 16.37s/it] 24%|██▎       | 44/186 [10:25<38:37, 16.32s/it] 24%|██▍       | 45/186 [10:41<38:34, 16.42s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 25%|██▍       | 46/186 [10:58<38:13, 16.38s/it] 25%|██▌       | 47/186 [11:14<37:55, 16.37s/it] 26%|██▌       | 48/186 [11:30<37:24, 16.26s/it] 26%|██▋       | 49/186 [11:47<37:35, 16.46s/it] 27%|██▋       | 50/186 [12:03<37:07, 16.38s/it] 27%|██▋       | 51/186 [12:19<36:44, 16.33s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1074 > 1048). Running this sequence through the model will result in indexing errors
 28%|██▊       | 52/186 [12:36<36:24, 16.30s/it] 28%|██▊       | 53/186 [12:52<36:30, 16.47s/it] 29%|██▉       | 54/186 [13:09<36:02, 16.39s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
 30%|██▉       | 55/186 [13:25<35:54, 16.44s/it] 30%|███       | 56/186 [13:41<35:30, 16.39s/it] 31%|███       | 57/186 [13:57<34:45, 16.16s/it] 31%|███       | 58/186 [14:13<34:09, 16.01s/it] 32%|███▏      | 59/186 [14:28<33:21, 15.76s/it] 32%|███▏      | 60/186 [14:43<32:52, 15.65s/it] 33%|███▎      | 61/186 [14:59<32:34, 15.64s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 33%|███▎      | 62/186 [15:15<32:37, 15.79s/it] 34%|███▍      | 63/186 [15:32<32:54, 16.05s/it] 34%|███▍      | 64/186 [15:43<29:31, 14.52s/it] 35%|███▍      | 65/186 [15:53<27:01, 13.40s/it] 35%|███▌      | 66/186 [16:05<25:29, 12.74s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 36%|███▌      | 67/186 [16:15<24:04, 12.14s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1159 > 1048). Running this sequence through the model will result in indexing errors
 37%|███▋      | 68/186 [16:26<22:54, 11.65s/it] 37%|███▋      | 69/186 [16:38<22:42, 11.65s/it] 38%|███▊      | 70/186 [16:49<22:34, 11.68s/it] 38%|███▊      | 71/186 [17:00<21:37, 11.28s/it] 39%|███▊      | 72/186 [17:10<21:07, 11.12s/it] 39%|███▉      | 73/186 [17:21<20:53, 11.10s/it] 40%|███▉      | 74/186 [17:32<20:27, 10.96s/it] 40%|████      | 75/186 [17:43<20:17, 10.97s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 41%|████      | 76/186 [17:54<20:03, 10.94s/it] 41%|████▏     | 77/186 [18:05<19:46, 10.88s/it] 42%|████▏     | 78/186 [18:15<19:28, 10.82s/it] 42%|████▏     | 79/186 [18:26<19:04, 10.70s/it] 43%|████▎     | 80/186 [18:37<18:57, 10.73s/it] 44%|████▎     | 81/186 [18:52<21:11, 12.11s/it] 44%|████▍     | 82/186 [19:08<22:59, 13.26s/it] 45%|████▍     | 83/186 [19:24<24:11, 14.10s/it] 45%|████▌     | 84/186 [19:40<24:52, 14.63s/it] 46%|████▌     | 85/186 [19:56<25:19, 15.05s/it] 46%|████▌     | 86/186 [20:12<25:33, 15.33s/it] 47%|████▋     | 87/186 [20:27<25:28, 15.44s/it] 47%|████▋     | 88/186 [20:43<25:24, 15.56s/it] 48%|████▊     | 89/186 [20:59<25:18, 15.66s/it] 48%|████▊     | 90/186 [21:15<25:15, 15.79s/it] 49%|████▉     | 91/186 [21:31<25:08, 15.88s/it] 49%|████▉     | 92/186 [21:47<24:59, 15.95s/it] 50%|█████     | 93/186 [22:04<25:00, 16.13s/it] 51%|█████     | 94/186 [22:20<24:46, 16.16s/it] 51%|█████     | 95/186 [22:37<24:52, 16.40s/it] 52%|█████▏    | 96/186 [22:54<24:40, 16.45s/it] 52%|█████▏    | 97/186 [23:10<24:22, 16.44s/it] 53%|█████▎    | 98/186 [23:27<24:04, 16.42s/it] 53%|█████▎    | 99/186 [23:43<23:43, 16.37s/it] 54%|█████▍    | 100/186 [24:00<23:35, 16.46s/it] 54%|█████▍    | 101/186 [24:16<23:14, 16.41s/it] 55%|█████▍    | 102/186 [24:33<23:09, 16.54s/it] 55%|█████▌    | 103/186 [24:50<23:05, 16.69s/it] 56%|█████▌    | 104/186 [25:06<22:39, 16.58s/it] 56%|█████▋    | 105/186 [25:23<22:32, 16.70s/it] 57%|█████▋    | 106/186 [25:40<22:27, 16.84s/it] 58%|█████▊    | 107/186 [25:57<22:07, 16.80s/it] 58%|█████▊    | 108/186 [26:13<21:35, 16.61s/it] 59%|█████▊    | 109/186 [26:29<21:01, 16.38s/it] 59%|█████▉    | 110/186 [26:45<20:31, 16.21s/it] 60%|█████▉    | 111/186 [27:01<20:13, 16.18s/it] 60%|██████    | 112/186 [27:18<20:09, 16.35s/it] 61%|██████    | 113/186 [27:34<19:56, 16.39s/it] 61%|██████▏   | 114/186 [27:50<19:28, 16.23s/it] 62%|██████▏   | 115/186 [28:06<19:05, 16.14s/it] 62%|██████▏   | 116/186 [28:22<18:50, 16.16s/it] 63%|██████▎   | 117/186 [28:38<18:39, 16.23s/it] 63%|██████▎   | 118/186 [28:55<18:21, 16.19s/it] 64%|██████▍   | 119/186 [29:10<17:59, 16.12s/it] 65%|██████▍   | 120/186 [29:26<17:34, 15.97s/it] 65%|██████▌   | 121/186 [29:41<17:05, 15.78s/it] 66%|██████▌   | 122/186 [29:57<16:38, 15.60s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1158 > 1048). Running this sequence through the model will result in indexing errors
 66%|██████▌   | 123/186 [30:13<16:45, 15.97s/it] 67%|██████▋   | 124/186 [30:29<16:28, 15.95s/it] 67%|██████▋   | 125/186 [30:40<14:35, 14.35s/it] 68%|██████▊   | 126/186 [30:51<13:14, 13.25s/it] 68%|██████▊   | 127/186 [31:01<12:18, 12.52s/it] 69%|██████▉   | 128/186 [31:12<11:39, 12.06s/it] 69%|██████▉   | 129/186 [31:23<11:04, 11.66s/it] 70%|██████▉   | 130/186 [31:35<10:54, 11.68s/it] 70%|███████   | 131/186 [31:47<10:43, 11.70s/it] 71%|███████   | 132/186 [31:57<10:17, 11.43s/it] 72%|███████▏  | 133/186 [32:08<09:56, 11.26s/it] 72%|███████▏  | 134/186 [32:19<09:37, 11.10s/it] 73%|███████▎  | 135/186 [32:29<09:14, 10.87s/it] 73%|███████▎  | 136/186 [32:41<09:13, 11.08s/it] 74%|███████▎  | 137/186 [32:52<09:03, 11.09s/it] 74%|███████▍  | 138/186 [33:03<08:48, 11.02s/it] 75%|███████▍  | 139/186 [33:14<08:32, 10.90s/it] 75%|███████▌  | 140/186 [33:24<08:10, 10.66s/it] 76%|███████▌  | 141/186 [33:35<08:13, 10.97s/it] 76%|███████▋  | 142/186 [33:52<09:11, 12.54s/it] 77%|███████▋  | 143/186 [34:08<09:48, 13.69s/it] 77%|███████▋  | 144/186 [34:24<10:05, 14.41s/it] 78%|███████▊  | 145/186 [34:40<10:13, 14.97s/it] 78%|███████▊  | 146/186 [34:56<10:12, 15.32s/it] 79%|███████▉  | 147/186 [35:13<10:15, 15.79s/it] 80%|███████▉  | 148/186 [35:29<09:59, 15.79s/it] 80%|████████  | 149/186 [35:45<09:43, 15.76s/it] 81%|████████  | 150/186 [36:01<09:29, 15.82s/it] 81%|████████  | 151/186 [36:17<09:17, 15.94s/it] 82%|████████▏ | 152/186 [36:33<09:05, 16.03s/it] 82%|████████▏ | 153/186 [36:49<08:50, 16.09s/it] 83%|████████▎ | 154/186 [37:05<08:33, 16.04s/it] 83%|████████▎ | 155/186 [37:22<08:20, 16.15s/it] 84%|████████▍ | 156/186 [37:39<08:10, 16.35s/it] 84%|████████▍ | 157/186 [37:55<07:54, 16.37s/it] 85%|████████▍ | 158/186 [38:11<07:36, 16.29s/it] 85%|████████▌ | 159/186 [38:27<07:18, 16.25s/it] 86%|████████▌ | 160/186 [38:43<07:01, 16.22s/it] 87%|████████▋ | 161/186 [39:00<06:46, 16.25s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 87%|████████▋ | 162/186 [39:16<06:33, 16.41s/it] 88%|████████▊ | 163/186 [39:33<06:21, 16.59s/it] 88%|████████▊ | 164/186 [39:50<06:03, 16.54s/it] 89%|████████▊ | 165/186 [40:06<05:44, 16.42s/it] 89%|████████▉ | 166/186 [40:22<05:28, 16.41s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 90%|████████▉ | 167/186 [40:39<05:12, 16.43s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1074 > 1048). Running this sequence through the model will result in indexing errors
 90%|█████████ | 168/186 [40:55<04:54, 16.37s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 91%|█████████ | 169/186 [41:12<04:38, 16.39s/it] 91%|█████████▏| 170/186 [41:28<04:21, 16.37s/it] 92%|█████████▏| 171/186 [41:45<04:08, 16.56s/it] 92%|█████████▏| 172/186 [42:02<03:52, 16.58s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 93%|█████████▎| 173/186 [42:18<03:33, 16.42s/it] 94%|█████████▎| 174/186 [42:34<03:17, 16.45s/it] 94%|█████████▍| 175/186 [42:51<03:02, 16.55s/it] 95%|█████████▍| 176/186 [43:07<02:44, 16.45s/it] 95%|█████████▌| 177/186 [43:23<02:26, 16.28s/it] 96%|█████████▌| 178/186 [43:39<02:10, 16.25s/it] 96%|█████████▌| 179/186 [43:55<01:52, 16.13s/it] 97%|█████████▋| 180/186 [44:10<01:35, 15.94s/it] 97%|█████████▋| 181/186 [44:26<01:19, 15.89s/it] 98%|█████████▊| 182/186 [44:42<01:03, 15.79s/it] 98%|█████████▊| 183/186 [44:58<00:47, 15.80s/it] 99%|█████████▉| 184/186 [45:14<00:31, 15.91s/it] 99%|█████████▉| 185/186 [45:29<00:15, 15.82s/it]100%|██████████| 186/186 [45:41<00:00, 14.46s/it]                                                 {'train_runtime': 2741.2189, 'train_samples_per_second': 4.901, 'train_steps_per_second': 0.068, 'train_loss': 0.13707312717232653, 'epoch': 1.0}
100%|██████████| 186/186 [45:41<00:00, 14.46s/it]100%|██████████| 186/186 [45:41<00:00, 14.74s/it]
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-4
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33msliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-4[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251017_172043-c7om0s7w/logs[0m
[2025-10-17 18:08:23,029] [INFO] [launch.py:347:main] Process 2550798 exits successfully.
[2025-10-17 18:08:23,030] [INFO] [launch.py:347:main] Process 2550795 exits successfully.
[2025-10-17 18:08:24,032] [INFO] [launch.py:347:main] Process 2550799 exits successfully.
[2025-10-17 18:08:24,032] [INFO] [launch.py:347:main] Process 2550797 exits successfully.
[2025-10-17 18:08:24,032] [INFO] [launch.py:347:main] Process 2550796 exits successfully.
[2025-10-17 18:08:26,035] [INFO] [launch.py:347:main] Process 2550794 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 18:08:29,525] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 18:08:30,097] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-17 18:08:30,097] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12423 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1.5e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-4 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-4 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 2 --use_anchor False --use_sample_weight True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 18:08:31,873] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 18:08:32,515] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-17 18:08:32,515] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-17 18:08:32,515] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-17 18:08:32,515] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-17 18:08:32,515] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 18:08:36,549] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 18:08:36,696] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-17 18:08:36,736] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 18:08:36,870] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-17 18:08:36,896] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 18:08:36,926] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-17 18:08:37,053] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 18:08:37,085] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 18:08:37,177] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 18:08:37,209] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 18:08:37,210] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-17 18:08:37,256] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 18:08:37,405] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: creating run
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251017_180838-1ncgq6jx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-4
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/1ncgq6jx
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.82s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.16s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.28s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.51s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.96s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.29s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.37s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.25s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.31s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.18s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.74s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.13s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.95s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.30s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.29s/it]
to bfloat16...
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.20s/it]
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.46s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.16s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.70s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.53s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.38s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.79s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.11s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.87s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.14s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.50s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.73s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.05s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.69s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.06s/it]
Formatting inputs...Skip in lazy mode
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.07s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.43s/it]
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:4')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:5')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:2')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:3')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:0')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:1')
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Time to load fused_adam op: 0.008529186248779297 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.010534286499023438 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10113072395324707 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10123181343078613 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.011453390121459961 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.2015094757080078 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/186 [00:10<31:43, 10.29s/it]  1%|          | 2/186 [00:19<29:18,  9.56s/it]  2%|▏         | 3/186 [00:28<28:34,  9.37s/it]  2%|▏         | 4/186 [00:37<28:02,  9.25s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1263 > 1048). Running this sequence through the model will result in indexing errors
  3%|▎         | 5/186 [00:49<31:12, 10.34s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
  3%|▎         | 6/186 [01:05<36:39, 12.22s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
  4%|▍         | 7/186 [01:21<40:00, 13.41s/it]  4%|▍         | 8/186 [01:37<42:09, 14.21s/it]  5%|▍         | 9/186 [01:53<43:30, 14.75s/it]  5%|▌         | 10/186 [02:09<44:31, 15.18s/it]  6%|▌         | 11/186 [02:25<44:46, 15.35s/it]  6%|▋         | 12/186 [02:41<44:55, 15.49s/it]  7%|▋         | 13/186 [02:56<44:59, 15.60s/it]  8%|▊         | 14/186 [03:12<44:50, 15.64s/it]  8%|▊         | 15/186 [03:29<45:24, 15.93s/it]  9%|▊         | 16/186 [03:45<45:22, 16.01s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1072 > 1048). Running this sequence through the model will result in indexing errors
  9%|▉         | 17/186 [04:02<45:54, 16.30s/it] 10%|▉         | 18/186 [04:18<45:37, 16.29s/it] 10%|█         | 19/186 [04:35<45:44, 16.43s/it] 11%|█         | 20/186 [04:51<45:05, 16.30s/it] 11%|█▏        | 21/186 [05:07<44:45, 16.28s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 12%|█▏        | 22/186 [05:24<44:32, 16.29s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 12%|█▏        | 23/186 [05:40<44:18, 16.31s/it] 13%|█▎        | 24/186 [05:57<44:23, 16.44s/it] 13%|█▎        | 25/186 [06:13<43:54, 16.36s/it] 14%|█▍        | 26/186 [06:29<43:26, 16.29s/it] 15%|█▍        | 27/186 [06:45<43:03, 16.25s/it] 15%|█▌        | 28/186 [07:01<42:43, 16.23s/it] 16%|█▌        | 29/186 [07:17<42:20, 16.18s/it] 16%|█▌        | 30/186 [07:34<42:22, 16.30s/it] 17%|█▋        | 31/186 [07:50<42:18, 16.38s/it] 17%|█▋        | 32/186 [08:07<42:06, 16.40s/it] 18%|█▊        | 33/186 [08:23<41:40, 16.35s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1263 > 1048). Running this sequence through the model will result in indexing errors
 18%|█▊        | 34/186 [08:40<41:52, 16.53s/it] 19%|█▉        | 35/186 [08:57<41:31, 16.50s/it] 19%|█▉        | 36/186 [09:13<40:54, 16.36s/it] 20%|█▉        | 37/186 [09:29<40:20, 16.24s/it] 20%|██        | 38/186 [09:45<40:04, 16.25s/it] 21%|██        | 39/186 [10:01<39:53, 16.28s/it] 22%|██▏       | 40/186 [10:18<39:40, 16.31s/it] 22%|██▏       | 41/186 [10:33<39:07, 16.19s/it] 23%|██▎       | 42/186 [10:51<39:28, 16.45s/it] 23%|██▎       | 43/186 [11:06<38:42, 16.24s/it] 24%|██▎       | 44/186 [11:22<37:59, 16.05s/it] 24%|██▍       | 45/186 [11:37<37:16, 15.86s/it] 25%|██▍       | 46/186 [11:53<36:50, 15.79s/it] 25%|██▌       | 47/186 [12:08<36:24, 15.72s/it] 26%|██▌       | 48/186 [12:25<36:23, 15.82s/it] 26%|██▋       | 49/186 [12:40<36:10, 15.84s/it] 27%|██▋       | 50/186 [12:51<32:30, 14.34s/it] 27%|██▋       | 51/186 [13:02<29:51, 13.27s/it] 28%|██▊       | 52/186 [13:13<28:06, 12.59s/it] 28%|██▊       | 53/186 [13:24<26:38, 12.02s/it] 29%|██▉       | 54/186 [13:35<25:38, 11.66s/it] 30%|██▉       | 55/186 [13:47<25:52, 11.85s/it] 30%|███       | 56/186 [13:58<25:25, 11.73s/it] 31%|███       | 57/186 [14:09<24:21, 11.33s/it] 31%|███       | 58/186 [14:20<23:53, 11.20s/it] 32%|███▏      | 59/186 [14:31<23:35, 11.15s/it] 32%|███▏      | 60/186 [14:42<23:29, 11.18s/it] 33%|███▎      | 61/186 [14:53<23:12, 11.14s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
 33%|███▎      | 62/186 [15:04<22:48, 11.04s/it] 34%|███▍      | 63/186 [15:15<22:31, 10.99s/it] 34%|███▍      | 64/186 [15:25<22:03, 10.85s/it] 35%|███▍      | 65/186 [15:36<21:42, 10.77s/it] 35%|███▌      | 66/186 [15:46<21:29, 10.75s/it] 36%|███▌      | 67/186 [15:57<21:18, 10.74s/it] 37%|███▋      | 68/186 [16:08<20:58, 10.66s/it] 37%|███▋      | 69/186 [16:19<21:06, 10.83s/it] 38%|███▊      | 70/186 [16:30<21:08, 10.93s/it] 38%|███▊      | 71/186 [16:41<20:47, 10.85s/it] 39%|███▊      | 72/186 [16:51<20:24, 10.74s/it] 39%|███▉      | 73/186 [17:02<20:10, 10.72s/it] 40%|███▉      | 74/186 [17:13<20:01, 10.73s/it] 40%|████      | 75/186 [17:23<19:49, 10.72s/it] 41%|████      | 76/186 [17:34<19:31, 10.65s/it] 41%|████▏     | 77/186 [17:44<19:19, 10.64s/it] 42%|████▏     | 78/186 [17:55<19:07, 10.62s/it] 42%|████▏     | 79/186 [18:06<19:03, 10.68s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1074 > 1048). Running this sequence through the model will result in indexing errors
 43%|████▎     | 80/186 [18:17<18:58, 10.74s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 44%|████▎     | 81/186 [18:27<18:46, 10.73s/it] 44%|████▍     | 82/186 [18:38<18:37, 10.74s/it] 45%|████▍     | 83/186 [18:49<18:26, 10.74s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 45%|████▌     | 84/186 [18:59<18:08, 10.67s/it] 46%|████▌     | 85/186 [19:10<17:54, 10.63s/it] 46%|████▌     | 86/186 [19:21<17:49, 10.69s/it] 47%|████▋     | 87/186 [19:31<17:38, 10.69s/it] 47%|████▋     | 88/186 [19:42<17:30, 10.72s/it] 48%|████▊     | 89/186 [19:53<17:20, 10.73s/it] 48%|████▊     | 90/186 [20:04<17:18, 10.82s/it] 49%|████▉     | 91/186 [20:15<17:09, 10.84s/it] 49%|████▉     | 92/186 [20:26<17:01, 10.87s/it] 50%|█████     | 93/186 [20:37<16:54, 10.91s/it] 51%|█████     | 94/186 [20:47<16:37, 10.85s/it] 51%|█████     | 95/186 [20:58<16:24, 10.82s/it] 52%|█████▏    | 96/186 [21:09<16:13, 10.82s/it] 52%|█████▏    | 97/186 [21:20<15:59, 10.78s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 53%|█████▎    | 98/186 [21:31<15:51, 10.81s/it] 53%|█████▎    | 99/186 [21:42<15:42, 10.83s/it] 54%|█████▍    | 100/186 [21:53<15:35, 10.88s/it] 54%|█████▍    | 101/186 [22:03<15:25, 10.89s/it] 55%|█████▍    | 102/186 [22:14<15:10, 10.83s/it] 55%|█████▌    | 103/186 [22:25<15:04, 10.90s/it] 56%|█████▌    | 104/186 [22:36<14:49, 10.85s/it] 56%|█████▋    | 105/186 [22:47<14:44, 10.92s/it] 57%|█████▋    | 106/186 [22:58<14:26, 10.83s/it] 58%|█████▊    | 107/186 [23:09<14:21, 10.91s/it] 58%|█████▊    | 108/186 [23:19<14:06, 10.86s/it] 59%|█████▊    | 109/186 [23:30<13:52, 10.81s/it] 59%|█████▉    | 110/186 [23:41<13:36, 10.74s/it] 60%|█████▉    | 111/186 [23:52<13:30, 10.80s/it] 60%|██████    | 112/186 [24:02<13:14, 10.73s/it] 61%|██████    | 113/186 [24:13<13:01, 10.71s/it] 61%|██████▏   | 114/186 [24:24<13:00, 10.85s/it] 62%|██████▏   | 115/186 [24:35<12:49, 10.83s/it] 62%|██████▏   | 116/186 [24:46<12:40, 10.87s/it] 63%|██████▎   | 117/186 [24:57<12:27, 10.84s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 63%|██████▎   | 118/186 [25:07<12:15, 10.82s/it] 64%|██████▍   | 119/186 [25:18<12:01, 10.77s/it] 65%|██████▍   | 120/186 [25:29<11:52, 10.79s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1153 > 1048). Running this sequence through the model will result in indexing errors
 65%|██████▌   | 121/186 [25:40<11:43, 10.82s/it] 66%|██████▌   | 122/186 [25:51<11:39, 10.93s/it] 66%|██████▌   | 123/186 [26:02<11:23, 10.84s/it] 67%|██████▋   | 124/186 [26:12<11:09, 10.80s/it] 67%|██████▋   | 125/186 [26:23<10:56, 10.76s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 68%|██████▊   | 126/186 [26:34<10:47, 10.78s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1263 > 1048). Running this sequence through the model will result in indexing errors
 68%|██████▊   | 127/186 [26:45<10:45, 10.93s/it] 69%|██████▉   | 128/186 [26:56<10:30, 10.87s/it] 69%|██████▉   | 129/186 [27:07<10:21, 10.91s/it] 70%|██████▉   | 130/186 [27:17<10:05, 10.82s/it] 70%|███████   | 131/186 [27:28<09:51, 10.75s/it] 71%|███████   | 132/186 [27:39<09:37, 10.69s/it] 72%|███████▏  | 133/186 [27:49<09:27, 10.70s/it] 72%|███████▏  | 134/186 [28:00<09:17, 10.72s/it] 73%|███████▎  | 135/186 [28:11<09:08, 10.75s/it] 73%|███████▎  | 136/186 [28:22<08:57, 10.76s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 74%|███████▎  | 137/186 [28:33<08:51, 10.85s/it] 74%|███████▍  | 138/186 [28:43<08:39, 10.82s/it] 75%|███████▍  | 139/186 [28:55<08:37, 11.01s/it] 75%|███████▌  | 140/186 [29:06<08:26, 11.02s/it] 76%|███████▌  | 141/186 [29:17<08:13, 10.97s/it] 76%|███████▋  | 142/186 [29:28<07:59, 10.90s/it] 77%|███████▋  | 143/186 [29:38<07:47, 10.88s/it] 77%|███████▋  | 144/186 [29:49<07:36, 10.87s/it] 78%|███████▊  | 145/186 [30:00<07:24, 10.83s/it] 78%|███████▊  | 146/186 [30:11<07:12, 10.80s/it] 79%|███████▉  | 147/186 [30:22<07:02, 10.83s/it] 80%|███████▉  | 148/186 [30:32<06:51, 10.82s/it] 80%|████████  | 149/186 [30:43<06:40, 10.81s/it] 81%|████████  | 150/186 [30:54<06:27, 10.76s/it] 81%|████████  | 151/186 [31:04<06:15, 10.72s/it] 82%|████████▏ | 152/186 [31:15<06:05, 10.76s/it] 82%|████████▏ | 153/186 [31:26<05:54, 10.73s/it] 83%|████████▎ | 154/186 [31:37<05:46, 10.84s/it] 83%|████████▎ | 155/186 [31:48<05:40, 10.97s/it]WARNING: tokenization mismatch: 1 vs. 47. (ignored)
 84%|████████▍ | 156/186 [31:59<05:27, 10.91s/it] 84%|████████▍ | 157/186 [32:10<05:15, 10.87s/it] 85%|████████▍ | 158/186 [32:21<05:03, 10.85s/it] 85%|████████▌ | 159/186 [32:31<04:52, 10.84s/it] 86%|████████▌ | 160/186 [32:42<04:42, 10.87s/it] 87%|████████▋ | 161/186 [32:53<04:31, 10.86s/it] 87%|████████▋ | 162/186 [33:05<04:25, 11.06s/it] 88%|████████▊ | 163/186 [33:16<04:12, 10.97s/it] 88%|████████▊ | 164/186 [33:27<04:03, 11.08s/it] 89%|████████▊ | 165/186 [33:38<03:52, 11.09s/it] 89%|████████▉ | 166/186 [33:49<03:39, 10.97s/it] 90%|████████▉ | 167/186 [33:59<03:27, 10.92s/it] 90%|█████████ | 168/186 [34:10<03:16, 10.89s/it] 91%|█████████ | 169/186 [34:21<03:06, 10.94s/it] 91%|█████████▏| 170/186 [34:32<02:55, 10.95s/it] 92%|█████████▏| 171/186 [34:44<02:45, 11.02s/it] 92%|█████████▏| 172/186 [34:55<02:34, 11.02s/it] 93%|█████████▎| 173/186 [35:06<02:23, 11.01s/it] 94%|█████████▎| 174/186 [35:16<02:11, 10.93s/it] 94%|█████████▍| 175/186 [35:27<01:59, 10.88s/it] 95%|█████████▍| 176/186 [35:38<01:49, 10.91s/it] 95%|█████████▌| 177/186 [35:49<01:38, 10.91s/it] 96%|█████████▌| 178/186 [36:00<01:27, 10.88s/it] 96%|█████████▌| 179/186 [36:11<01:15, 10.86s/it] 97%|█████████▋| 180/186 [36:21<01:04, 10.81s/it] 97%|█████████▋| 181/186 [36:32<00:54, 10.80s/it] 98%|█████████▊| 182/186 [36:43<00:43, 10.95s/it] 98%|█████████▊| 183/186 [36:54<00:32, 11.00s/it] 99%|█████████▉| 184/186 [37:05<00:21, 10.94s/it] 99%|█████████▉| 185/186 [37:17<00:11, 11.07s/it]100%|██████████| 186/186 [37:28<00:00, 11.07s/it]                                                 {'train_runtime': 2248.1649, 'train_samples_per_second': 5.976, 'train_steps_per_second': 0.083, 'train_loss': 0.14118402747697728, 'epoch': 1.0}
100%|██████████| 186/186 [37:28<00:00, 11.07s/it]100%|██████████| 186/186 [37:28<00:00, 12.09s/it]
[2025-10-17 18:47:54,253] [INFO] [launch.py:347:main] Process 2603982 exits successfully.
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-4
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33msliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-4[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251017_180838-1ncgq6jx/logs[0m
[2025-10-17 18:47:58,257] [INFO] [launch.py:347:main] Process 2603981 exits successfully.
[2025-10-17 18:47:59,259] [INFO] [launch.py:347:main] Process 2603979 exits successfully.
[2025-10-17 18:47:59,260] [INFO] [launch.py:347:main] Process 2603983 exits successfully.
[2025-10-17 18:47:59,260] [INFO] [launch.py:347:main] Process 2603980 exits successfully.
[2025-10-17 18:48:04,266] [INFO] [launch.py:347:main] Process 2603978 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 18:48:07,776] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 18:48:08,331] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-17 18:48:08,331] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12423 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-5 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-5 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 0.5 --use_anchor False --use_sample_weight True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 18:48:09,801] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 18:48:10,404] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-17 18:48:10,404] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-17 18:48:10,404] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-17 18:48:10,404] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-17 18:48:10,404] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 18:48:14,661] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 18:48:14,666] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-17 18:48:14,826] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 18:48:14,979] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-17 18:48:15,004] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 18:48:15,038] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 18:48:15,152] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 18:48:15,359] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 18:48:15,359] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-10-17 18:48:15,797] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 18:48:16,112] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 18:48:16,143] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
[2025-10-17 18:48:16,598] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251017_184816-fy6ygwk0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-5
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/fy6ygwk0
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.07s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.91s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.65s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.20s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.61s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.64s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.96s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.93s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.55s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.75s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.89s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.74s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.07s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.58s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.88s/it]
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.65s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.95s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.21s/it]
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.69s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.05s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.48s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.44s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.83s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.30s/it]
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:5')
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.24s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.66s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.05s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.79s/it]
Formatting inputs...Skip in lazy mode
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:3')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:4')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:1')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:2')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:0')
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.01201176643371582 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...

Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.01000213623046875 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.010907411575317383 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.1011815071105957 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10147285461425781 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10437822341918945 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/186 [00:09<30:39,  9.94s/it]  1%|          | 2/186 [00:19<28:55,  9.43s/it]  2%|▏         | 3/186 [00:28<29:09,  9.56s/it]  2%|▏         | 4/186 [00:38<29:35,  9.75s/it]  3%|▎         | 5/186 [00:48<29:43,  9.85s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1245 > 1048). Running this sequence through the model will result in indexing errors
  3%|▎         | 6/186 [00:58<29:43,  9.91s/it]  4%|▍         | 7/186 [01:09<30:04, 10.08s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
Token indices sequence length is longer than the specified maximum sequence length for this model (1165 > 1048). Running this sequence through the model will result in indexing errors
  4%|▍         | 8/186 [01:19<30:21, 10.23s/it]  5%|▍         | 9/186 [01:30<30:11, 10.23s/it]  5%|▌         | 10/186 [01:40<30:16, 10.32s/it]  6%|▌         | 11/186 [01:50<30:10, 10.35s/it]  6%|▋         | 12/186 [02:01<30:01, 10.35s/it]  7%|▋         | 13/186 [02:11<29:46, 10.33s/it]  8%|▊         | 14/186 [02:22<29:42, 10.37s/it]  8%|▊         | 15/186 [02:32<29:34, 10.38s/it]  9%|▊         | 16/186 [02:42<29:22, 10.37s/it]  9%|▉         | 17/186 [02:53<29:06, 10.34s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 10%|▉         | 18/186 [03:03<29:03, 10.38s/it] 10%|█         | 19/186 [03:13<28:50, 10.36s/it] 11%|█         | 20/186 [03:24<28:41, 10.37s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 11%|█▏        | 21/186 [03:34<28:26, 10.34s/it] 12%|█▏        | 22/186 [03:44<28:17, 10.35s/it] 12%|█▏        | 23/186 [03:55<28:06, 10.35s/it] 13%|█▎        | 24/186 [04:06<28:16, 10.47s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1080 > 1048). Running this sequence through the model will result in indexing errors
 13%|█▎        | 25/186 [04:16<28:21, 10.57s/it] 14%|█▍        | 26/186 [04:27<28:16, 10.60s/it] 15%|█▍        | 27/186 [04:38<28:24, 10.72s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1073 > 1048). Running this sequence through the model will result in indexing errors
 15%|█▌        | 28/186 [04:49<28:10, 10.70s/it] 16%|█▌        | 29/186 [04:59<28:05, 10.74s/it] 16%|█▌        | 30/186 [05:10<27:48, 10.70s/it] 17%|█▋        | 31/186 [05:21<27:48, 10.77s/it] 17%|█▋        | 32/186 [05:32<27:31, 10.73s/it] 18%|█▊        | 33/186 [05:42<27:15, 10.69s/it] 18%|█▊        | 34/186 [05:53<27:00, 10.66s/it]WARNING: tokenization mismatch: 1 vs. 47. (ignored)
 19%|█▉        | 35/186 [06:04<27:04, 10.76s/it] 19%|█▉        | 36/186 [06:15<26:51, 10.74s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1075 > 1048). Running this sequence through the model will result in indexing errors
 20%|█▉        | 37/186 [06:25<26:47, 10.79s/it] 20%|██        | 38/186 [06:36<26:27, 10.73s/it] 21%|██        | 39/186 [06:47<26:22, 10.77s/it] 22%|██▏       | 40/186 [06:58<26:14, 10.78s/it] 22%|██▏       | 41/186 [07:09<26:09, 10.82s/it] 23%|██▎       | 42/186 [07:19<25:55, 10.80s/it] 23%|██▎       | 43/186 [07:30<25:35, 10.73s/it] 24%|██▎       | 44/186 [07:40<25:15, 10.67s/it] 24%|██▍       | 45/186 [07:51<25:05, 10.68s/it]WARNING: tokenization mismatch: 1 vs. 48. (ignored)
 25%|██▍       | 46/186 [08:02<24:54, 10.67s/it] 25%|██▌       | 47/186 [08:13<24:52, 10.74s/it] 26%|██▌       | 48/186 [08:23<24:34, 10.69s/it] 26%|██▋       | 49/186 [08:34<24:21, 10.67s/it] 27%|██▋       | 50/186 [08:44<24:05, 10.63s/it] 27%|██▋       | 51/186 [08:55<23:58, 10.65s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
 28%|██▊       | 52/186 [09:06<23:54, 10.70s/it] 28%|██▊       | 53/186 [09:17<23:45, 10.72s/it] 29%|██▉       | 54/186 [09:28<23:42, 10.78s/it] 30%|██▉       | 55/186 [09:38<23:30, 10.77s/it] 30%|███       | 56/186 [09:49<23:18, 10.76s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 31%|███       | 57/186 [10:00<23:14, 10.81s/it] 31%|███       | 58/186 [10:11<22:58, 10.77s/it] 32%|███▏      | 59/186 [10:22<22:49, 10.78s/it] 32%|███▏      | 60/186 [10:32<22:30, 10.72s/it] 33%|███▎      | 61/186 [10:43<22:25, 10.76s/it] 33%|███▎      | 62/186 [10:54<22:13, 10.75s/it] 34%|███▍      | 63/186 [11:04<22:02, 10.75s/it] 34%|███▍      | 64/186 [11:15<21:55, 10.78s/it] 35%|███▍      | 65/186 [11:26<21:41, 10.76s/it] 35%|███▌      | 66/186 [11:37<21:39, 10.83s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1221 > 1048). Running this sequence through the model will result in indexing errors
 36%|███▌      | 67/186 [11:48<21:33, 10.87s/it] 37%|███▋      | 68/186 [11:58<21:11, 10.78s/it] 37%|███▋      | 69/186 [12:10<21:09, 10.85s/it] 38%|███▊      | 70/186 [12:21<21:05, 10.91s/it] 38%|███▊      | 71/186 [12:31<20:50, 10.88s/it] 39%|███▊      | 72/186 [12:42<20:40, 10.88s/it] 39%|███▉      | 73/186 [12:53<20:30, 10.89s/it] 40%|███▉      | 74/186 [13:04<20:19, 10.89s/it] 40%|████      | 75/186 [13:15<20:08, 10.89s/it] 41%|████      | 76/186 [13:26<19:48, 10.81s/it] 41%|████▏     | 77/186 [13:36<19:33, 10.77s/it] 42%|████▏     | 78/186 [13:47<19:24, 10.78s/it] 42%|████▏     | 79/186 [13:58<19:21, 10.86s/it] 43%|████▎     | 80/186 [14:09<19:07, 10.83s/it] 44%|████▎     | 81/186 [14:20<19:06, 10.92s/it] 44%|████▍     | 82/186 [14:31<18:55, 10.92s/it] 45%|████▍     | 83/186 [14:42<18:49, 10.97s/it] 45%|████▌     | 84/186 [14:53<18:28, 10.87s/it] 46%|████▌     | 85/186 [15:03<18:13, 10.83s/it] 46%|████▌     | 86/186 [15:14<18:02, 10.82s/it] 47%|████▋     | 87/186 [15:25<17:47, 10.78s/it] 47%|████▋     | 88/186 [15:36<17:40, 10.82s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 48%|████▊     | 89/186 [15:47<17:32, 10.86s/it] 48%|████▊     | 90/186 [15:57<17:19, 10.83s/it] 49%|████▉     | 91/186 [16:09<17:15, 10.90s/it] 49%|████▉     | 92/186 [16:20<17:12, 10.98s/it] 50%|█████     | 93/186 [16:31<17:08, 11.05s/it] 51%|█████     | 94/186 [16:42<16:53, 11.01s/it] 51%|█████     | 95/186 [16:53<16:38, 10.98s/it] 52%|█████▏    | 96/186 [17:04<16:34, 11.05s/it] 52%|█████▏    | 97/186 [17:15<16:19, 11.00s/it] 53%|█████▎    | 98/186 [17:26<16:01, 10.92s/it] 53%|█████▎    | 99/186 [17:36<15:46, 10.88s/it] 54%|█████▍    | 100/186 [17:47<15:40, 10.93s/it] 54%|█████▍    | 101/186 [17:58<15:29, 10.94s/it] 55%|█████▍    | 102/186 [18:09<15:13, 10.88s/it] 55%|█████▌    | 103/186 [18:20<15:13, 11.00s/it] 56%|█████▌    | 104/186 [18:31<15:02, 11.00s/it] 56%|█████▋    | 105/186 [18:42<14:51, 11.00s/it] 57%|█████▋    | 106/186 [18:53<14:39, 11.00s/it] 58%|█████▊    | 107/186 [19:04<14:27, 10.98s/it] 58%|█████▊    | 108/186 [19:15<14:10, 10.90s/it] 59%|█████▊    | 109/186 [19:26<13:58, 10.90s/it] 59%|█████▉    | 110/186 [19:36<13:40, 10.79s/it] 60%|█████▉    | 111/186 [19:47<13:29, 10.79s/it] 60%|██████    | 112/186 [19:58<13:18, 10.80s/it] 61%|██████    | 113/186 [20:09<13:09, 10.82s/it] 61%|██████▏   | 114/186 [20:20<13:04, 10.90s/it] 62%|██████▏   | 115/186 [20:31<12:53, 10.89s/it] 62%|██████▏   | 116/186 [20:42<12:52, 11.03s/it] 63%|██████▎   | 117/186 [20:53<12:40, 11.02s/it] 63%|██████▎   | 118/186 [21:04<12:28, 11.01s/it] 64%|██████▍   | 119/186 [21:15<12:09, 10.89s/it] 65%|██████▍   | 120/186 [21:26<11:55, 10.83s/it] 65%|██████▌   | 121/186 [21:36<11:41, 10.79s/it] 66%|██████▌   | 122/186 [21:47<11:35, 10.86s/it] 66%|██████▌   | 123/186 [21:58<11:31, 10.97s/it] 67%|██████▋   | 124/186 [22:09<11:16, 10.91s/it] 67%|██████▋   | 125/186 [22:20<11:06, 10.92s/it] 68%|██████▊   | 126/186 [22:31<10:54, 10.91s/it] 68%|██████▊   | 127/186 [22:42<10:40, 10.86s/it] 69%|██████▉   | 128/186 [22:53<10:27, 10.82s/it] 69%|██████▉   | 129/186 [23:04<10:19, 10.87s/it] 70%|██████▉   | 130/186 [23:15<10:12, 10.93s/it] 70%|███████   | 131/186 [23:25<09:59, 10.90s/it] 71%|███████   | 132/186 [23:37<09:59, 11.10s/it] 72%|███████▏  | 133/186 [23:48<09:45, 11.05s/it] 72%|███████▏  | 134/186 [23:59<09:35, 11.07s/it] 73%|███████▎  | 135/186 [24:10<09:21, 11.01s/it] 73%|███████▎  | 136/186 [24:21<09:09, 10.98s/it] 74%|███████▎  | 137/186 [24:32<09:04, 11.11s/it] 74%|███████▍  | 138/186 [24:43<08:46, 10.98s/it] 75%|███████▍  | 139/186 [24:54<08:38, 11.03s/it] 75%|███████▌  | 140/186 [25:05<08:24, 10.97s/it] 76%|███████▌  | 141/186 [25:16<08:12, 10.95s/it] 76%|███████▋  | 142/186 [25:27<08:02, 10.97s/it] 77%|███████▋  | 143/186 [25:38<07:48, 10.89s/it] 77%|███████▋  | 144/186 [25:49<07:39, 10.93s/it] 78%|███████▊  | 145/186 [26:00<07:28, 10.94s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 78%|███████▊  | 146/186 [26:10<07:13, 10.84s/it] 79%|███████▉  | 147/186 [26:21<07:06, 10.93s/it] 80%|███████▉  | 148/186 [26:32<06:56, 10.95s/it] 80%|████████  | 149/186 [26:43<06:42, 10.88s/it] 81%|████████  | 150/186 [26:54<06:33, 10.94s/it] 81%|████████  | 151/186 [27:05<06:21, 10.91s/it] 82%|████████▏ | 152/186 [27:16<06:11, 10.93s/it] 82%|████████▏ | 153/186 [27:27<05:58, 10.87s/it] 83%|████████▎ | 154/186 [27:37<05:46, 10.84s/it] 83%|████████▎ | 155/186 [27:48<05:37, 10.88s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 84%|████████▍ | 156/186 [27:59<05:27, 10.93s/it] 84%|████████▍ | 157/186 [28:10<05:15, 10.87s/it] 85%|████████▍ | 158/186 [28:21<05:03, 10.82s/it] 85%|████████▌ | 159/186 [28:32<04:52, 10.83s/it] 86%|████████▌ | 160/186 [28:42<04:40, 10.80s/it] 87%|████████▋ | 161/186 [28:53<04:29, 10.77s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
 87%|████████▋ | 162/186 [29:04<04:19, 10.83s/it] 88%|████████▊ | 163/186 [29:15<04:10, 10.91s/it] 88%|████████▊ | 164/186 [29:26<03:59, 10.88s/it] 89%|████████▊ | 165/186 [29:37<03:49, 10.91s/it] 89%|████████▉ | 166/186 [29:48<03:38, 10.93s/it] 90%|████████▉ | 167/186 [29:59<03:29, 11.01s/it] 90%|█████████ | 168/186 [30:10<03:17, 10.99s/it] 91%|█████████ | 169/186 [30:21<03:05, 10.92s/it] 91%|█████████▏| 170/186 [30:32<02:53, 10.85s/it] 92%|█████████▏| 171/186 [30:43<02:43, 10.91s/it] 92%|█████████▏| 172/186 [30:54<02:32, 10.92s/it] 93%|█████████▎| 173/186 [31:04<02:21, 10.91s/it] 94%|█████████▎| 174/186 [31:15<02:10, 10.87s/it] 94%|█████████▍| 175/186 [31:26<02:00, 10.92s/it] 95%|█████████▍| 176/186 [31:37<01:49, 10.93s/it] 95%|█████████▌| 177/186 [31:48<01:38, 10.92s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 96%|█████████▌| 178/186 [31:59<01:27, 10.92s/it] 96%|█████████▌| 179/186 [32:10<01:16, 10.88s/it] 97%|█████████▋| 180/186 [32:21<01:05, 10.93s/it] 97%|█████████▋| 181/186 [32:32<00:54, 10.95s/it] 98%|█████████▊| 182/186 [32:43<00:43, 10.89s/it] 98%|█████████▊| 183/186 [32:54<00:32, 10.92s/it] 99%|█████████▉| 184/186 [33:04<00:21, 10.90s/it] 99%|█████████▉| 185/186 [33:16<00:10, 10.95s/it]100%|██████████| 186/186 [33:27<00:00, 10.98s/it]                                                 {'train_runtime': 2007.0458, 'train_samples_per_second': 6.694, 'train_steps_per_second': 0.093, 'train_loss': 0.14729851548389722, 'epoch': 1.0}
100%|██████████| 186/186 [33:27<00:00, 10.98s/it]100%|██████████| 186/186 [33:27<00:00, 10.79s/it]
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-5
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33msliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-5[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251017_184816-fy6ygwk0/logs[0m
[2025-10-17 19:23:27,890] [INFO] [launch.py:347:main] Process 2632911 exits successfully.
[2025-10-17 19:23:27,891] [INFO] [launch.py:347:main] Process 2632912 exits successfully.
[2025-10-17 19:23:28,892] [INFO] [launch.py:347:main] Process 2632914 exits successfully.
[2025-10-17 19:23:28,892] [INFO] [launch.py:347:main] Process 2632915 exits successfully.
[2025-10-17 19:23:28,892] [INFO] [launch.py:347:main] Process 2632913 exits successfully.
[2025-10-17 19:23:32,897] [INFO] [launch.py:347:main] Process 2632910 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 19:23:36,149] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 19:23:36,703] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-17 19:23:36,704] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12423 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-5 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-5 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 1 --use_anchor False --use_sample_weight True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 19:23:38,381] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 19:23:38,971] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-17 19:23:38,971] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-17 19:23:38,971] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-17 19:23:38,971] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-17 19:23:38,971] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 19:23:42,659] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 19:23:42,764] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 19:23:42,974] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 19:23:43,072] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 19:23:43,203] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 19:23:43,519] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 19:23:43,856] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-17 19:23:44,017] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 19:23:44,162] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 19:23:44,336] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 19:23:44,336] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-10-17 19:23:44,548] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 19:23:44,873] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: creating run
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251017_192345-wnh8z5zy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-5
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/wnh8z5zy
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.26s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.53s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.79s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.38s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.63s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.77s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.94s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.86s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.87s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.63s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.95s/it]
to bfloat16...
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.57s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.91s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.27s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.52s/it]
to bfloat16...
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.76s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.98s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.55s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.88s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.74s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.08s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.88s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.87s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.83s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.64s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.96s/it]
Formatting inputs...Skip in lazy mode
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:1')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:3')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:5')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:2')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:0')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:4')
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.009462833404541016 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.010111808776855469 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.013183116912841797 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.01271820068359375 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.1023094654083252 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10292506217956543 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/186 [00:09<29:58,  9.72s/it]  1%|          | 2/186 [00:18<28:19,  9.24s/it]  2%|▏         | 3/186 [00:27<28:14,  9.26s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1285 > 1048). Running this sequence through the model will result in indexing errors
  2%|▏         | 4/186 [00:37<28:32,  9.41s/it]  3%|▎         | 5/186 [00:47<28:38,  9.49s/it]  3%|▎         | 6/186 [00:56<28:45,  9.58s/it]  4%|▍         | 7/186 [01:07<29:07,  9.76s/it]  4%|▍         | 8/186 [01:17<29:28,  9.94s/it]  5%|▍         | 9/186 [01:27<29:44, 10.08s/it]  5%|▌         | 10/186 [01:38<29:46, 10.15s/it]  6%|▌         | 11/186 [01:48<29:42, 10.19s/it]  6%|▋         | 12/186 [01:58<29:46, 10.27s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
  7%|▋         | 13/186 [02:09<29:38, 10.28s/it]  8%|▊         | 14/186 [02:19<29:28, 10.28s/it]  8%|▊         | 15/186 [02:29<29:31, 10.36s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1240 > 1048). Running this sequence through the model will result in indexing errors
  9%|▊         | 16/186 [02:40<29:33, 10.43s/it]  9%|▉         | 17/186 [02:51<29:33, 10.50s/it] 10%|▉         | 18/186 [03:01<29:14, 10.44s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1074 > 1048). Running this sequence through the model will result in indexing errors
 10%|█         | 19/186 [03:12<29:18, 10.53s/it] 11%|█         | 20/186 [03:23<29:19, 10.60s/it] 11%|█▏        | 21/186 [03:33<29:02, 10.56s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1160 > 1048). Running this sequence through the model will result in indexing errors
 12%|█▏        | 22/186 [03:44<28:51, 10.56s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 12%|█▏        | 23/186 [03:54<28:46, 10.59s/it] 13%|█▎        | 24/186 [04:05<28:51, 10.69s/it] 13%|█▎        | 25/186 [04:16<28:26, 10.60s/it] 14%|█▍        | 26/186 [04:26<28:12, 10.58s/it] 15%|█▍        | 27/186 [04:37<28:01, 10.58s/it] 15%|█▌        | 28/186 [04:47<27:56, 10.61s/it] 16%|█▌        | 29/186 [04:58<27:59, 10.69s/it] 16%|█▌        | 30/186 [05:09<27:34, 10.61s/it] 17%|█▋        | 31/186 [05:19<27:21, 10.59s/it] 17%|█▋        | 32/186 [05:30<27:10, 10.59s/it] 18%|█▊        | 33/186 [05:40<27:01, 10.60s/it] 18%|█▊        | 34/186 [05:51<27:04, 10.69s/it] 19%|█▉        | 35/186 [06:02<26:45, 10.64s/it] 19%|█▉        | 36/186 [06:12<26:38, 10.66s/it] 20%|█▉        | 37/186 [06:23<26:30, 10.68s/it] 20%|██        | 38/186 [06:34<26:11, 10.62s/it] 21%|██        | 39/186 [06:44<26:03, 10.63s/it] 22%|██▏       | 40/186 [06:55<25:59, 10.68s/it] 22%|██▏       | 41/186 [07:06<25:46, 10.67s/it] 23%|██▎       | 42/186 [07:16<25:31, 10.63s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 23%|██▎       | 43/186 [07:27<25:22, 10.65s/it] 24%|██▎       | 44/186 [07:38<25:05, 10.60s/it] 24%|██▍       | 45/186 [07:49<25:14, 10.74s/it] 25%|██▍       | 46/186 [07:59<25:02, 10.73s/it] 25%|██▌       | 47/186 [08:10<25:01, 10.80s/it] 26%|██▌       | 48/186 [08:21<24:45, 10.76s/it] 26%|██▋       | 49/186 [08:32<24:30, 10.73s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
Token indices sequence length is longer than the specified maximum sequence length for this model (1297 > 1048). Running this sequence through the model will result in indexing errors
 27%|██▋       | 50/186 [08:43<24:29, 10.81s/it] 27%|██▋       | 51/186 [08:54<24:28, 10.88s/it] 28%|██▊       | 52/186 [09:04<24:04, 10.78s/it] 28%|██▊       | 53/186 [09:15<24:05, 10.87s/it] 29%|██▉       | 54/186 [09:26<24:01, 10.92s/it] 30%|██▉       | 55/186 [09:37<23:46, 10.89s/it] 30%|███       | 56/186 [09:48<23:26, 10.82s/it] 31%|███       | 57/186 [09:59<23:18, 10.84s/it] 31%|███       | 58/186 [10:09<23:02, 10.80s/it] 32%|███▏      | 59/186 [10:20<22:51, 10.80s/it] 32%|███▏      | 60/186 [10:31<22:36, 10.77s/it] 33%|███▎      | 61/186 [10:42<22:23, 10.74s/it] 33%|███▎      | 62/186 [10:52<22:04, 10.68s/it] 34%|███▍      | 63/186 [11:03<22:07, 10.79s/it] 34%|███▍      | 64/186 [11:14<21:49, 10.74s/it] 35%|███▍      | 65/186 [11:24<21:32, 10.68s/it] 35%|███▌      | 66/186 [11:35<21:35, 10.80s/it] 36%|███▌      | 67/186 [11:46<21:26, 10.81s/it] 37%|███▋      | 68/186 [11:57<21:14, 10.80s/it] 37%|███▋      | 69/186 [12:08<21:10, 10.86s/it] 38%|███▊      | 70/186 [12:19<21:05, 10.91s/it] 38%|███▊      | 71/186 [12:30<20:50, 10.87s/it] 39%|███▊      | 72/186 [12:40<20:33, 10.82s/it] 39%|███▉      | 73/186 [12:51<20:15, 10.75s/it] 40%|███▉      | 74/186 [13:02<20:04, 10.76s/it] 40%|████      | 75/186 [13:12<19:50, 10.72s/it] 41%|████      | 76/186 [13:23<19:33, 10.67s/it] 41%|████▏     | 77/186 [13:33<19:15, 10.60s/it] 42%|████▏     | 78/186 [13:44<19:01, 10.57s/it] 42%|████▏     | 79/186 [13:55<18:50, 10.57s/it] 43%|████▎     | 80/186 [14:05<18:44, 10.61s/it] 44%|████▎     | 81/186 [14:16<18:43, 10.70s/it] 44%|████▍     | 82/186 [14:27<18:36, 10.74s/it] 45%|████▍     | 83/186 [14:38<18:25, 10.73s/it] 45%|████▌     | 84/186 [14:48<18:11, 10.70s/it] 46%|████▌     | 85/186 [14:59<18:08, 10.78s/it] 46%|████▌     | 86/186 [15:10<17:54, 10.74s/it] 47%|████▋     | 87/186 [15:21<17:40, 10.71s/it] 47%|████▋     | 88/186 [15:31<17:29, 10.71s/it] 48%|████▊     | 89/186 [15:42<17:19, 10.72s/it] 48%|████▊     | 90/186 [15:53<17:09, 10.73s/it] 49%|████▉     | 91/186 [16:04<17:05, 10.79s/it] 49%|████▉     | 92/186 [16:15<16:54, 10.79s/it] 50%|█████     | 93/186 [16:26<16:52, 10.89s/it] 51%|█████     | 94/186 [16:36<16:35, 10.82s/it] 51%|█████     | 95/186 [16:47<16:23, 10.80s/it] 52%|█████▏    | 96/186 [16:58<16:09, 10.78s/it] 52%|█████▏    | 97/186 [17:08<15:54, 10.73s/it] 53%|█████▎    | 98/186 [17:19<15:41, 10.70s/it] 53%|█████▎    | 99/186 [17:30<15:31, 10.71s/it] 54%|█████▍    | 100/186 [17:40<15:21, 10.71s/it] 54%|█████▍    | 101/186 [17:51<15:13, 10.74s/it] 55%|█████▍    | 102/186 [18:02<14:56, 10.67s/it] 55%|█████▌    | 103/186 [18:13<14:52, 10.75s/it] 56%|█████▌    | 104/186 [18:24<14:44, 10.79s/it] 56%|█████▋    | 105/186 [18:34<14:32, 10.77s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 57%|█████▋    | 106/186 [18:45<14:27, 10.84s/it] 58%|█████▊    | 107/186 [18:56<14:15, 10.82s/it] 58%|█████▊    | 108/186 [19:07<14:09, 10.90s/it] 59%|█████▊    | 109/186 [19:18<14:00, 10.91s/it] 59%|█████▉    | 110/186 [19:29<13:44, 10.84s/it]WARNING: tokenization mismatch: 1 vs. 51. (ignored)
 60%|█████▉    | 111/186 [19:39<13:28, 10.77s/it] 60%|██████    | 112/186 [19:50<13:21, 10.83s/it] 61%|██████    | 113/186 [20:01<13:11, 10.85s/it] 61%|██████▏   | 114/186 [20:12<12:55, 10.78s/it] 62%|██████▏   | 115/186 [20:22<12:40, 10.72s/it] 62%|██████▏   | 116/186 [20:34<12:41, 10.88s/it] 63%|██████▎   | 117/186 [20:44<12:27, 10.83s/it] 63%|██████▎   | 118/186 [20:55<12:16, 10.83s/it] 64%|██████▍   | 119/186 [21:06<12:09, 10.88s/it] 65%|██████▍   | 120/186 [21:17<12:01, 10.93s/it] 65%|██████▌   | 121/186 [21:28<11:50, 10.92s/it] 66%|██████▌   | 122/186 [21:39<11:38, 10.91s/it] 66%|██████▌   | 123/186 [21:50<11:28, 10.93s/it] 67%|██████▋   | 124/186 [22:01<11:13, 10.87s/it] 67%|██████▋   | 125/186 [22:12<11:00, 10.83s/it] 68%|██████▊   | 126/186 [22:22<10:49, 10.83s/it] 68%|██████▊   | 127/186 [22:33<10:39, 10.83s/it] 69%|██████▉   | 128/186 [22:44<10:29, 10.85s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1240 > 1048). Running this sequence through the model will result in indexing errors
 69%|██████▉   | 129/186 [22:55<10:20, 10.89s/it] 70%|██████▉   | 130/186 [23:06<10:10, 10.90s/it] 70%|███████   | 131/186 [23:17<09:57, 10.86s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 71%|███████   | 132/186 [23:28<09:48, 10.89s/it] 72%|███████▏  | 133/186 [23:38<09:34, 10.84s/it] 72%|███████▏  | 134/186 [23:49<09:26, 10.89s/it] 73%|███████▎  | 135/186 [24:01<09:18, 10.95s/it] 73%|███████▎  | 136/186 [24:11<09:06, 10.93s/it] 74%|███████▎  | 137/186 [24:22<08:55, 10.93s/it] 74%|███████▍  | 138/186 [24:33<08:46, 10.96s/it] 75%|███████▍  | 139/186 [24:45<08:38, 11.03s/it]WARNING: tokenization mismatch: 1 vs. 56. (ignored)
 75%|███████▌  | 140/186 [24:55<08:25, 10.98s/it] 76%|███████▌  | 141/186 [25:06<08:11, 10.93s/it] 76%|███████▋  | 142/186 [25:17<07:58, 10.86s/it] 77%|███████▋  | 143/186 [25:28<07:45, 10.82s/it] 77%|███████▋  | 144/186 [25:38<07:34, 10.81s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 78%|███████▊  | 145/186 [25:49<07:25, 10.86s/it] 78%|███████▊  | 146/186 [26:00<07:14, 10.87s/it] 79%|███████▉  | 147/186 [26:11<07:01, 10.81s/it] 80%|███████▉  | 148/186 [26:22<06:49, 10.78s/it] 80%|████████  | 149/186 [26:32<06:37, 10.73s/it] 81%|████████  | 150/186 [26:43<06:28, 10.81s/it] 81%|████████  | 151/186 [26:54<06:16, 10.75s/it] 82%|████████▏ | 152/186 [27:05<06:05, 10.74s/it] 82%|████████▏ | 153/186 [27:15<05:55, 10.77s/it] 83%|████████▎ | 154/186 [27:26<05:44, 10.77s/it] 83%|████████▎ | 155/186 [27:37<05:34, 10.78s/it] 84%|████████▍ | 156/186 [27:48<05:26, 10.87s/it] 84%|████████▍ | 157/186 [27:59<05:15, 10.89s/it] 85%|████████▍ | 158/186 [28:10<05:03, 10.85s/it] 85%|████████▌ | 159/186 [28:20<04:51, 10.80s/it] 86%|████████▌ | 160/186 [28:31<04:40, 10.78s/it] 87%|████████▋ | 161/186 [28:42<04:29, 10.76s/it] 87%|████████▋ | 162/186 [28:53<04:21, 10.91s/it] 88%|████████▊ | 163/186 [29:04<04:09, 10.86s/it] 88%|████████▊ | 164/186 [29:15<03:57, 10.78s/it] 89%|████████▊ | 165/186 [29:25<03:45, 10.75s/it] 89%|████████▉ | 166/186 [29:36<03:36, 10.83s/it] 90%|████████▉ | 167/186 [29:47<03:26, 10.85s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 90%|█████████ | 168/186 [29:58<03:16, 10.92s/it] 91%|█████████ | 169/186 [30:09<03:05, 10.91s/it] 91%|█████████▏| 170/186 [30:20<02:52, 10.79s/it]WARNING: tokenization mismatch: 1 vs. 48. (ignored)
 92%|█████████▏| 171/186 [30:31<02:42, 10.86s/it] 92%|█████████▏| 172/186 [30:41<02:31, 10.81s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 93%|█████████▎| 173/186 [30:52<02:20, 10.82s/it] 94%|█████████▎| 174/186 [31:03<02:09, 10.78s/it] 94%|█████████▍| 175/186 [31:13<01:57, 10.72s/it] 95%|█████████▍| 176/186 [31:24<01:46, 10.69s/it] 95%|█████████▌| 177/186 [31:35<01:36, 10.73s/it] 96%|█████████▌| 178/186 [31:46<01:25, 10.75s/it] 96%|█████████▌| 179/186 [31:57<01:15, 10.77s/it] 97%|█████████▋| 180/186 [32:07<01:04, 10.82s/it] 97%|█████████▋| 181/186 [32:18<00:53, 10.79s/it] 98%|█████████▊| 182/186 [32:29<00:42, 10.70s/it] 98%|█████████▊| 183/186 [32:39<00:32, 10.69s/it] 99%|█████████▉| 184/186 [32:50<00:21, 10.70s/it] 99%|█████████▉| 185/186 [33:01<00:10, 10.82s/it]100%|██████████| 186/186 [33:12<00:00, 10.82s/it]                                                 {'train_runtime': 1992.4819, 'train_samples_per_second': 6.743, 'train_steps_per_second': 0.093, 'train_loss': 0.15261248106597572, 'epoch': 1.0}
100%|██████████| 186/186 [33:12<00:00, 10.82s/it]100%|██████████| 186/186 [33:12<00:00, 10.71s/it]
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-5
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33msliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-1-5[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251017_192345-wnh8z5zy/logs[0m
[2025-10-17 19:58:45,428] [INFO] [launch.py:347:main] Process 2642663 exits successfully.
[2025-10-17 19:58:45,428] [INFO] [launch.py:347:main] Process 2642666 exits successfully.
[2025-10-17 19:58:46,429] [INFO] [launch.py:347:main] Process 2642664 exits successfully.
[2025-10-17 19:58:46,430] [INFO] [launch.py:347:main] Process 2642665 exits successfully.
[2025-10-17 19:58:46,430] [INFO] [launch.py:347:main] Process 2642662 exits successfully.
[2025-10-17 19:58:50,435] [INFO] [launch.py:347:main] Process 2642661 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 19:58:53,705] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 19:58:54,259] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-17 19:58:54,260] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12423 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-5 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-5 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 2 --use_anchor False --use_sample_weight True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 19:58:55,721] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 19:58:56,287] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-17 19:58:56,287] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-17 19:58:56,287] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-17 19:58:56,287] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-17 19:58:56,287] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 19:59:00,587] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 19:59:00,615] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 19:59:00,896] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 19:59:00,921] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 19:59:00,925] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 19:59:00,925] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 19:59:01,380] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 19:59:01,385] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 19:59:01,390] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 19:59:01,390] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 19:59:01,760] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 19:59:02,038] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 19:59:02,493] [INFO] [comm.py:637:init_distributed] cdb=None
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251017_195902-05wfgvg3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-5
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/05wfgvg3
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.19s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.57s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.58s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.82s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.17s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.05s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.42s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.42s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.23s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.06s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.17s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.48s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.16s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.44s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.63s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.93s/it]
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.84s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.05s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.32s/it]
Formatting inputs...Skip in lazy mode
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.61s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.61s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.22s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.52s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.34s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.77s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.08s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.78s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.21s/it]
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:0')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:2')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:1')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:4')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:5')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:3')
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.011214494705200195 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.011630535125732422 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.1013631820678711 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.013125419616699219 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10132551193237305 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10192179679870605 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/186 [00:14<45:37, 14.80s/it]  1%|          | 2/186 [00:22<33:26, 10.90s/it]  2%|▏         | 3/186 [00:31<30:31, 10.01s/it]  2%|▏         | 4/186 [00:41<29:26,  9.71s/it]  3%|▎         | 5/186 [00:50<29:13,  9.69s/it]  3%|▎         | 6/186 [01:00<29:23,  9.80s/it]  4%|▍         | 7/186 [01:10<29:27,  9.87s/it]  4%|▍         | 8/186 [01:20<29:29,  9.94s/it]  5%|▍         | 9/186 [01:31<29:33, 10.02s/it]  5%|▌         | 10/186 [01:41<29:29, 10.05s/it]  6%|▌         | 11/186 [01:51<29:29, 10.11s/it]  6%|▋         | 12/186 [02:01<29:33, 10.19s/it]  7%|▋         | 13/186 [02:12<29:25, 10.20s/it]  8%|▊         | 14/186 [02:22<29:18, 10.22s/it]  8%|▊         | 15/186 [02:32<29:22, 10.31s/it]  9%|▊         | 16/186 [02:43<29:15, 10.33s/it]  9%|▉         | 17/186 [02:53<29:03, 10.32s/it] 10%|▉         | 18/186 [03:03<28:51, 10.31s/it] 10%|█         | 19/186 [03:14<28:41, 10.31s/it] 11%|█         | 20/186 [03:24<28:30, 10.30s/it] 11%|█▏        | 21/186 [03:34<28:24, 10.33s/it] 12%|█▏        | 22/186 [03:45<28:16, 10.34s/it] 12%|█▏        | 23/186 [03:55<28:02, 10.32s/it] 13%|█▎        | 24/186 [04:06<28:02, 10.39s/it] 13%|█▎        | 25/186 [04:16<27:55, 10.41s/it] 14%|█▍        | 26/186 [04:26<27:41, 10.38s/it] 15%|█▍        | 27/186 [04:37<27:41, 10.45s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 15%|█▌        | 28/186 [04:47<27:22, 10.39s/it] 16%|█▌        | 29/186 [04:58<27:13, 10.40s/it] 16%|█▌        | 30/186 [05:08<26:57, 10.37s/it] 17%|█▋        | 31/186 [05:18<26:46, 10.37s/it] 17%|█▋        | 32/186 [05:29<26:37, 10.37s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1297 > 1048). Running this sequence through the model will result in indexing errors
 18%|█▊        | 33/186 [05:39<26:49, 10.52s/it] 18%|█▊        | 34/186 [05:50<26:26, 10.44s/it] 19%|█▉        | 35/186 [06:00<26:22, 10.48s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 19%|█▉        | 36/186 [06:11<26:13, 10.49s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1065 > 1048). Running this sequence through the model will result in indexing errors
 20%|█▉        | 37/186 [06:22<26:14, 10.57s/it] 20%|██        | 38/186 [06:32<26:06, 10.58s/it] 21%|██        | 39/186 [06:43<25:57, 10.60s/it] 22%|██▏       | 40/186 [06:53<25:44, 10.58s/it] 22%|██▏       | 41/186 [07:04<25:28, 10.54s/it] 23%|██▎       | 42/186 [07:14<25:07, 10.47s/it] 23%|██▎       | 43/186 [07:25<24:54, 10.45s/it] 24%|██▎       | 44/186 [07:35<24:30, 10.35s/it] 24%|██▍       | 45/186 [07:45<24:21, 10.36s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1263 > 1048). Running this sequence through the model will result in indexing errors
 25%|██▍       | 46/186 [07:56<24:24, 10.46s/it] 25%|██▌       | 47/186 [08:07<24:28, 10.56s/it] 26%|██▌       | 48/186 [08:17<24:14, 10.54s/it] 26%|██▋       | 49/186 [08:28<24:07, 10.57s/it] 27%|██▋       | 50/186 [08:38<23:50, 10.52s/it] 27%|██▋       | 51/186 [08:49<23:45, 10.56s/it] 28%|██▊       | 52/186 [08:59<23:32, 10.54s/it] 28%|██▊       | 53/186 [09:10<23:22, 10.54s/it] 29%|██▉       | 54/186 [09:20<23:10, 10.53s/it] 30%|██▉       | 55/186 [09:31<23:04, 10.57s/it] 30%|███       | 56/186 [09:41<22:50, 10.54s/it] 31%|███       | 57/186 [09:52<22:40, 10.55s/it] 31%|███       | 58/186 [10:02<22:22, 10.49s/it] 32%|███▏      | 59/186 [10:13<22:17, 10.53s/it] 32%|███▏      | 60/186 [10:23<22:01, 10.49s/it] 33%|███▎      | 61/186 [10:34<22:02, 10.58s/it] 33%|███▎      | 62/186 [10:45<21:56, 10.62s/it] 34%|███▍      | 63/186 [10:55<21:46, 10.63s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1073 > 1048). Running this sequence through the model will result in indexing errors
 34%|███▍      | 64/186 [11:07<21:55, 10.78s/it] 35%|███▍      | 65/186 [11:17<21:38, 10.74s/it] 35%|███▌      | 66/186 [11:28<21:25, 10.72s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1297 > 1048). Running this sequence through the model will result in indexing errors
 36%|███▌      | 67/186 [11:39<21:15, 10.72s/it] 37%|███▋      | 68/186 [11:49<20:55, 10.64s/it] 37%|███▋      | 69/186 [12:00<21:06, 10.83s/it] 38%|███▊      | 70/186 [12:11<20:53, 10.81s/it] 38%|███▊      | 71/186 [12:22<20:34, 10.74s/it] 39%|███▊      | 72/186 [12:32<20:17, 10.68s/it] 39%|███▉      | 73/186 [12:43<20:13, 10.73s/it] 40%|███▉      | 74/186 [12:54<19:59, 10.71s/it] 40%|████      | 75/186 [13:04<19:43, 10.67s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1159 > 1048). Running this sequence through the model will result in indexing errors
 41%|████      | 76/186 [13:15<19:34, 10.68s/it] 41%|████▏     | 77/186 [13:26<19:21, 10.65s/it] 42%|████▏     | 78/186 [13:36<19:08, 10.64s/it] 42%|████▏     | 79/186 [13:47<18:52, 10.58s/it] 43%|████▎     | 80/186 [13:57<18:37, 10.54s/it] 44%|████▎     | 81/186 [14:08<18:28, 10.56s/it] 44%|████▍     | 82/186 [14:18<18:16, 10.54s/it] 45%|████▍     | 83/186 [14:29<18:08, 10.57s/it] 45%|████▌     | 84/186 [14:40<18:09, 10.68s/it] 46%|████▌     | 85/186 [14:50<17:53, 10.63s/it] 46%|████▌     | 86/186 [15:01<17:46, 10.67s/it] 47%|████▋     | 87/186 [15:12<17:36, 10.67s/it]WARNING: tokenization mismatch: 1 vs. 47. (ignored)
 47%|████▋     | 88/186 [15:22<17:26, 10.67s/it] 48%|████▊     | 89/186 [15:33<17:17, 10.70s/it] 48%|████▊     | 90/186 [15:44<17:05, 10.68s/it]WARNING: tokenization mismatch: 1 vs. 48. (ignored)
 49%|████▉     | 91/186 [15:55<16:58, 10.72s/it] 49%|████▉     | 92/186 [16:05<16:50, 10.75s/it] 50%|█████     | 93/186 [16:16<16:46, 10.83s/it] 51%|█████     | 94/186 [16:27<16:30, 10.76s/it] 51%|█████     | 95/186 [16:38<16:10, 10.67s/it] 52%|█████▏    | 96/186 [16:49<16:09, 10.77s/it] 52%|█████▏    | 97/186 [16:59<16:00, 10.79s/it] 53%|█████▎    | 98/186 [17:10<15:46, 10.76s/it] 53%|█████▎    | 99/186 [17:21<15:36, 10.76s/it] 54%|█████▍    | 100/186 [17:32<15:26, 10.77s/it] 54%|█████▍    | 101/186 [17:42<15:12, 10.74s/it] 55%|█████▍    | 102/186 [17:53<15:00, 10.72s/it] 55%|█████▌    | 103/186 [18:04<14:58, 10.83s/it] 56%|█████▌    | 104/186 [18:15<14:45, 10.80s/it] 56%|█████▋    | 105/186 [18:25<14:30, 10.75s/it] 57%|█████▋    | 106/186 [18:36<14:14, 10.68s/it] 58%|█████▊    | 107/186 [18:47<14:06, 10.71s/it] 58%|█████▊    | 108/186 [18:57<13:54, 10.70s/it] 59%|█████▊    | 109/186 [19:08<13:41, 10.66s/it] 59%|█████▉    | 110/186 [19:18<13:25, 10.60s/it] 60%|█████▉    | 111/186 [19:29<13:12, 10.57s/it] 60%|██████    | 112/186 [19:39<13:00, 10.54s/it] 61%|██████    | 113/186 [19:50<12:56, 10.64s/it] 61%|██████▏   | 114/186 [20:01<12:47, 10.66s/it] 62%|██████▏   | 115/186 [20:12<12:38, 10.69s/it] 62%|██████▏   | 116/186 [20:22<12:29, 10.71s/it] 63%|██████▎   | 117/186 [20:33<12:20, 10.74s/it] 63%|██████▎   | 118/186 [20:44<12:10, 10.74s/it] 64%|██████▍   | 119/186 [20:55<11:59, 10.73s/it] 65%|██████▍   | 120/186 [21:06<11:50, 10.76s/it] 65%|██████▌   | 121/186 [21:16<11:35, 10.71s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 66%|██████▌   | 122/186 [21:27<11:27, 10.74s/it] 66%|██████▌   | 123/186 [21:38<11:16, 10.74s/it] 67%|██████▋   | 124/186 [21:48<11:01, 10.68s/it] 67%|██████▋   | 125/186 [21:59<10:49, 10.65s/it] 68%|██████▊   | 126/186 [22:10<10:40, 10.68s/it] 68%|██████▊   | 127/186 [22:21<10:39, 10.83s/it] 69%|██████▉   | 128/186 [22:32<10:27, 10.81s/it] 69%|██████▉   | 129/186 [22:42<10:14, 10.78s/it] 70%|██████▉   | 130/186 [22:53<09:59, 10.71s/it] 70%|███████   | 131/186 [23:04<09:49, 10.73s/it] 71%|███████   | 132/186 [23:15<09:46, 10.86s/it] 72%|███████▏  | 133/186 [23:25<09:32, 10.79s/it] 72%|███████▏  | 134/186 [23:36<09:19, 10.75s/it] 73%|███████▎  | 135/186 [23:47<09:09, 10.77s/it] 73%|███████▎  | 136/186 [23:57<08:56, 10.73s/it] 74%|███████▎  | 137/186 [24:08<08:50, 10.82s/it] 74%|███████▍  | 138/186 [24:19<08:37, 10.78s/it] 75%|███████▍  | 139/186 [24:30<08:29, 10.84s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
 75%|███████▌  | 140/186 [24:41<08:14, 10.75s/it] 76%|███████▌  | 141/186 [24:51<08:01, 10.71s/it] 76%|███████▋  | 142/186 [25:02<07:49, 10.67s/it]WARNING: tokenization mismatch: 1 vs. 49. (ignored)
 77%|███████▋  | 143/186 [25:13<07:45, 10.83s/it] 77%|███████▋  | 144/186 [25:24<07:33, 10.80s/it] 78%|███████▊  | 145/186 [25:35<07:21, 10.78s/it] 78%|███████▊  | 146/186 [25:46<07:13, 10.85s/it] 79%|███████▉  | 147/186 [25:56<07:00, 10.79s/it] 80%|███████▉  | 148/186 [26:07<06:55, 10.92s/it] 80%|████████  | 149/186 [26:18<06:45, 10.95s/it] 81%|████████  | 150/186 [26:29<06:33, 10.93s/it] 81%|████████  | 151/186 [26:40<06:20, 10.88s/it] 82%|████████▏ | 152/186 [26:51<06:10, 10.90s/it] 82%|████████▏ | 153/186 [27:02<05:56, 10.81s/it] 83%|████████▎ | 154/186 [27:12<05:45, 10.81s/it] 83%|████████▎ | 155/186 [27:23<05:34, 10.78s/it] 84%|████████▍ | 156/186 [27:34<05:24, 10.82s/it] 84%|████████▍ | 157/186 [27:45<05:11, 10.75s/it] 85%|████████▍ | 158/186 [27:55<04:59, 10.71s/it] 85%|████████▌ | 159/186 [28:06<04:48, 10.68s/it] 86%|████████▌ | 160/186 [28:17<04:39, 10.77s/it] 87%|████████▋ | 161/186 [28:28<04:30, 10.83s/it] 87%|████████▋ | 162/186 [28:39<04:19, 10.83s/it] 88%|████████▊ | 163/186 [28:49<04:07, 10.76s/it] 88%|████████▊ | 164/186 [29:00<03:57, 10.80s/it] 89%|████████▊ | 165/186 [29:11<03:46, 10.78s/it] 89%|████████▉ | 166/186 [29:22<03:34, 10.75s/it] 90%|████████▉ | 167/186 [29:32<03:22, 10.68s/it] 90%|█████████ | 168/186 [29:43<03:12, 10.72s/it] 91%|█████████ | 169/186 [29:53<03:01, 10.68s/it] 91%|█████████▏| 170/186 [30:04<02:50, 10.67s/it] 92%|█████████▏| 171/186 [30:15<02:43, 10.87s/it] 92%|█████████▏| 172/186 [30:26<02:31, 10.79s/it] 93%|█████████▎| 173/186 [30:37<02:19, 10.75s/it] 94%|█████████▎| 174/186 [30:47<02:08, 10.74s/it] 94%|█████████▍| 175/186 [30:58<01:57, 10.69s/it] 95%|█████████▍| 176/186 [31:09<01:46, 10.68s/it] 95%|█████████▌| 177/186 [31:19<01:36, 10.71s/it] 96%|█████████▌| 178/186 [31:31<01:26, 10.81s/it] 96%|█████████▌| 179/186 [31:41<01:15, 10.80s/it] 97%|█████████▋| 180/186 [31:52<01:04, 10.83s/it] 97%|█████████▋| 181/186 [32:03<00:53, 10.78s/it] 98%|█████████▊| 182/186 [32:14<00:43, 10.87s/it] 98%|█████████▊| 183/186 [32:25<00:32, 10.83s/it] 99%|█████████▉| 184/186 [32:36<00:21, 10.87s/it] 99%|█████████▉| 185/186 [32:46<00:10, 10.87s/it]100%|██████████| 186/186 [32:57<00:00, 10.80s/it]                                                 {'train_runtime': 1977.6454, 'train_samples_per_second': 6.793, 'train_steps_per_second': 0.094, 'train_loss': 0.15333868867607528, 'epoch': 1.0}
100%|██████████| 186/186 [32:57<00:00, 10.80s/it]100%|██████████| 186/186 [32:57<00:00, 10.63s/it]
文件夹已存在: /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-5
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33msliding_avg_norm_-lr-1e-05-acc_batch-12-beta-0.1-ls_factor_weight-2-5[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251017_195902-05wfgvg3/logs[0m
[2025-10-17 20:33:39,724] [INFO] [launch.py:347:main] Process 2647960 exits successfully.
[2025-10-17 20:33:39,725] [INFO] [launch.py:347:main] Process 2647957 exits successfully.
[2025-10-17 20:33:39,725] [INFO] [launch.py:347:main] Process 2647961 exits successfully.
[2025-10-17 20:33:39,726] [INFO] [launch.py:347:main] Process 2647959 exits successfully.
[2025-10-17 20:33:39,726] [INFO] [launch.py:347:main] Process 2647958 exits successfully.
[2025-10-17 20:33:44,731] [INFO] [launch.py:347:main] Process 2647956 exits successfully.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 20:33:48,013] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 20:33:48,569] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-17 20:33:48,569] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=12423 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./preference_data/yilin_pref_data_pooler_output.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 12 --evaluation_strategy no --save_strategy no --learning_rate 1.5e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/Re-Align/output/llava-v1.5-7b/sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-5 --image_folder /data/yilin/train2014/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-5 --project_name yilin-align --use_text_similarity False --ls_factor_text_weight 0.5 --use_img_similarity False --ls_factor_img_weight 0.5 --beta_dpo False --ls_factor_weight 0.5 --use_anchor False --use_sample_weight True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 20:33:50,045] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 20:33:50,620] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-10-17 20:33:50,620] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-10-17 20:33:50,620] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-10-17 20:33:50,620] [INFO] [launch.py:163:main] dist_world_size=6
[2025-10-17 20:33:50,620] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-17 20:33:54,422] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-17 20:33:54,755] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-17 20:33:54,918] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 20:33:55,070] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 20:33:55,182] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 20:33:55,210] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 20:33:55,516] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 20:33:55,525] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-17 20:33:55,527] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-17 20:33:55,725] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 20:33:55,848] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-17 20:33:55,848] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-17 20:33:56,170] [INFO] [comm.py:637:init_distributed] cdb=None
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: creating run
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/yilin/Re-Align/wandb/run-20251017_203356-i0yow86o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sliding_avg_norm_-lr-1.5e-05-acc_batch-12-beta-0.1-ls_factor_weight-0.5-5
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/i0yow86o
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.14s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.38s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.74s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.10s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.36s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.31s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.49s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.64s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.77s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.68s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.59s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.90s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.58s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.91s/it]
to bfloat16...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.38s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.64s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.31s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.88s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.26s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.07s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.46s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.20s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.50s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.72s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.67s/it]Formatting inputs...Skip in lazy mode
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.70s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.57s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.89s/it]
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:3')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:5')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:4')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:1')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:0')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:2')
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.011158227920532227 seconds
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yilin/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Using /home/yilin/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.01374959945678711 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10103893280029297 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10108733177185059 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10151886940002441 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10348749160766602 seconds
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = get_accelerator().IntTensor([0])
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
  0%|          | 0/186 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1958: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/186 [00:09<30:27,  9.88s/it]  1%|          | 2/186 [00:18<27:41,  9.03s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1075 > 1048). Running this sequence through the model will result in indexing errors
  2%|▏         | 3/186 [00:28<28:45,  9.43s/it]  2%|▏         | 4/186 [00:37<28:38,  9.44s/it]  3%|▎         | 5/186 [00:47<28:46,  9.54s/it]  3%|▎         | 6/186 [00:57<29:01,  9.67s/it]  4%|▍         | 7/186 [01:07<29:07,  9.76s/it]  4%|▍         | 8/186 [01:17<29:12,  9.85s/it]  5%|▍         | 9/186 [01:27<29:19,  9.94s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1074 > 1048). Running this sequence through the model will result in indexing errors
  5%|▌         | 10/186 [01:37<29:29, 10.05s/it]  6%|▌         | 11/186 [01:47<29:23, 10.08s/it]  6%|▋         | 12/186 [01:58<29:27, 10.16s/it]  7%|▋         | 13/186 [02:08<29:19, 10.17s/it]WARNING: tokenization mismatch: 1 vs. 50. (ignored)
  8%|▊         | 14/186 [02:18<29:10, 10.18s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1080 > 1048). Running this sequence through the model will result in indexing errors
  8%|▊         | 15/186 [02:29<29:28, 10.34s/it]  9%|▊         | 16/186 [02:39<29:14, 10.32s/it]  9%|▉         | 17/186 [02:50<29:11, 10.37s/it] 10%|▉         | 18/186 [03:00<29:07, 10.40s/it] 10%|█         | 19/186 [03:10<28:57, 10.40s/it] 11%|█         | 20/186 [03:21<28:47, 10.40s/it] 11%|█▏        | 21/186 [03:31<28:32, 10.38s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1080 > 1048). Running this sequence through the model will result in indexing errors
 12%|█▏        | 22/186 [03:42<28:30, 10.43s/it] 12%|█▏        | 23/186 [03:52<28:24, 10.45s/it] 13%|█▎        | 24/186 [04:03<28:48, 10.67s/it] 13%|█▎        | 25/186 [04:14<28:28, 10.61s/it] 14%|█▍        | 26/186 [04:24<28:16, 10.60s/it] 15%|█▍        | 27/186 [04:35<28:02, 10.58s/it] 15%|█▌        | 28/186 [04:46<27:47, 10.56s/it] 16%|█▌        | 29/186 [04:56<27:51, 10.65s/it] 16%|█▌        | 30/186 [05:07<27:50, 10.71s/it] 17%|█▋        | 31/186 [05:18<27:31, 10.66s/it] 17%|█▋        | 32/186 [05:28<27:19, 10.65s/it] 18%|█▊        | 33/186 [05:39<27:08, 10.64s/it] 18%|█▊        | 34/186 [05:50<26:50, 10.59s/it] 19%|█▉        | 35/186 [06:00<26:47, 10.65s/it] 19%|█▉        | 36/186 [06:11<26:47, 10.72s/it] 20%|█▉        | 37/186 [06:22<26:27, 10.65s/it] 20%|██        | 38/186 [06:32<26:11, 10.62s/it] 21%|██        | 39/186 [06:43<26:03, 10.63s/it] 22%|██▏       | 40/186 [06:54<25:58, 10.67s/it] 22%|██▏       | 41/186 [07:05<26:03, 10.78s/it] 23%|██▎       | 42/186 [07:15<25:46, 10.74s/it] 23%|██▎       | 43/186 [07:26<25:46, 10.81s/it] 24%|██▎       | 44/186 [07:37<25:29, 10.77s/it] 24%|██▍       | 45/186 [07:48<25:22, 10.79s/it] 25%|██▍       | 46/186 [07:59<25:17, 10.84s/it] 25%|██▌       | 47/186 [08:09<25:00, 10.79s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 26%|██▌       | 48/186 [08:20<24:42, 10.74s/it] 26%|██▋       | 49/186 [08:31<24:30, 10.73s/it] 27%|██▋       | 50/186 [08:41<24:16, 10.71s/it] 27%|██▋       | 51/186 [08:52<24:01, 10.68s/it] 28%|██▊       | 52/186 [09:02<23:40, 10.60s/it]WARNING: tokenization mismatch: 1 vs. 54. (ignored)
 28%|██▊       | 53/186 [09:13<23:30, 10.60s/it] 29%|██▉       | 54/186 [09:24<23:16, 10.58s/it] 30%|██▉       | 55/186 [09:34<23:12, 10.63s/it][2025-10-17 20:45:05,461] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2655199
Terminated
[2025-10-17 20:45:08,252] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2655200
[2025-10-17 20:45:08,283] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2655201
[2025-10-17 20:45:08,300] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2655202
[2025-10-17 20:45:08,413] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2655203
[2025-10-17 20:45:08,432] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2655204
[2025-10-17 20:45:08,452] [INFO] [launch.py:324:sigkill_handler] Main process received SIGTERM, exiting
